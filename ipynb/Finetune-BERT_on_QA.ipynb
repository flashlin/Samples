{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## BLOG"
      ],
      "metadata": {
        "id": "wTCuUoEHokHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@xiaohan_63326/fine-tune-fine-tuning-bert-for-question-answering-qa-task-5c29e3d518f1"
      ],
      "metadata": {
        "id": "6Le1w-cgokdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CODE"
      ],
      "metadata": {
        "id": "DiJB7Svool3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ZuU163olSx",
        "outputId": "344e5aff-38e9-499f-a289-6f3e289f7d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "waTrrqb3ow8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "from datasets import load_dataset\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
        "squad = squad.train_test_split(test_size=0.2)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ZIwPFHv-ow_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Convert the dataset to a dictionary\n",
        "data_dict = squad[\"train\"].to_dict()\n",
        "# Create a DataFrame from the dictionary\n",
        "df = pd.DataFrame.from_dict(data_dict)"
      ],
      "metadata": {
        "id": "D9PgV35MoxCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "questions = [q.strip() for q in df[\"question\"]]\n",
        "context = [q.strip() for q in df[\"context\"]]\n",
        "inputs = tokenizer(\n",
        "        questions,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "answers = df['answers']\n",
        "for i, offset in enumerate(offset_mapping):\n",
        "    answer = answers[i]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    # If the answer is not fully inside the context, label it (0, 0)\n",
        "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "        start_positions.append(0)\n",
        "        end_positions.append(0)\n",
        "    else:\n",
        "        # Otherwise it's the start and end token positions\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions.append(idx + 1)\n",
        "\n",
        "df[\"start_positions\"] = start_positions\n",
        "df[\"end_positions\"] = end_positions\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "data = {'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        'start_positions':start_positions,\n",
        "        'end_positions': end_positions,\n",
        "       }\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('encoding_train.csv',index=False)\n",
        "train = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "FukGhA5yozZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Convert the dataset to a dictionary\n",
        "data_dict = squad[\"test\"].to_dict()\n",
        "# Create a DataFrame from the dictionary\n",
        "df = pd.DataFrame.from_dict(data_dict)\n",
        "\n",
        "questions = [q.strip() for q in df[\"question\"]]\n",
        "context = [q.strip() for q in df[\"context\"]]\n",
        "inputs = tokenizer(\n",
        "        questions,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "answers = df['answers']\n",
        "for i, offset in enumerate(offset_mapping):\n",
        "    answer = answers[i]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    # If the answer is not fully inside the context, label it (0, 0)\n",
        "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "        start_positions.append(0)\n",
        "        end_positions.append(0)\n",
        "    else:\n",
        "        # Otherwise it's the start and end token positions\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions.append(idx + 1)\n",
        "\n",
        "df[\"start_positions\"] = start_positions\n",
        "df[\"end_positions\"] = end_positions\n",
        "\n",
        "data = {'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        'start_positions':start_positions,\n",
        "        'end_positions': end_positions,\n",
        "       }\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('encoding_test.csv',index=False)\n",
        "test = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "jeNDcCqMozb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_qa_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "pg-NRfC2ozdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "-B3zJt3JozfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cXOLq2saozhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0K-Bw8Teozjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VV-bt7KNoxEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sz-tylpEoxGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivQ01_W6oxIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}