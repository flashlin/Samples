{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "532426d2b4a843eb8d650f5666b51874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1473672cad24f2889a5419e0a31b813",
              "IPY_MODEL_34cad9c04a304f37b1ea1d9250c3c372",
              "IPY_MODEL_2c541fafda5346af9e6af352e50fc661"
            ],
            "layout": "IPY_MODEL_b4c4519b493e45eeb9a284edca97d228"
          }
        },
        "e1473672cad24f2889a5419e0a31b813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_132f532d53ac4a2e9e4c0c1da9095a0d",
            "placeholder": "​",
            "style": "IPY_MODEL_5e7acd33a8794a28a70e443d254a77ef",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "34cad9c04a304f37b1ea1d9250c3c372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19128917c9624b7aa5eb546b5a856c36",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de8fe61621b9442ba8ed11c0bbe6ff99",
            "value": 2
          }
        },
        "2c541fafda5346af9e6af352e50fc661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d21ba2adced43e5862cf298536cecba",
            "placeholder": "​",
            "style": "IPY_MODEL_683c7f41b54a46889c357bfd1bd54804",
            "value": " 2/2 [01:23&lt;00:00, 38.14s/it]"
          }
        },
        "b4c4519b493e45eeb9a284edca97d228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "132f532d53ac4a2e9e4c0c1da9095a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e7acd33a8794a28a70e443d254a77ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19128917c9624b7aa5eb546b5a856c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de8fe61621b9442ba8ed11c0bbe6ff99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d21ba2adced43e5862cf298536cecba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "683c7f41b54a46889c357bfd1bd54804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4d6dfe7972a4a0faffe3b79b0bdeb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e3a169a860344a3867440e5fa3e0fa8",
              "IPY_MODEL_ebc6236f25f04e29aca2397438f76f21",
              "IPY_MODEL_5b52af673f6d42d4a8c951e09f981792"
            ],
            "layout": "IPY_MODEL_d105d2a398f046888062d4e77f12c279"
          }
        },
        "8e3a169a860344a3867440e5fa3e0fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56f85c16b5542409d7e0bdee24c0c49",
            "placeholder": "​",
            "style": "IPY_MODEL_5b0c522274564de7976856e5094b0639",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ebc6236f25f04e29aca2397438f76f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a0f7eb3e114fcaafede75bde8fcd96",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47c84d61f95b4053ad0b89a22081830f",
            "value": 2
          }
        },
        "5b52af673f6d42d4a8c951e09f981792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2992d31b4eca4fa0a621b11d1b6702ff",
            "placeholder": "​",
            "style": "IPY_MODEL_c7cfc76a02624b88868662029ea24e93",
            "value": " 2/2 [01:13&lt;00:00, 33.38s/it]"
          }
        },
        "d105d2a398f046888062d4e77f12c279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56f85c16b5542409d7e0bdee24c0c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0c522274564de7976856e5094b0639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21a0f7eb3e114fcaafede75bde8fcd96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c84d61f95b4053ad0b89a22081830f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2992d31b4eca4fa0a621b11d1b6702ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7cfc76a02624b88868662029ea24e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c4290d5dd9c4a1ba1cd5fd7d66be61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48ddbc7e8f544a49bd0bc874e1c6792e",
              "IPY_MODEL_c66743df57a94ca989b911de771639b5",
              "IPY_MODEL_c7ea4b4979404af69e3e3dd3e9c7da50"
            ],
            "layout": "IPY_MODEL_90db7372ff6d4f48afbf1eb3bef7f993"
          }
        },
        "48ddbc7e8f544a49bd0bc874e1c6792e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b249be173c9948bf98d334765862af15",
            "placeholder": "​",
            "style": "IPY_MODEL_a98a8620ee814ce9b5de35065957da2a",
            "value": "pytorch_model-00001-of-00002.bin: 100%"
          }
        },
        "c66743df57a94ca989b911de771639b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9c36608e47d4f94a8c283dd2a922943",
            "max": 9976637950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a07ee43ae04e40df85691b81c3609c4a",
            "value": 9976637950
          }
        },
        "c7ea4b4979404af69e3e3dd3e9c7da50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a6acd5ce2c4f31ba86ba4ad2691e34",
            "placeholder": "​",
            "style": "IPY_MODEL_67f248e7a450413584a8ef51d11b180b",
            "value": " 9.98G/9.98G [14:50&lt;00:00, 11.4MB/s]"
          }
        },
        "90db7372ff6d4f48afbf1eb3bef7f993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b249be173c9948bf98d334765862af15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98a8620ee814ce9b5de35065957da2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9c36608e47d4f94a8c283dd2a922943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07ee43ae04e40df85691b81c3609c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a6acd5ce2c4f31ba86ba4ad2691e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f248e7a450413584a8ef51d11b180b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b29994caf9e4c49836bfdd21cfc8fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b28a0d635e90438488dda6610a19349b",
              "IPY_MODEL_3a7ad4fde88043a88cc2c3000c333cd6",
              "IPY_MODEL_71a453726b064227a2401a3025ca7c4a"
            ],
            "layout": "IPY_MODEL_f88af1775c05420584294d5fe9cd037a"
          }
        },
        "b28a0d635e90438488dda6610a19349b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf5fa571062482f8ec554d5cf93a970",
            "placeholder": "​",
            "style": "IPY_MODEL_845a185db0204cf9a08006b84225e231",
            "value": "Upload 2 LFS files: 100%"
          }
        },
        "3a7ad4fde88043a88cc2c3000c333cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc7d384e0b5f428b9d123e2f13ec413d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cf2bdfbea934d6684880392405c64c7",
            "value": 2
          }
        },
        "71a453726b064227a2401a3025ca7c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15b45de3122c4241a9c71dc7a5cc7d7f",
            "placeholder": "​",
            "style": "IPY_MODEL_12bb1a750ee440738093e852387a4ab7",
            "value": " 2/2 [14:51&lt;00:00, 891.43s/it]"
          }
        },
        "f88af1775c05420584294d5fe9cd037a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf5fa571062482f8ec554d5cf93a970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "845a185db0204cf9a08006b84225e231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc7d384e0b5f428b9d123e2f13ec413d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf2bdfbea934d6684880392405c64c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15b45de3122c4241a9c71dc7a5cc7d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12bb1a750ee440738093e852387a4ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc5094d073944096a2d6a7243494ccdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c71e60eb88ba4bacb18dc9201204df24",
              "IPY_MODEL_dbcd6d08f542461ea653a81d755f67fc",
              "IPY_MODEL_f9d3b41ab2034183b2807df610e8ed9c"
            ],
            "layout": "IPY_MODEL_c84c653b92734591ad57ffd799396a5d"
          }
        },
        "c71e60eb88ba4bacb18dc9201204df24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c862f87d58149938308fbb6dda0ea30",
            "placeholder": "​",
            "style": "IPY_MODEL_35adabd0c9db41049cf9273371247fbf",
            "value": "pytorch_model-00002-of-00002.bin: 100%"
          }
        },
        "dbcd6d08f542461ea653a81d755f67fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5a254d683d429b99e102239feb41e9",
            "max": 3500316627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_394cb5ff0efa428081621d2a60af6262",
            "value": 3500316627
          }
        },
        "f9d3b41ab2034183b2807df610e8ed9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fdb7ccdd963438ea3df2e8ca1fcbddc",
            "placeholder": "​",
            "style": "IPY_MODEL_6435d0c17a874da1a5fe59f44ebe23f2",
            "value": " 3.50G/3.50G [05:25&lt;00:00, 11.9MB/s]"
          }
        },
        "c84c653b92734591ad57ffd799396a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c862f87d58149938308fbb6dda0ea30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35adabd0c9db41049cf9273371247fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de5a254d683d429b99e102239feb41e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394cb5ff0efa428081621d2a60af6262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fdb7ccdd963438ea3df2e8ca1fcbddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6435d0c17a874da1a5fe59f44ebe23f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction-tuning/Fine-tuning Llama 2 for News Category Prediction\n",
        "\n",
        "**Author: [Kshitiz Sahay](https://www.linkedin.com/in/k-kshitiz26/)**\n",
        "\n",
        "In this notebook, I will guide you through the process of fine-tuning Meta's Llama 2 7B model for news article categorization across 18 different categories. I will utilize a news classification instruction dataset that I previously created using GPT 3.5. If you're interested in how I generated that dataset and the motivation behind this mini-project, you can refer to my earlier [blog](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) or [notebook](https://colab.research.google.com/drive/16rZ8DlvQp5YJED1ECUNbLKbu2YWLcLST?usp=sharing#scrollTo=9kfUqKIOSH5J) where I discuss the details.\n",
        "\n",
        "The purpose of this notebook is to provide a comprehensive, step-by-step tutorial for fine-tuning any LLM (Large Language Model). Unlike many tutorials available, I'll explain each step in a detailed manner, covering all classes, functions, and parameters used.\n",
        "\n",
        "This guide will be divided into two parts:\n",
        "\n",
        "**Part 1: Setting up and Preparing for Fine-Tuning**\n",
        "1. Installing and loading the required modules\n",
        "2. Steps to get approval for Meta's Llama 2 family of models\n",
        "3. Setting up Hugging Face CLI and user authentication\n",
        "4. Loading a pre-trained model and its associated tokenizer\n",
        "5. Loading the training dataset\n",
        "6. Preprocessing the training dataset for model fine-tuning\n",
        "\n",
        "**Part 2: Fine-Tuning and Open-Sourcing**\n",
        "1. Configuring PEFT (Parameter Efficient Fine-Tuning) method QLoRA for efficient fine-tuning\n",
        "2. Fine-tuning the pre-trained model\n",
        "3. Saving the fine-tuned model and its associated tokenizer\n",
        "4. Pushing the fine-tuned model to the Hugging Face Hub for public usage\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "**Note that running this on a CPU is practically impossible. If running on Google Colab, go to Runtime > Change runtime type. Change Hardware accelarator to GPU. Change GPU type to T4. Change Runtime shape to High-RAM.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OSHlAbqzDFDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Required Libraries\n",
        "\n",
        "First, we will install some required libraries.\n",
        "\n",
        "`transformers`: for loading a large language model and fine-tuning it.\n",
        "\n",
        "`bitsandbytes`: for loading the model in 4-bit precision.\n",
        "\n",
        "`accelerate`: for training models and performing inference at scale.\n",
        "\n",
        "`peft`: for fine-tuning a small number of parameters.\n",
        "\n",
        "`trl`: for training transformer language models using Reinforcement Learning.\n"
      ],
      "metadata": {
        "id": "HaxucZrSujXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 --progress-bar off\n",
        "!pip install -q peft==0.4.0 --progress-bar off\n",
        "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
        "!pip install -q transformers==4.31.0 --progress-bar off\n",
        "!pip install -q trl==0.4.7 --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Required Libraries\n",
        "\n",
        "Next, we will load the required libraries for fine-tuning a Large Language Model (LLM) like Llama 2. We will look at each imported class in greater detail in subsequent sections."
      ],
      "metadata": {
        "id": "i3USAIUqul9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from random import randrange\n",
        "from functools import partial\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          HfArgumentParser,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          EarlyStoppingCallback,\n",
        "                          pipeline,\n",
        "                          logging,\n",
        "                          set_seed)\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
        "from trl import SFTTrainer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1044f40a-ce77-4646-bed6-75230e979c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Hub Login\n",
        "\n",
        "Meta's family of Llama 2 models is gated. You will require approval to access it using the Hugging Face Hub.\n",
        "\n",
        "Below are the steps to request permission for the Llama-2-7B model:\n",
        "1. Get approval from Hugging Face (https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
        "2. Get approval from Meta (https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
        "3. Create a WRITE access token on Hugging Face (https://huggingface.co/settings/tokens).\n",
        "4. Execute `!huggingface-cli login` in Google Colab Notebook, enter the token, and enter \"Y.\"\n",
        "\n",
        "Note: Make sure your email address on your Hugging Face account is the same as the one you enter on Meta's website for approval.\n",
        "\n",
        "If you don't want to perform the above steps, use a cloned version of Llama-2-7B, such as https://huggingface.co/daryl149/llama-2-7b-chat-hf. Additionally, you'll have to set `use_auth_token` to `False` while loading the model and its tokenizer."
      ],
      "metadata": {
        "id": "N2jeE5EyvB8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi6lOiqEuySY",
        "outputId": "4d481780-d341-4d3e-d75d-457d2e8f8b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Bitsandbytes Configuration\n",
        "\n",
        "Before loading the model, we will define a function `create_bnb_config` to define the `bitsandbytes` configuration. The `bitsandbytes` library allows model quantization. Quantization is a technique used to compress deep learning models by reducing the number of bits used to represent their weights and activations. This compression allows for faster inference and reduced memory consumption, making it possible to deploy these models on edge devices with limited resources.\n",
        "\n",
        "By using 4-bit transformer language models, we can achieve impressive results while significantly reducing memory and computational requirements.\n",
        "\n",
        "Hugging Face Transformers (`transformers`) is closely integrated with `bitsandbytes`. The `BitsAndBytesConfig` class from the `transformers` library allows configuring the model quantization method.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "`load_in_4bit`: Load the model in 4-bit precision, i.e., divide memory usage by 4.\n",
        "\n",
        "`bnb_4bit_use_double_quant`: Use nested quantization techniques for more memory-efficient inference at no additional cost.\n",
        "\n",
        "`bnb_4bit_quant_type`: Set quantization data type. The options are either FP4 (4-bit precision), which is the default quantization data type, or NF4 (Normal Float 4), a new 4-bit data type adapted for weights that have been initialized using a normal distribution.\n",
        "\n",
        "`bnb_4bit_compute_dtype`: Set the computational data type for 4-bit models. Default value: torch.float32"
      ],
      "metadata": {
        "id": "mDQHT4nivpBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
        "    \"\"\"\n",
        "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
        "\n",
        "    :param load_in_4bit: Load model in 4-bit precision mode\n",
        "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
        "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
        "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
        "    \"\"\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
        "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
        "    )\n",
        "\n",
        "    return bnb_config"
      ],
      "metadata": {
        "id": "H2o8PO-T0JWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Hugging Face Model and Tokenizer\n",
        "\n",
        "We will now define a function `load_model` that accepts the model name (`model_name`) from Hugging Face Hub and the `bitsandbytes` configuration for model quantization.\n",
        "\n",
        "In this function, we will perform the following steps:\n",
        " 1. Get the number of GPUs available.\n",
        " 2. Set the maximum GPU memory.\n",
        " 3. Use the from_pretrained` method from the `AutoModelForCausalLM` class to load a pre-trained Hugging Face model in 4-bit precision using the model name and the quantization configuration.\n",
        " 4. Set which device to send the model to using `device_map`. Passing `device_map = 0` means putting the whole model on GPU 0. Other inputs could be `cpu`, `cuda:1`, etc. Setting `device_map = auto` will let `accelerate` compute the most optimized `device_map` automatically.\n",
        " 5. Set `max_memory`, a dictionary device identifier, to maximum memory, which will default to the maximum memory available for each GPU and the available CPU RAM if unset.\n",
        " 6. Load the model tokenizer from the model name on Hugging Face.\n",
        " 7. Set a padding token to ensure shorter sequences will have the same length as the longest sequence in a batch. In this case, we will set the EOS (End of Sentence) token as the padding token.\n",
        "\n",
        " **Important Note:  A tokenizer for a model will preprocess and tokenize (convert letters/words/sub-words to tokens or numbers) the input in a way that the model expects. Model tokenizers are also responsible for correctly applying special tokens and certain special embeddings or positional encoders specific to a model in the input.**"
      ],
      "metadata": {
        "id": "542r2h250cMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, bnb_config):\n",
        "    \"\"\"\n",
        "    Loads model and model tokenizer\n",
        "\n",
        "    :param model_name: Hugging Face model name\n",
        "    :param bnb_config: Bitsandbytes configuration\n",
        "    \"\"\"\n",
        "\n",
        "    # Get number of GPU device and set maximum memory\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config = bnb_config,\n",
        "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
        "    )\n",
        "\n",
        "    # Load model tokenizer with the user authentication token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)\n",
        "\n",
        "    # Set padding token as EOS token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "gIqPR8O-s7WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Transformers and Bitsandbytes Parameters\n",
        "\n",
        "We will now initialize input parameters for the `transformers` and `bitsandbytes` modules."
      ],
      "metadata": {
        "id": "2uv2iEdHzvJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# transformers parameters\n",
        "################################################################################\n",
        "\n",
        "# The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "load_in_4bit = True\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "bnb_4bit_use_double_quant = True\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Compute data type for 4-bit base models\n",
        "bnb_4bit_compute_dtype = torch.bfloat16"
      ],
      "metadata": {
        "id": "_BxEZPWQtK6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will call the above functions to get `model` and `tokenizer` objects."
      ],
      "metadata": {
        "id": "D6-s-A19z8I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
        "\n",
        "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
        "\n",
        "model, tokenizer = load_model(model_name, bnb_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "532426d2b4a843eb8d650f5666b51874",
            "e1473672cad24f2889a5419e0a31b813",
            "34cad9c04a304f37b1ea1d9250c3c372",
            "2c541fafda5346af9e6af352e50fc661",
            "b4c4519b493e45eeb9a284edca97d228",
            "132f532d53ac4a2e9e4c0c1da9095a0d",
            "5e7acd33a8794a28a70e443d254a77ef",
            "19128917c9624b7aa5eb546b5a856c36",
            "de8fe61621b9442ba8ed11c0bbe6ff99",
            "7d21ba2adced43e5862cf298536cecba",
            "683c7f41b54a46889c357bfd1bd54804"
          ]
        },
        "id": "Pb6Q94ZttDEB",
        "outputId": "185017bd-b8da-496d-8e0d-15ce1f0314fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "532426d2b4a843eb8d650f5666b51874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset\n",
        "\n",
        "Now that we have loaded the Llama-2-7B model and its tokenizer, we will move on to loading our news classification instruction dataset from the previous blog as a Hugging Face `Datasets`.\n",
        "\n",
        "Firstly, we will initialize the path of the dataset. In this case, we have a CSV file that contains 99 records, or prompts. This dataset contains an `instruction` column containing the instruction to categorize a news article into 18 categories, an `input` column containing the news article, and an `output` column containing the actual news category for training.\n",
        "\n",
        "We will use the `load_dataset` function and pass the file location. We will define a generic dataset builder name `csv` because our dataset is a CSV file. You can similarly load a JSON file by passing `json` and the dataset location to a JSON file. All the records are assigned to the `train` split by default, which we would retrieve using the `split` parameter."
      ],
      "metadata": {
        "id": "cQsdgLh44yM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The instruction dataset to use\n",
        "dataset_name = \"/content/drive/MyDrive/news_classification.csv\""
      ],
      "metadata": {
        "id": "4gEb5z2j5FyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"csv\", data_files = dataset_name, split = \"train\")"
      ],
      "metadata": {
        "id": "Ftmfi4M9RJAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of prompts: {len(dataset)}')\n",
        "print(f'Column names are: {dataset.column_names}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLoaZvdTmIZT",
        "outputId": "7f2822e9-67f9-462f-d46a-77bd0de70cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of prompts: 99\n",
            "Column names are: ['instruction', 'input', 'output']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `load_dataset` function will convert the CSV file into a dictionary of prompts. We can look at a random prompt in the dataset using a random index."
      ],
      "metadata": {
        "id": "t6hxLRYZ7Fwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[randrange(len(dataset))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulIcDOQVpun3",
        "outputId": "a82ac9b8-d4db-457a-fdf6-4a939b71f7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Categorize the news article into one of the 18 categories:\\n\\nWORLD NEWS\\nCOMEDY\\nPOLITICS\\nTECH\\nSPORTS\\nBUSINESS\\nOTHERS\\nENTERTAINMENT\\nCULTURE & ARTS\\nFOOD & DRINK\\nMEDIA\\nRELIGION\\nMONEY\\nHEALTHY LIVING\\nSCIENCE\\nEDUCATION\\nCRIME\\nENVIRONMENT\\n\\n',\n",
              " 'input': 'From the Wires Sep. 16, 2015 03:00 AM \\n   \\n\\r \\n\\nFRANKFURT, Germany , Sept. 16, 2015 /PRNewswire/ --\\xa0 SAP SE (NYSE: SAP) will showcase new technology, applications and software to run the next generation of smart cities and drive urban mobility at the International Motor Show (IAA) in Germany from Sept. 16-27 in Frankfurt . \\n\\n\"We are focused on creating intelligent, open and flexible solutions that foster the urban ecosystems in which we live and work,\" said Dr. Jurgen Muller , head of Innovation Center Network, SAP SE. \"As cities grow fast and face challenges in terms of resources and infrastructure, they need transparent, collaborative and innovative technology to help them prosper.\" \\n\\nSAP will present the following innovations at IAA\\'s New Mobility World exhibition. Through demonstration pods and a stage program, participants will be able to see how cities use SAP® technology to become smarter. \\n\\nCityApp \\n\\nThis solution features crowdsourcing functionality that lets citizens report defects and damage in their immediate vicinity. Algorithms assimilate these reports with data, such as traffic density within that city sector, which can result in optimized city administration. SAP presents its \"CityApp\" project in cooperation with the city of Nuremberg. \\n\\nSmart traffic control \\n\\nThis technology shows how Big Data-driven insights based on real-time traffic conditions and predictive analytics can help cities run smarter. Smart traffic control allows optimized traffic light controls and additional car lanes to help avoid rush-hour traffic congestion. Congestion indexes and speed controls based on data from RFID, GPS, camera and induction loop technologies provide pictures of real-time traffic issues and compare conditions between city sectors. Origin-destination analysis compares travel behaviors between city sectors, areas, streets and multimodal travel. \\n\\nTwoGo® mobile app \\n  \\nThis mobile app by SAP shows how software can help users share rides, thereby helping enterprises, employees, institutions and local authorities save time and money. \\n\\nSAP Vehicle Insights \\n\\nThis analytics cloud application combines vehicular data with sensor data to provide actionable insight into driver behavior patterns and driver efficiency. The software helps logistics and mobility services monitor live vehicle conditions and manage within constraints imposed by pollution and traffic congestion. The application helps fleet operators manage their fleets optimally. \\n\\n\"With the growing number of connected vehicles, broader use of sensors and even autonomous driving on the near horizon, there is a serious opportunity to transform cities, industries and businesses,\" said Stephan Brand , vice president, Products & Innovation at SAP. \\n\\nFor more information, visit the SAP News Center . Follow SAP on Twitter at @sapnews . \\n\\nMedia Contact: \\nHilmar Schepp , SAP, +49 (6227) 7-46799, hilmar.schepp@sap.com , CET \\n\\nAny statements contained in this document that are not historical facts are forward-looking statements as defined in the U.S. Private Securities Litigation Reform Act of 1995. Words such as \"anticipate,\" \"believe,\" \"estimate,\" \"expect,\" \"forecast,\" \"intend,\" \"may,\" \"plan,\" \"project,\" \"predict,\" \"should\" and \"will\" and similar expressions as they relate to SAP are intended to identify such forward-looking statements. SAP undertakes no obligation to publicly update or revise any forward-looking statements. All forward-looking statements are subject to various risks and uncertainties that could cause actual results to differ materially from expectations. The factors that could affect SAP\\'s future financial results are discussed more fully in SAP\\'s filings with the U.S. Securities and Exchange Commission (\"SEC\"), including SAP\\'s most recent Annual Report on Form 20-F filed with the SEC. Readers are cautioned not to place undue reliance on these forward-looking statements, which speak only as of their dates. \\n\\nLogo - http://photos.prnewswire.com/prnh/20110126/AQ34470LOGO  \\n\\nTo view the original version on PR Newswire, visit: http://www.prnewswire.com/news-releases/newsbyte-sap-powers-smart-city-forum-at-new-mobility-world-iaa-2015-frankfurt-300143796.html \\n\\nSOURCE  SAP SE \\n About PR Newswire \\nCopyright © 2007 PR Newswire. All rights reserved. Republication or redistribution of PRNewswire content is expressly prohibited without the prior written consent of PRNewswire. PRNewswire shall not be liable for any errors or delays in the content, or for any actions taken in reliance thereon.',\n",
              " 'output': 'TECH'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Prompt Template\n",
        "\n",
        "After loading the instruction dataset, we will define the `create_prompt_formats` function to create a prompt template against each prompt in our dataset and save it in a new dictionary key `text` for further data preprocessing and fine-tuning."
      ],
      "metadata": {
        "id": "VmOzs7Wj6NnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
        "\n",
        "    :param sample: Prompt or sample from the instruction dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize static strings for the prompt template\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruction:\"\n",
        "    INPUT_KEY = \"Input:\"\n",
        "    RESPONSE_KEY = \"### Response:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    # Combine a prompt with the static strings\n",
        "    blurb = f\"{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
        "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    # Create a list of prompt template elements\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    # Join prompt template elements into a single string to create the prompt template\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "\n",
        "    # Store the formatted prompt template in a new key \"text\"\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "KgLM5FR0mSP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_prompt_formats(dataset[randrange(len(dataset))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL1qmnMER4nH",
        "outputId": "7990c83d-56e3-488a-f9ce-d48df5e2b5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Categorize the news article into one of the 18 categories:\\n\\nWORLD NEWS\\nCOMEDY\\nPOLITICS\\nTECH\\nSPORTS\\nBUSINESS\\nOTHERS\\nENTERTAINMENT\\nCULTURE & ARTS\\nFOOD & DRINK\\nMEDIA\\nRELIGION\\nMONEY\\nHEALTHY LIVING\\nSCIENCE\\nEDUCATION\\nCRIME\\nENVIRONMENT\\n\\n',\n",
              " 'input': '-- -- \\n\\n[Nation] A court has declined to stop the replacement of Ms Grace Kaindi as deputy inspector-general of police. \\n(c) AllAfrica News: Kenya – Read entire story here .',\n",
              " 'output': 'CRIME',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the news article into one of the 18 categories:\\n\\nWORLD NEWS\\nCOMEDY\\nPOLITICS\\nTECH\\nSPORTS\\nBUSINESS\\nOTHERS\\nENTERTAINMENT\\nCULTURE & ARTS\\nFOOD & DRINK\\nMEDIA\\nRELIGION\\nMONEY\\nHEALTHY LIVING\\nSCIENCE\\nEDUCATION\\nCRIME\\nENVIRONMENT\\n\\n\\n\\nInput:\\n-- -- \\n\\n[Nation] A court has declined to stop the replacement of Ms Grace Kaindi as deputy inspector-general of police. \\n(c) AllAfrica News: Kenya – Read entire story here .\\n\\n### Response:\\nCRIME\\n\\n### End'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Maximum Sequence Length of the Pre-trained Model\n",
        "\n",
        "In the next cell, we will define the `get_max_length` function to find out the maximum sequence length of the Llama-2-7B model. This function will pull the model configuration and attempt to find the maximum sequence length from one of the several configuration keys that may contain it. If the maximum sequence length is not found, it will default to 1024. We will use the maximum sequence length during dataset preprocessing to remove records that exceed that context length because the pre-trained model won't accept them."
      ],
      "metadata": {
        "id": "V42Lk6Vx6y6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "    \"\"\"\n",
        "    Extracts maximum token length from the model configuration\n",
        "\n",
        "    :param model: Hugging Face model\n",
        "    \"\"\"\n",
        "\n",
        "    # Pull model configuration\n",
        "    conf = model.config\n",
        "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
        "    max_length = None\n",
        "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "q6bAo-8nmtFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing Dataset Batch\n",
        "\n",
        "The user-defined `preprocess_batch` function will tokenize a batch of the input dataset (`batch`) using the `tokenizer` object. We will set the maximum sequence length using the `max_length` parameter, which will control the maximum length used by the padding or truncation parameter. `truncation = True` will truncate the input to the maximum length provided by the `max_length` parameter. Similarly, `padding = max_length` will pad the input to the maximum length provided. This function will be called in the `preprocess_dataset` function defined next."
      ],
      "metadata": {
        "id": "TNs971cLTTTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizes dataset batch\n",
        "\n",
        "    :param batch: Dataset batch\n",
        "    :param tokenizer: Model tokenizer\n",
        "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length = max_length,\n",
        "        truncation = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "C7vwFmJhmt6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Dataset\n",
        "\n",
        "To preprocess the complete dataset for fine-tuning, we will define the `preprocess_dataset` function, which will perform the following operations:\n",
        "\n",
        "1. Create the formatted prompts against each prompt in the instruction dataset using the `create_prompt_formats` function.\n",
        "2. Tokenize the dataset in batches using the `preprocess_batch` function and removing the original dictionary keys (instruction, input, output, and text).\n",
        "3. Filter out prompts with input token sizes exceeding the maximum length.\n",
        "4. Shuffle the dataset using a random seed."
      ],
      "metadata": {
        "id": "9rsrRaiMDdIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
        "    \"\"\"\n",
        "    Tokenizes dataset for fine-tuning\n",
        "\n",
        "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
        "    :param seed: Random seed for reproducibility\n",
        "    :param dataset (str): Instruction dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched = True,\n",
        "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed = seed)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "S9UkOnqgmvlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seed\n",
        "seed = 33\n",
        "\n",
        "max_length = get_max_length(model)\n",
        "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50XY9r6ssu-x",
        "outputId": "5e96df60-efe9-483b-d821-3f63508127a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found max lenth: 4096\n",
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now look at the preprocessed dataset, which contains tokens or IDs."
      ],
      "metadata": {
        "id": "jepKTqUuIgID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-aACsuus0ps",
        "outputId": "82702532-0185-4f78-e83d-3f1a1b668763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wN2y7yAIfN1",
        "outputId": "51ab6fcf-ec93-4b99-a9c2-44b696f6655b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 29907, 20440, 675, 278, 9763, 4274, 964, 697, 310, 278, 29871, 29896, 29947, 13997, 29901, 13, 13, 11686, 10249, 14693, 7811, 13, 3217, 2303, 29928, 29979, 13, 29925, 5607, 1806, 2965, 29903, 13, 4330, 3210, 13, 5550, 8476, 29903, 13, 29933, 3308, 8895, 1799, 13, 2891, 4448, 29903, 13, 3919, 1001, 6040, 1177, 13780, 13, 29907, 8647, 11499, 669, 9033, 9375, 13, 5800, 13668, 669, 26900, 1177, 29968, 13, 2303, 4571, 29909, 13, 1525, 5265, 29954, 2725, 13, 29924, 12413, 29979, 13, 9606, 1964, 4690, 29979, 365, 5667, 4214, 13, 7187, 29902, 1430, 4741, 13, 3352, 23129, 8098, 13, 29907, 3960, 2303, 13, 25838, 8193, 1164, 13780, 13, 13, 13, 13, 4290, 29901, 13, 797, 9937, 350, 25151, 29915, 29879, 716, 3814, 29871, 29955, 2799, 424, 3077, 2841, 540, 5662, 1973, 393, 366, 1033, 2048, 901, 20793, 3573, 4158, 322, 16951, 14505, 596, 1532, 2264, 297, 871, 29871, 29955, 6233, 594, 388, 29889, 6527, 445, 451, 367, 2089, 29973, 1317, 372, 28326, 1821, 304, 2048, 2301, 2841, 5172, 29973, 29871, 13, 1576, 2446, 2655, 366, 674, 6398, 304, 12709, 338, 679, 777, 3077, 2841, 17166, 5883, 3174, 393, 526, 1781, 29889, 306, 29915, 29885, 1854, 366, 505, 6091, 310, 1432, 697, 310, 278, 5193, 322, 26823, 1048, 963, 448, 322, 366, 881, 367, 9543, 310, 393, 896, 526, 7795, 5611, 304, 19912, 366, 304, 1653, 2301, 2841, 29889, 887, 674, 1284, 263, 3287, 310, 1316, 22535, 297, 278, 9999, 6689, 29892, 541, 697, 607, 1258, 286, 3055, 7799, 363, 592, 338, 4257, 376, 29933, 24456, 1920, 305, 1642, 29871, 13, 13, 1576, 1820, 19262, 2629, 1749, 17873, 338, 1749, 298, 555, 2873, 29889, 341, 2122, 505, 263, 3287, 901, 1243, 15664, 650, 29892, 322, 5480, 505, 263, 13682, 931, 8363, 373, 3077, 2841, 17166, 9179, 944, 29879, 260, 15118, 29889, 19955, 583, 505, 304, 12879, 14238, 9950, 29892, 263, 3353, 3287, 901, 707, 307, 1885, 29892, 607, 1122, 3275, 29889, 1205, 17126, 310, 445, 21578, 297, 1749, 13994, 29892, 297, 1206, 263, 11379, 756, 263, 19396, 2948, 304, 7688, 29880, 24377, 322, 3033, 6751, 297, 8267, 1919, 1135, 1183, 1033, 1603, 9269, 297, 278, 9939, 11174, 29889, 29871, 13, 13, 6716, 756, 304, 367, 16010, 451, 304, 2225, 29581, 385, 975, 29881, 852, 363, 10952, 23633, 29889, 450, 21957, 5505, 4319, 29889, 5806, 9324, 8333, 902, 5824, 526, 1304, 29892, 896, 7738, 14568, 3077, 2841, 17166, 349, 6090, 310, 10430, 2629, 278, 3573, 29889, 7338, 336, 12871, 1795, 6963, 373, 13394, 269, 2361, 1791, 14656, 29892, 2343, 1829, 29892, 26414, 10767, 21219, 29892, 470, 1040, 666, 362, 29889, 739, 1795, 4556, 596, 10416, 12959, 304, 7910, 29889, 29871, 13, 13, 8439, 526, 12727, 1462, 944, 29879, 393, 2693, 2301, 7799, 29889, 450, 1556, 11828, 1462, 944, 29879, 2048, 2301, 7799, 322, 526, 9045, 29891, 408, 5872, 29889, 7419, 408, 777, 409, 5942, 363, 26823, 29892, 1773, 440, 277, 314, 1144, 29892, 2999, 29899, 21305, 374, 1237, 322, 1880, 5864, 3144, 1682, 852, 29889, 1932, 1310, 366, 6523, 278, 1462, 944, 366, 864, 17386, 372, 6892, 947, 451, 23764, 270, 17006, 29889, 887, 29915, 645, 817, 263, 18254, 768, 2738, 652, 300, 304, 6985, 3867, 596, 3573, 411, 1571, 18254, 29878, 654, 29889, 29871, 13, 13, 8439, 1079, 304, 1234, 278, 937, 1108, 29892, 338, 3077, 2841, 3833, 29895, 1781, 363, 366, 29973, 450, 1650, 338, 22483, 29889, 887, 723, 763, 4996, 261, 4944, 393, 366, 671, 372, 297, 17768, 362, 322, 411, 263, 4226, 664, 449, 366, 769, 508, 679, 278, 2582, 29889, 29871, 13, 13, 6154, 2965, 29968, 379, 27267, 7495, 16999, 1525, 15233, 4936, 29922, 5099, 6778, 6778, 1732, 597, 29894, 277, 6436, 12350, 25762, 29889, 510, 29914, 1688, 29877, 29899, 19790, 29899, 29916, 29914, 13, 13, 2277, 29937, 13291, 29901, 13, 2891, 4448, 29903, 13, 13, 2277, 29937, 2796], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With everything set up, we can move forward to fine-tuning or instruction-tuning Llama-2-7B on our news classification instruction dataset."
      ],
      "metadata": {
        "id": "iOYguJcERvOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating PEFT Configuration\n",
        "\n",
        "\n",
        "Fine-tuning pretrained LLMs on downstream datasets results in huge performance gains when compared to using the pretrained LLMs out-of-the-box. However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!\n",
        "\n",
        "\n",
        "PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. It also helps in portability, wherein users can tune models using PEFT methods to get tiny checkpoints worth a few MB compared to the large checkpoints of full fine-tuning.\n",
        "\n",
        "\n",
        "**In short, PEFT approaches enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.**\n",
        "\n",
        "\n",
        "Hugging Face provides the PEFT library, which provides the latest Parameter-Efficient Fine-tuning techniques seamlessly integrated with Hugging Face Transformers and Hugging Face Accelerate.\n",
        "\n",
        "\n",
        "There are several PEFT methods. In the next cell, we will use QLoRA, one of the latest methods that reduces the memory usage of LLM finetuning without performance tradeoffs, using the `LoraConfig` class from the `peft` library.\n",
        "\n",
        "\n",
        "QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen, and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training."
      ],
      "metadata": {
        "id": "fhI55wj7R9gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
        "    \"\"\"\n",
        "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
        "\n",
        "    :param r: LoRA attention dimension\n",
        "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
        "    :param modules: Names of the modules to apply LoRA to\n",
        "    :param lora_dropout: Dropout Probability for LoRA layers\n",
        "    :param bias: Specifies if the bias parameters should be trained\n",
        "    \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r = r,\n",
        "        lora_alpha = lora_alpha,\n",
        "        target_modules = target_modules,\n",
        "        lora_dropout = lora_dropout,\n",
        "        bias = bias,\n",
        "        task_type = task_type,\n",
        "    )\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "913uHanlnYef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Modules for LoRA Application\n",
        "\n",
        "In the next cell, we will define the `find_all_linear_names` function to find the module to apply LoRA to. This function will get the module names from `model.named_modules()` and store it in a set to keep distinct module names."
      ],
      "metadata": {
        "id": "PVKxwcjPbBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "    \"\"\"\n",
        "    Find modules to apply LoRA to.\n",
        "\n",
        "    :param model: PEFT model\n",
        "    \"\"\"\n",
        "\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:\n",
        "        lora_module_names.remove('lm_head')\n",
        "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "dg8pIUgMm_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Trainable Parameters\n",
        "\n",
        "We can use the `print_trainable_parameters` function to find out the number and percentage of trainable model parameters. This function will calculate the number of total parameters in `model.named_parameters()` and then those that would get updated."
      ],
      "metadata": {
        "id": "AaaQtyB5b_Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model, use_4bit = False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "\n",
        "    :param model: PEFT model\n",
        "    \"\"\"\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "\n",
        "    print(\n",
        "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "F6iEs6pVnCac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning the Pre-trained Model\n",
        "\n",
        "We will create `fine_tune`, our final function, to wrap everything we have done so far and initiate the fine-tuning process. This function will perform the following model preprocessing operations to prepare it for training:\n",
        "\n",
        "\n",
        "1. Enable gradient checkpointing to reduce memory usage during fine-tuning.\n",
        "2. Use the `prepare_model_for_kbit_training` function from PEFT to prepare the model for fine-tuning.\n",
        "3. Call find_all_linear_names` to get the module names to apply LoRA to.\n",
        "4. Create LoRA configuration by calling the `create_peft_config` function.\n",
        "5. Wrap the base Hugging Face model for fine-tuning to PEFT using the `get_peft_model` function.\n",
        "6. Print the trainable parameters.\n",
        "\n",
        "\n",
        "For training, we will instantiate a `Trainer()` object within the `fine_tune` function. This class requires the model, preprocessed dataset, and training arguments, listed below.\n",
        "\n",
        "\n",
        "`per_device_train_batch_size`: The batch size per GPU/TPU/CPU for training.\n",
        "\n",
        "\n",
        "`gradient_accumulation_steps`: Number of update steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "\n",
        "\n",
        "`warmup_steps`: Number of steps used for a linear warmup from 0 to `learning_rate`.\n",
        "\n",
        "\n",
        "`max_steps`: If set to a positive number, the total number of training steps to perform.\n",
        "\n",
        "\n",
        "`learning_rate`: The initial learning rate for Adam.\n",
        "\n",
        "\n",
        "`fp16`: Whether to use 16-bit (mixed) precision training (through NVIDIA apex) instead of 32-bit training.\n",
        "\n",
        "\n",
        "`logging_steps`: Number of update steps between two logs.\n",
        "\n",
        "\n",
        "`output_dir`: The output directory where the model predictions and checkpoints will be written.\n",
        "\n",
        "\n",
        "`optim`: The optimizer to use for training.\n",
        "\n",
        "\n",
        "Next, we will use the `train` method on the trainer` object to start the training and log and save the model metrics on the training dataset. Finally, we will save the model checkpoint (model weights, configuration file, and tokenizer) in the output directory and delete the model to free up memory. You can load the model for inference later using its saved checkpoint."
      ],
      "metadata": {
        "id": "MOICPBjig9ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(model,\n",
        "          tokenizer,\n",
        "          dataset,\n",
        "          lora_r,\n",
        "          lora_alpha,\n",
        "          lora_dropout,\n",
        "          bias,\n",
        "          task_type,\n",
        "          per_device_train_batch_size,\n",
        "          gradient_accumulation_steps,\n",
        "          warmup_steps,\n",
        "          max_steps,\n",
        "          learning_rate,\n",
        "          fp16,\n",
        "          logging_steps,\n",
        "          output_dir,\n",
        "          optim):\n",
        "    \"\"\"\n",
        "    Prepares and fine-tune the pre-trained model.\n",
        "\n",
        "    :param model: Pre-trained Hugging Face model\n",
        "    :param tokenizer: Model tokenizer\n",
        "    :param dataset: Preprocessed training dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Prepare the model for training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Get LoRA module names\n",
        "    target_modules = find_all_linear_names(model)\n",
        "\n",
        "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
        "    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print information about the percentage of trainable parameters\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Training parameters\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        train_dataset = dataset,\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = per_device_train_batch_size,\n",
        "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "            warmup_steps = warmup_steps,\n",
        "            max_steps = max_steps,\n",
        "            learning_rate = learning_rate,\n",
        "            fp16 = fp16,\n",
        "            logging_steps = logging_steps,\n",
        "            output_dir = output_dir,\n",
        "            optim = optim,\n",
        "        ),\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    do_train = True\n",
        "\n",
        "    # Launch training and log metrics\n",
        "    print(\"Training...\")\n",
        "\n",
        "    if do_train:\n",
        "        train_result = trainer.train()\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(metrics)\n",
        "\n",
        "    # Save model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(output_dir, exist_ok = True)\n",
        "    trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "    # Free memory for merging weights\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tCbnhnxtnhvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing QLoRA and TrainingArguments parameters below for training."
      ],
      "metadata": {
        "id": "zP3k_qjEvvSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 16\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 64\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# Bias\n",
        "bias = \"none\"\n",
        "\n",
        "# Task type\n",
        "task_type = \"CAUSAL_LM\""
      ],
      "metadata": {
        "id": "FxC2aY8eUzH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 1\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 20\n",
        "\n",
        "# Linear warmup steps from 0 to learning_rate\n",
        "warmup_steps = 2\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = True\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 1"
      ],
      "metadata": {
        "id": "nnfOKEvipXLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the `fine_tune` function below to fine-tune or instruction-tune the pre-trained model on our preprocessed news classification instruction dataset."
      ],
      "metadata": {
        "id": "C2wHM9mlv3vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune(model,\n",
        "      tokenizer,\n",
        "      preprocessed_dataset,\n",
        "      lora_r,\n",
        "      lora_alpha,\n",
        "      lora_dropout,\n",
        "      bias,\n",
        "      task_type,\n",
        "      per_device_train_batch_size,\n",
        "      gradient_accumulation_steps,\n",
        "      warmup_steps,\n",
        "      max_steps,\n",
        "      learning_rate,\n",
        "      fp16,\n",
        "      logging_steps,\n",
        "      output_dir,\n",
        "      optim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "rRJZ6L7qpPIM",
        "outputId": "f6cc7b71-3f91-497e-e770-4e88ade83c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA module names: ['gate_proj', 'up_proj', 'q_proj', 'v_proj', 'down_proj', 'k_proj', 'o_proj']\n",
            "All Parameters: 3,540,389,888 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291682911958425\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 06:22, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.149300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.991600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.810100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.703000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.604200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.426800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.297600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.524500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.039200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.278300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.165900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.037300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.133200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.434300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.421500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.931400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.308600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =       0.81\n",
            "  total_flos               =  1216695GF\n",
            "  train_loss               =     1.4032\n",
            "  train_runtime            = 0:06:38.71\n",
            "  train_samples_per_second =      0.201\n",
            "  train_steps_per_second   =       0.05\n",
            "{'train_runtime': 398.7129, 'train_samples_per_second': 0.201, 'train_steps_per_second': 0.05, 'total_flos': 1306416521502720.0, 'train_loss': 1.4032274067401886, 'epoch': 0.81}\n",
            "Saving last checkpoint of the model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these steps, we have fine-tuned a popular open-source pre-trained model, Llama-2-7B, on an instruction dataset that we created for news classification!\n",
        "\n",
        "We can see from the log that there are 3,540,389,888 parameters in the model, out of which 39,976,960 are trainable. That's approximately 1% of the total parameters. The model trained for 20 steps and converged at a loss value of 1.4. It is possible that the converged weights are not the best weights. We can fix this by adding `EarlyStoppingCallback` to the `trainer`, which would regularly evaluate the model on a validation dataset and keep only the best weights."
      ],
      "metadata": {
        "id": "071X0VYt0OP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging Weights & Pushing to Hugging Face\n",
        "\n",
        "After saving the fine-tuned weights, we can create our fine-tuned model by merging the fine-tuned weights and saving it to a new directory with its tokenizer. By performing this step, we can have a memory-efficient, fine-tuned model and tokenizer for inference. We will also push the fine-tuned model and its associated tokenizer to Hugging Face Hub for public usage.\n"
      ],
      "metadata": {
        "id": "UXt51ufT2vg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine-tuned weights\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", torch_dtype = torch.bfloat16)\n",
        "# Merge the LoRA layers with the base model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save fine-tuned model at a new location\n",
        "output_merged_dir = \"results/news_classification_llama2_7b/final_merged_checkpoint\"\n",
        "os.makedirs(output_merged_dir, exist_ok = True)\n",
        "model.save_pretrained(output_merged_dir, safe_serialization = True)\n",
        "\n",
        "# Save tokenizer for easy inference\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "id": "kXMHbUeq0gRn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "c4d6dfe7972a4a0faffe3b79b0bdeb16",
            "8e3a169a860344a3867440e5fa3e0fa8",
            "ebc6236f25f04e29aca2397438f76f21",
            "5b52af673f6d42d4a8c951e09f981792",
            "d105d2a398f046888062d4e77f12c279",
            "e56f85c16b5542409d7e0bdee24c0c49",
            "5b0c522274564de7976856e5094b0639",
            "21a0f7eb3e114fcaafede75bde8fcd96",
            "47c84d61f95b4053ad0b89a22081830f",
            "2992d31b4eca4fa0a621b11d1b6702ff",
            "c7cfc76a02624b88868662029ea24e93"
          ]
        },
        "outputId": "b56340c5-69f0-4c4c-c143-277f9c48834e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4d6dfe7972a4a0faffe3b79b0bdeb16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('results/news_classification_llama2_7b/final_merged_checkpoint/tokenizer_config.json',\n",
              " 'results/news_classification_llama2_7b/final_merged_checkpoint/special_tokens_map.json',\n",
              " 'results/news_classification_llama2_7b/final_merged_checkpoint/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "DALr5u8V4drE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f291eaf6-4227-49d2-e463-9e8cfc81e991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "KOsIH8F14fFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece38dea-70e4-41f8-f414-5f8b6055aaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned model name on Hugging Face Hub\n",
        "new_model = \"sahayk/news-classification-18-llama-2-7b\""
      ],
      "metadata": {
        "id": "jS3wjPcd4pSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push fine-tuned model and tokenizer to Hugging Face Hub\n",
        "model.push_to_hub(new_model, use_auth_token = True)\n",
        "tokenizer.push_to_hub(new_model, use_auth_token = True)"
      ],
      "metadata": {
        "id": "x-xPb-_qB0dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "1c4290d5dd9c4a1ba1cd5fd7d66be61b",
            "48ddbc7e8f544a49bd0bc874e1c6792e",
            "c66743df57a94ca989b911de771639b5",
            "c7ea4b4979404af69e3e3dd3e9c7da50",
            "90db7372ff6d4f48afbf1eb3bef7f993",
            "b249be173c9948bf98d334765862af15",
            "a98a8620ee814ce9b5de35065957da2a",
            "b9c36608e47d4f94a8c283dd2a922943",
            "a07ee43ae04e40df85691b81c3609c4a",
            "29a6acd5ce2c4f31ba86ba4ad2691e34",
            "67f248e7a450413584a8ef51d11b180b",
            "5b29994caf9e4c49836bfdd21cfc8fb3",
            "b28a0d635e90438488dda6610a19349b",
            "3a7ad4fde88043a88cc2c3000c333cd6",
            "71a453726b064227a2401a3025ca7c4a",
            "f88af1775c05420584294d5fe9cd037a",
            "cdf5fa571062482f8ec554d5cf93a970",
            "845a185db0204cf9a08006b84225e231",
            "dc7d384e0b5f428b9d123e2f13ec413d",
            "4cf2bdfbea934d6684880392405c64c7",
            "15b45de3122c4241a9c71dc7a5cc7d7f",
            "12bb1a750ee440738093e852387a4ab7",
            "dc5094d073944096a2d6a7243494ccdd",
            "c71e60eb88ba4bacb18dc9201204df24",
            "dbcd6d08f542461ea653a81d755f67fc",
            "f9d3b41ab2034183b2807df610e8ed9c",
            "c84c653b92734591ad57ffd799396a5d",
            "1c862f87d58149938308fbb6dda0ea30",
            "35adabd0c9db41049cf9273371247fbf",
            "de5a254d683d429b99e102239feb41e9",
            "394cb5ff0efa428081621d2a60af6262",
            "6fdb7ccdd963438ea3df2e8ca1fcbddc",
            "6435d0c17a874da1a5fe59f44ebe23f2"
          ]
        },
        "outputId": "838d0297-ab88-461e-816f-607bca33f106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c4290d5dd9c4a1ba1cd5fd7d66be61b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b29994caf9e4c49836bfdd21cfc8fb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc5094d073944096a2d6a7243494ccdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/sahayk/news-classification-18-llama-2-7b/commit/b7d5007d5442dee4eecc81ed3387f504bae92ae4', commit_message='Upload tokenizer', commit_description='', oid='b7d5007d5442dee4eecc81ed3387f504bae92ae4', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the fine-tuned model on Hugging Face: https://huggingface.co/sahayk/news-classification-18-llama-2-7b"
      ],
      "metadata": {
        "id": "wXQNClKDJh9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "**1. https://huggingface.co/**\n",
        "\n",
        "**2. https://huggingface.co/blog**\n",
        "\n",
        "**3. https://www.philschmid.de/**\n",
        "\n",
        "**4. https://blog.ovhcloud.com/**"
      ],
      "metadata": {
        "id": "RR7pMdWaKyn1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2zkyhY5LL15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}