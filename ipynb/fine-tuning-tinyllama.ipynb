{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>Fine Tuning TinyLlama</center>\n    \n    \n<center><img src='https://github.com/jzhang38/TinyLlama/raw/main/.github/TinyLlama_logo.png' height=380px width=380px></center>\n\n## Project Summary\n    \nTinyLlama is a 1.1B Llama model that is currently being trained on 3 trillion tokens, which recently started on September 1st. In this project, I fine-tune the latest version of TinyLlama to generate song lyrics in the style of Taylor Swift. \n\nI used Hugging Face's transformers and peft (parameter-efficient fine-tuning) packages for this project. One of the major challenges of fine-tuning a large language model (LLM) is the high memory usage on the GPU. To address this challenge, I used the quantization and fine-tuning methods described in the 2023 paper \"QLoRA: Efficient Finetuning of Quantized LLMs\". These methods are summarized below:\n\n- Low-rank adaptation: This technique freezes the existing weights of TinyLlama and adds two smaller matrices with lower rank than the weight matrices into the model. Only these two smaller matrices are then trained, instead of all of the model weights. Another way to think of this is that we are grouping weights together and traing a scalar for each group, which is much easier than traing each weight by individually. In addition, low-rank adaptation is only done for the query and values weights in the attention heads of the transformers, while all other areas of the model are frozen. This greatly reduces the computation needed to fine-tune the model, while not impairing performance. \n\n- Double quantization: All weights in TinyLlama are quantized into 4 bits, and the quantization constants are then quantized into 8 bits. This further reduces the memory usage of the model. Low-rank adaptation weights are stored in 16 bits, and model weights are upscaled to 16 bits at computation time. \n\n- NormalFloat data type: The NormalFloat data type is used for quantization. This data type minimizes information loss during quantization by assigning each data point to a quantile bin based on the estimated normal distribution of the data.\n\n- Gradient checkpointing: This technique minimizes the memory storage requirements during training by recalculating some of the gradients from the forward pass instead of storing them all.\n\n- Paged optimizers: This technique enables the CPU to help the GPU with any memory spikes that occur during training, especially when the backward pass reaches a checkpoint. \n\nThese methods collectively enhance the efficiency of the project, enabling the creation of Taylor Swift-style song lyrics while optimizing GPU memory utilization and computational resources.\n\nLink to TinyLlama - https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install bitsandbytes einops wandb -Uqqq","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:30:54.478274Z","iopub.execute_input":"2023-09-13T18:30:54.478962Z","iopub.status.idle":"2023-09-13T18:31:51.481873Z","shell.execute_reply.started":"2023-09-13T18:30:54.478926Z","shell.execute_reply":"2023-09-13T18:31:51.480565Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport glob\nimport pandas as pd\nimport numpy as np\nimport re\nfrom peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\nfrom trl import SFTTrainer\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:31:51.485224Z","iopub.execute_input":"2023-09-13T18:31:51.485617Z","iopub.status.idle":"2023-09-13T18:32:05.194582Z","shell.execute_reply.started":"2023-09-13T18:31:51.485579Z","shell.execute_reply":"2023-09-13T18:32:05.193554Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing the dataset \npath = '/kaggle/input/taylor-swift-song-lyrics-all-albums/'\ncsv_files = glob.glob(path + \"/*.csv\")\ndf_list = (pd.read_csv(i) for i in csv_files)\ndf = pd.concat(df_list, ignore_index=True)\nlyrics = '\\n'.join(df.loc[:,'lyric']) \nprint(lyrics[:200])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.196217Z","iopub.execute_input":"2023-09-13T18:32:05.197224Z","iopub.status.idle":"2023-09-13T18:32:05.335912Z","shell.execute_reply.started":"2023-09-13T18:32:05.197186Z","shell.execute_reply":"2023-09-13T18:32:05.334977Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Knew he was a killer first time that I saw him\nWondered how many girls he had loved and left haunted\nBut if he's a ghost, then I can be a phantom\nHoldin' him for ransom, some\nSome boys are tryin' too \n","output_type":"stream"}]},{"cell_type":"code","source":"# List of all unique characters\nprint(' '.join(sorted(set(lyrics))))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.338568Z","iopub.execute_input":"2023-09-13T18:32:05.339495Z","iopub.status.idle":"2023-09-13T18:32:05.349315Z","shell.execute_reply.started":"2023-09-13T18:32:05.339459Z","shell.execute_reply":"2023-09-13T18:32:05.347798Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n   ! \" & ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z |   é í ï ó е   ​ – — ‘ ’ ” …  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning the file by removing/replacing unnecessary characters and removing sections \n# that are not lyrics\nreplace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\nreplace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'} \nremove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n\ncleaned_lyrics = lyrics\n\nfor old, new in replace_letters.items():\n    cleaned_lyrics = cleaned_lyrics.replace(old, new)\nfor string in remove_list:\n    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\nfor string in replace_with_space:\n    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\nprint(''.join(sorted(set(cleaned_lyrics))))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.350579Z","iopub.execute_input":"2023-09-13T18:32:05.350857Z","iopub.status.idle":"2023-09-13T18:32:05.396171Z","shell.execute_reply.started":"2023-09-13T18:32:05.350822Z","shell.execute_reply":"2023-09-13T18:32:05.395192Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n !',.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz…\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setting aside a portion for training the model and a portion for testing the data to prevent \n# the model from overfitting to the data it is tested on\nsplit_point = int(len(cleaned_lyrics)*0.95)\ntrain_data = cleaned_lyrics[:split_point]\ntest_data = cleaned_lyrics[split_point:]\ntrain_data_seg = []\nfor i in range(0, len(train_data), 500):\n        text = train_data[i:min(i+500, len(train_data))]\n        train_data_seg.append(text)\ntrain_data_seg = Dataset.from_dict({'text':train_data_seg})\nprint(len(train_data_seg))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:32:05.397597Z","iopub.execute_input":"2023-09-13T18:32:05.397924Z","iopub.status.idle":"2023-09-13T18:32:05.420652Z","shell.execute_reply.started":"2023-09-13T18:32:05.397893Z","shell.execute_reply":"2023-09-13T18:32:05.419761Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"557\n","output_type":"stream"}]},{"cell_type":"code","source":"# You will need to create a Hugging Face account if you do not have one, \n# and then generate a write token to enter in the widget below\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:33:58.450608Z","iopub.execute_input":"2023-09-13T18:33:58.451009Z","iopub.status.idle":"2023-09-13T18:33:58.473811Z","shell.execute_reply.started":"2023-09-13T18:33:58.450976Z","shell.execute_reply":"2023-09-13T18:33:58.472688Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad65a6225c0242a39ba80850166a7570"}},"metadata":{}}]},{"cell_type":"code","source":"# Loading the model with double quantization\nmodel_name = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,           \n    bnb_4bit_quant_type=\"nf4\",    \n    bnb_4bit_use_double_quant=True, \n    bnb_4bit_compute_dtype=torch.bfloat16, \n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, \n    device_map=\"auto\",  \n    trust_remote_code=True, \n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:04.854443Z","iopub.execute_input":"2023-09-13T18:34:04.854830Z","iopub.status.idle":"2023-09-13T18:34:09.453336Z","shell.execute_reply.started":"2023-09-13T18:34:04.854799Z","shell.execute_reply":"2023-09-13T18:34:09.452093Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Creating tokenizer and defining the pad token\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:09.455390Z","iopub.execute_input":"2023-09-13T18:34:09.456008Z","iopub.status.idle":"2023-09-13T18:34:09.630490Z","shell.execute_reply.started":"2023-09-13T18:34:09.455942Z","shell.execute_reply":"2023-09-13T18:34:09.629261Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Generating lyrics with the base model. The repetition penalty in the generation config prevents the model from continually repeating the same string.\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndef generate_lyrics(query, model):\n    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n    generation_config = GenerationConfig(max_new_tokens=250, pad_token_id = tokenizer.eos_token_id,repetition_penalty=1.3, eos_token_id = tokenizer.eos_token_id)\n    outputs = model.generate(input_ids=encoding.input_ids, generation_config=generation_config)\n    text_output = tokenizer.decode(outputs[0],skip_special_tokens=True)\n    print('INPUT\\n', query, '\\n\\nOUTPUT\\n', text_output[len(query):])\ngenerate_lyrics(test_data[200:700], model)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:09.631970Z","iopub.execute_input":"2023-09-13T18:34:09.634524Z","iopub.status.idle":"2023-09-13T18:34:27.510143Z","shell.execute_reply.started":"2023-09-13T18:34:09.634491Z","shell.execute_reply":"2023-09-13T18:34:27.509186Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"INPUT\n  to get you where you wanna go\nOh, they didn't teach you that in prep school so it's up to me\nBut no amount of vintage dresses gives you dignity\nThink about what you did\n\nShe's not a saint and she's not what you think\nShe's an actress, whoa\nShe's better known for the things that she does\nOn the mattress, whoa\nSoon she's gonna find stealing other people's toys\nOn the playground won't make you many friends\nShe should keep in mind, she should keep in mind\nThere is nothing I do better than revenge,  \n\nOUTPUT\n \nI don't know why but I feel like I have something to say\nAnd if there was one thing I would change about myself\nIt'd be my name.\n\nSo now we can all just sit back and enjoy this song\nWe could even sing along with her on our own\nIf only we had some money left over from last time\nWell then maybe we could buy her a car\nOr at least give her a ride home\n\n<NAME> - <NAME>, <NAME>. (2013). \"The Best Of The Olsen Twins\". Retrieved April 9, 2018, from https://www.youtube.com/watch?v=g7Yfy-6X5x4&feature=emb_logo\n ---\ntitle: 'Tutorial: Configure Azure IoT Hub | Microsoft Docs'\ndescription: This tutorial shows how to configure your IoT hub using Visual Studio Code.\nservices: iot-hub\ndocumentationcenter: ''\nauthor: dominicbetts\nmanager: timlt\neditor: tysonn\nms.assetid: \nms.service:\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setting arguments for low-rank adaptation \n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_alpha = 32 # The weight matrix is scaled by lora_alpha/lora_rank, so I set lora_alpha = lora_rank to remove scaling\nlora_dropout = 0.05 \nlora_rank = 32 \n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\")\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:27.515727Z","iopub.execute_input":"2023-09-13T18:34:27.518158Z","iopub.status.idle":"2023-09-13T18:34:28.963773Z","shell.execute_reply.started":"2023-09-13T18:34:27.518121Z","shell.execute_reply":"2023-09-13T18:34:28.962709Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Setting training arguments \n\noutput_dir = \"tommyadams/tinyllama\" # Model repo on your hugging face account where you want to save your model\nper_device_train_batch_size = 3\ngradient_accumulation_steps = 2  \noptim = \"paged_adamw_32bit\" \nsave_strategy=\"steps\" \nsave_steps = 10 \nlogging_steps = 10  \nlearning_rate = 2e-3  \nmax_grad_norm = 0.3 # Sets limit for gradient clipping\nmax_steps = 200     # Number of training steps\nwarmup_ratio = 0.03 # Portion of steps used for learning_rate to warmup from 0\nlr_scheduler_type = \"cosine\" # I chose cosine to avoid learning plateaus\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:28.969063Z","iopub.execute_input":"2023-09-13T18:34:28.971384Z","iopub.status.idle":"2023-09-13T18:34:28.982056Z","shell.execute_reply.started":"2023-09-13T18:34:28.971346Z","shell.execute_reply":"2023-09-13T18:34:28.980879Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=train_data_seg,\n    peft_config=peft_config,\n    max_seq_length=500,\n    dataset_text_field='text',\n    tokenizer=tokenizer,\n    args=training_arguments\n)\npeft_model.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:28.983634Z","iopub.execute_input":"2023-09-13T18:34:28.984399Z","iopub.status.idle":"2023-09-13T18:34:29.430658Z","shell.execute_reply.started":"2023-09-13T18:34:28.984361Z","shell.execute_reply":"2023-09-13T18:34:29.429642Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81962dd3e7ae493e9f5b98ff45c2d2f0"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:34:29.435093Z","iopub.execute_input":"2023-09-13T18:34:29.435585Z","iopub.status.idle":"2023-09-13T18:40:20.272515Z","shell.execute_reply.started":"2023-09-13T18:34:29.435549Z","shell.execute_reply":"2023-09-13T18:40:20.271435Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:46, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.829800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.635200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.581500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.616600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.588800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.532900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.510100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.479100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.527700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.299700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.258100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.318700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.287400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.222000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.353700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.248900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.189900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.338700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.186200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.060400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=2.403262138366699, metrics={'train_runtime': 348.0262, 'train_samples_per_second': 3.448, 'train_steps_per_second': 0.575, 'total_flos': 649013488091136.0, 'train_loss': 2.403262138366699, 'epoch': 2.15})"},"metadata":{}}]},{"cell_type":"code","source":"# Generating lyrics with fine-tuned model\ngenerate_lyrics(test_data[200:700], model)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:40:20.273656Z","iopub.execute_input":"2023-09-13T18:40:20.274795Z","iopub.status.idle":"2023-09-13T18:41:06.666649Z","shell.execute_reply.started":"2023-09-13T18:40:20.274757Z","shell.execute_reply":"2023-09-13T18:41:06.665543Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"INPUT\n  to get you where you wanna go\nOh, they didn't teach you that in prep school so it's up to me\nBut no amount of vintage dresses gives you dignity\nThink about what you did\n\nShe's not a saint and she's not what you think\nShe's an actress, whoa\nShe's better known for the things that she does\nOn the mattress, whoa\nSoon she's gonna find stealing other people's toys\nOn the playground won't make you many friends\nShe should keep in mind, she should keep in mind\nThere is nothing I do better than revenge,  \n\nOUTPUT\n 20/10\nAnd if there was one thing I could have done differently\nIt would be this: If I had been around when he broke my heart\nI wouldn't have felt like such a fool then\nIf I hadn't seen him through all those years\nHe never got over his breakup with me\nSo why don't we just say goodbye?\nGoodbye, baby, oh, yeah\nYou know how much I love you\nBaby, oh, yeah\nWe can always re-live our first kiss\nIn your old car, on the way home from work\nWhen you were still single\nNow we are married, but we live apart\nOur lives are intertwined now\nThe only place we feel at peace is inside another person\nThey said, We need to talk\nAbout everything\nEverything\nAll these days, I wondered\nWhat happened between us\nTook away all the fun\nWasn't enough, wasn't enough\nForced her hand into mine\nDidn't want to let go\nOf all the reasons why\nWhy couldn't we ever stay together?\nWell, well, well, well\nDon\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Results\n\nFine-tuning the model for 200 steps on a P100 GPU took about 6 minutes. Before fine-tuning, the model generated a few lines of lyrics in response to the prompt, but then listed some video data from YouTube that it was likely trained on. After fine-tuning, the language model showed improvement in that it learned the common words in Taylor Swift's song lyrics. However, many of the lines were still nonsensical and humorous. To further improve this model, I could start with a larger base model with more parameters (such as Falcon 7b), train the model for longer, and provide longer training segments so that the model can learn song structure in terms of verses and choruses.","metadata":{"execution":{"iopub.status.busy":"2023-09-12T20:25:10.697384Z","iopub.execute_input":"2023-09-12T20:25:10.697755Z","iopub.status.idle":"2023-09-12T20:25:10.703128Z","shell.execute_reply.started":"2023-09-12T20:25:10.697716Z","shell.execute_reply":"2023-09-12T20:25:10.702211Z"}}}]}