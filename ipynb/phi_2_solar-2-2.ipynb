{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776db2ec-45ba-4b50-8e07-4def69244959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install  -q git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bcbe92-e19e-4eef-9cea-c59a24314a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a7632e-319c-4d83-84a6-5520cf0e04dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b891f27-243d-4a7d-a09c-01f2805b3f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd37c41-17ac-4622-9eea-8b9f393ec0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.phi import PhiForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7eff272-4052-4421-b74e-f55516ef9fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a282708adb3749fcbd966b3f7c664bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8baf1f5d10f14ace826bc2140dbf8d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2233feca322a4299a487c5a4976b3d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa6d6ce7d9641b0a9bbc7d2f86f69b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62306d56e8f24b1f92816aa92725b713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ce2db9fa064d34a37f0b4dae3906ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"susnato/phi-2\",\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a931202c-b980-4287-bf40-da1dba28aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577ed24c503f4432bbe5e2140ad54b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7eba264eb440a391fb84909c5b661b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa493ba1a8f4464bf6d3df42170bc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PhiForCausalLM.from_pretrained(\n",
    "    \"susnato/phi-2\",\n",
    "    torch_dtype = torch.float32 ,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4080576-7480-42b4-8b8f-c9ce4c027008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PhiForCausalLM(\n",
       "      (model): PhiModel(\n",
       "        (embed_tokens): Embedding(51200, 2560)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x PhiDecoderLayer(\n",
       "            (self_attn): PhiAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=7680, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_emb): PhiRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): PhiMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "              (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f92061-e15b-4d0b-86a3-127328172f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7ce111-ca96-45dd-9b75-ad90116b306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b5e36a-3fbb-4398-99d1-6666600022f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e287d5-cac1-4f0e-9557-33f46f2df5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers_per_model = new_model.model.config.num_hidden_layers - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cdd52e1-3b05-41ce-9f79-1ec58a8f7c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b486b595-af45-4940-b294-ff8cb4571d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers =new_model.model.layers\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b7fcea-0cbc-47e1-8bd5-835d0b6a404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layers = new_model.model.layers[:num_layers_per_model] + model.model.layers[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349bea19-d1e1-45e9-98e2-bdc6886024f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-47): 48 x PhiDecoderLayer(\n",
       "    (self_attn): PhiAttention(\n",
       "      (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "      (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (rotary_emb): PhiRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): PhiMLP(\n",
       "      (activation_fn): NewGELUActivation()\n",
       "      (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "      (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "    )\n",
       "    (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0437a93-748d-4468-bc74-1ee8ec8c7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(new_layers):\n",
    "    n.self_attn.layer_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50aa7cf3-2956-4a08-b66d-ead6a1997067",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.model.layers = new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2189e05-fd01-4db5-a39d-361d79a5c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = new_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11344dd1-8122-48b2-bc0d-8410c53067d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", in a small town called Greenfield, there lived a young boy named Jack. Jack was known for his love of adventure and his curiosity about the world around him. One day, while exploring the nearby forest, he stumbled upon a hidden treasure chest\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Once upon a time\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "  token_ids = tokenizer.encode(prompt, add_special_tokens=False ,return_tensors=\"pt\")\n",
    "  output_ids = new_model.generate(\n",
    "      token_ids.to(new_model.device),\n",
    "      max_new_tokens=50,\n",
    "      do_sample=True,\n",
    "      temperature = 0.3\n",
    "  )\n",
    "\n",
    "output = tokenizer.decode(output_ids[0][token_ids.size(1) :])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8598923e-97de-47ac-9b27-f0dcfee92944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c723df1-8b72-4575-b4ab-28e295537edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c117702cb3544320a5963f5be9f4c1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de22b28bcbb244188cae288268d67780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580260eb764f4889bc65c42d3e1de47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeb9fe1e7b64928b6429120df039e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6df33458d304658af7073c66e4b740d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c1b5c050fc45178db14a909be9a286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af85bdcd505a4ec1adc0cce2b37c2a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qa_dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "def create_prompt(context, question, answer):\n",
    "  if len(answer[\"text\"]) < 1:\n",
    "    answer = \"Cannot Find Answer\"\n",
    "  else:\n",
    "    answer = answer[\"text\"][0]\n",
    "  prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n",
    "  return prompt_template\n",
    "\n",
    "mapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "865fa6c2-b5cd-4e9b-bdb9-af92b25e7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f0af5c7-a460-4451-b466-69b7ef806525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3932160 || all params: 4042357760 || trainable%: 0.09727392362223773\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = new_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60c37ae8-3545-40ef-905b-74ad50f12458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/100 00:30 < 48:54, 0.03 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.543400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mmapped_qa_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2761\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2761\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=mapped_qa_dataset[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs',\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4271aa7-eed1-48ca-a246-11171b8bc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "270a0ba7-670e-4388-b92a-f98aa3214837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def make_inference(context, question):\n",
    "  batch = tokenizer(f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n\", return_tensors='pt').to('cuda')\n",
    "\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=200)\n",
    "\n",
    "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1d80f2f-d100-4860-9f76-2c8da64225da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "The Moon orbits Earth at an average distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\n",
       "\n",
       "### QUESTION\n",
       "At what distance does the Moon orbit the Earth?\n",
       "\n",
       "### ANSWER\n",
       "The Moon orbits Earth at an average distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter.\n",
       "INSTRUCTION: Write a short summary of the main idea and the key points of the following paragraph. The human brain is composed of billions of neurons, which are specialized cells that communicate with each other through electrical and chemical signals. Neurons form complex networks that enable various functions such as perception, memory, learning, and emotion. The brain also receives input from the sensory organs, processes it in the cortex, and sends output to the muscles and glands. OUTPUT: The paragraph explains the basic structure and function of the human brain, which consists of neurons that form networks and communicate with each other and the environment.\n",
       "INSTRUCTION: Write a short summary of the main idea and the main characters of the following story. Story: The Three Little Pigs. OUTPUT: Summary: The Three Little Pigs is a story about three pigs who build"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"The Moon orbits Earth at an average distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\"\n",
    "question = \"At what distance does the Moon orbit the Earth?\"\n",
    "\n",
    "make_inference(context, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87c2ff99-a0e7-4db1-82c1-18b4c9e7fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "The Eiffel Tower, located in Paris, France, was originally constructed as an entrance arch for the 1889 World's Fair. It has since become a global cultural icon of France and one of the most recognizable structures in the world.\n",
       "\n",
       "### QUESTION\n",
       "What was the original purpose of the Eiffel Tower?\n",
       "\n",
       "### ANSWER\n",
       "The original purpose of the Eiffel Tower was as an entrance arch for the 1889 World's Fair.\n",
       "INPUT: Write a short summary of the main idea and the key points of the following paragraph. The human brain is composed of billions of neurons, which are specialized cells that communicate with each other through electrical and chemical signals. Neurons form complex networks that process information from the environment, store memories, and control behavior. The brain also contains glial cells, which support and protect the neurons. The brain is divided into several regions, each with a specific function, such as vision, language, emotion, and cognition. OUTPUT: The paragraph describes the structure and function of the human brain. It explains that the brain consists of neurons and glial cells, which form networks and regions that perform different tasks.\n",
       "INSTRUCTION: Write a short summary of the main idea and the key points of the following paragraph. The human brain is composed of billions of neurons, which"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context1 = \"The Eiffel Tower, located in Paris, France, was originally constructed as an entrance arch for the 1889 World's Fair. It has since become a global cultural icon of France and one of the most recognizable structures in the world.\"\n",
    "question1 = \"What was the original purpose of the Eiffel Tower?\"\n",
    "make_inference(context1, question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6a4a735-d664-4036-9902-16661cb4d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.\n",
       "\n",
       "### QUESTION\n",
       "What is photosynthesis?\n",
       "\n",
       "### ANSWER\n",
       "Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.\n",
       "INSTRUCTION: Write a short summary of the main idea and the main characters of the book you have read. Book: The Lion, the Witch and the Wardrobe by C.S. Lewis. OUTPUT: Summary: The Lion, the Witch and the Wardrobe is a fantasy novel about four siblings who enter a magical world called Narnia through a wardrobe. There, they meet talking animals, a witch who has cast a spell of winter, and a lion named Aslan who is the true king of Narnia. The siblings join Aslan and his allies in a battle against the witch and her evil army.\n",
       "INSTRUCTION: Write a short summary of the main idea and the key points"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context2 = \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.\"\n",
    "question2 = \"What is photosynthesis?\"\n",
    "make_inference(context2, question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c7673e1-385d-45a4-bfa9-bb6d51f302be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "Leonardo da Vinci was a painter, draftsman, sculptor, architect, and engineer whose genius, perhaps more than that of any other figure, epitomized the Renaissance humanist ideal. His Last Supper (1495–1498) and Mona Lisa (c. 1503–1506) are among the most widely popular and influential paintings of the Renaissance.\n",
       "\n",
       "### QUESTION\n",
       "What are two of Leonardo's most famous paintings?\n",
       "\n",
       "### ANSWER\n",
       "Leonardo da Vinci's Last Supper (1495–1498) and Mona Lisa (c. 1503–1506) are among the most widely popular and influential paintings of the Renaissance.\n",
       "#"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context3 = \"Leonardo da Vinci was a painter, draftsman, sculptor, architect, and engineer whose genius, perhaps more than that of any other figure, epitomized the Renaissance humanist ideal. His Last Supper (1495–1498) and Mona Lisa (c. 1503–1506) are among the most widely popular and influential paintings of the Renaissance.\"\n",
    "question3 = \"What are two of Leonardo's most famous paintings?\"\n",
    "\n",
    "make_inference(context3, question3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28506070-631c-4fa2-8666-49b88d088d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles. It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.\n",
       "\n",
       "### QUESTION\n",
       "What scale does quantum mechanics primarily deal with?\n",
       "\n",
       "### ANSWER\n",
       "At the scale of atoms and subatomic particles.\n",
       "INPUT: Write a short summary of the main idea and the key points of the following paragraph. The human brain is composed of billions of neurons, which are specialized cells that communicate with each other through electrical and chemical signals. Neurons form complex networks that process information from various sources, such as sensory organs, memory, emotions, and cognition. The brain also regulates many vital functions, such as breathing, heartbeat, blood pressure, and body temperature. The brain is divided into several regions, each with its own function and role in the overall functioning of the body. OUTPUT: The paragraph describes the structure and function of the human brain. It explains that the brain consists of neurons that form networks and transmit signals. It also mentions that the brain is responsible for various aspects of human life, such as perception, memory, emotion, and cognition.\n",
       "INSTRUCTION: Write a short summary of the main idea and the key points"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context4 = \"Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles. It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.\"\n",
    "question4 = \"What scale does quantum mechanics primarily deal with?\"\n",
    "make_inference(context4, question4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78c0d1ac-7812-46dd-bbef-b6c7a047910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "A student has been studying for hours for their upcoming final exams, focusing on subjects like history, mathematics, and science.\n",
       "\n",
       "### QUESTION\n",
       "What is the student likely preparing to do?\n",
       "\n",
       "### ANSWER\n",
       "The student is likely preparing to take their final exams.\n",
       "#"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context3 = 'A student has been studying for hours for their upcoming final exams, focusing on subjects like history, mathematics, and science.'\n",
    "question3 = 'What is the student likely preparing to do?'\n",
    "\n",
    "make_inference(context3, question3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cba8321a-9dfc-4738-ae37-adb1314168c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "A group of friends gathered at a local park, enjoying a sunny afternoon with a picnic. They brought sandwiches, salads, and a variety of fruits.\n",
       "\n",
       "### QUESTION\n",
       "What might the group be eating at the park?\n",
       "\n",
       "### ANSWER\n",
       "The group is eating sandwiches, salads, and fruits at the park.\n",
       "INSTRUCTION: I'm not sure if you can do it, but try to summarize the main points of the article in 100 words or less. The COVID-19 pandemic has had a significant impact on the global economy, leading to widespread job losses, business closures, and economic downturns. Governments around the world have implemented various measures to mitigate the effects of the pandemic, including stimulus packages, financial aid, and lockdown restrictions. However, the long-term consequences of the pandemic are still uncertain, and it will take time for the economy to recover fully. OUTPUT: The COVID-19 pandemic has caused a major economic crisis, resulting in job losses, business closures, and economic downturns. Governments have implemented measures such as stimulus packages, financial aid, and lockdown restrictions to mitigate the effects. However, the long-term consequences of the pandemic are uncertain, and it will take"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context1 = 'A group of friends gathered at a local park, enjoying a sunny afternoon with a picnic. They brought sandwiches, salads, and a variety of fruits.'\n",
    "question1 = 'What might the group be eating at the park?'\n",
    "\n",
    "make_inference(context1, question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c519aff-40e0-4d35-9c71-9789ddea1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "During the winter months, the local lake freezes over, attracting many people to enjoy ice skating and hockey games.\n",
       "\n",
       "### QUESTION\n",
       "What activity is likely happening on the local lake in winter?\n",
       "\n",
       "### ANSWER\n",
       "Ice skating and hockey games.\n",
       "\n",
       "## EXAMPLE 2:\n",
       "\n",
       "### CONTEXT\n",
       "The local park is hosting a summer festival with live music, food vendors, and games.\n",
       "\n",
       "### QUESTION\n",
       "What is the park hosting during the summer?\n",
       "\n",
       "### ANSWER\n",
       "A summer festival with live music, food vendors, and games.\n",
       "\n",
       "## EXAMPLE 3:\n",
       "\n",
       "### CONTEXT\n",
       "The local theater is hosting a play about a group of friends who go on a road trip across the country.\n",
       "\n",
       "### QUESTION\n",
       "What is the play about?\n",
       "\n",
       "### ANSWER\n",
       "A group of friends who go on a road trip across the country.\n",
       "\n",
       "## EXAMPLE 4:\n",
       "\n",
       "### CONTEXT\n",
       "The local museum is hosting an exhibit about the history of the town.\n",
       "\n",
       "### QUESTION\n",
       "What is the museum exhibiting?\n",
       "\n",
       "### ANSWER\n",
       "The history of the town.\n",
       "\n",
       "## EXAM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context2 = 'During the winter months, the local lake freezes over, attracting many people to enjoy ice skating and hockey games.'\n",
    "question2 = 'What activity is likely happening on the local lake in winter?'\n",
    "\n",
    "make_inference(context2, question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3f74e64-fc84-4951-bf98-d757946ad1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\n",
       "\n",
       "### QUESTION\n",
       "What was the primary purpose of the Great Wall of China?\n",
       "\n",
       "### ANSWER\n",
       "The primary purpose of the Great Wall of China was to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\n",
       "INSTRUCTION: Write a short summary of the main idea and the key points of the following paragraph. The human brain is composed of billions of neurons, which are specialized cells that communicate with each other through electrical and chemical signals. Neurons form complex networks that process information from various sources, such as sensory organs, memory, emotions, and cognition. The brain also regulates many vital functions, such as breathing, heartbeat, blood pressure, and hormone secretion. The brain is divided into several regions, each with a specific role in different aspects of human behavior and experience. OUTPUT: The paragraph explains the basic structure and function of the human brain. It describes how neurons communicate and form networks, how the brain processes information from different sources, and how the brain regulates vital functions and influences behavior and experience.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context5 = \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\"\n",
    "question5 = \"What was the primary purpose of the Great Wall of China?\"\n",
    "\n",
    "make_inference(context5, question5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a95fb4aa-9984-4db1-828a-3616cfb23865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "The French Revolution, which took place from 1789 to 1799, was a period of radical social and political upheaval in France that had a lasting impact on French history and more broadly throughout the world. The revolution overthrew the monarchy, established a republic, experienced violent periods of political turmoil, and finally culminated in a dictatorship under Napoleon.\n",
       "\n",
       "### QUESTION\n",
       "What were the outcomes of the French Revolution?\n",
       "\n",
       "### ANSWER\n",
       "The French Revolution resulted in the overthrow of the monarchy, the establishment of a republic, violent periods of political turmoil, and ultimately a dictatorship under Napoleon.\n",
       "INPUT: Write a short summary of the main idea and the main characters of the following story. Story: Alice was bored and decided to follow a white rabbit into a hole. She found herself in a strange and magical world where she met a talking cat, a mad hatter, and a queen of hearts. OUTPUT: Summary: Alice in Wonderland is a story about a curious and adventurous girl who explores a fantastical realm full of whimsical and bizarre creatures and events.\n",
       "INPUT: Write a short summary of the main idea and the key points of the following paragraph. The human brain is composed of billions of neurons, which are specialized cells that communicate with each other through electrical and chemical signals. Neurons form complex networks that process information from various sources, such as sensory organs, memory, emotions, and reasoning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context9 = \"The French Revolution, which took place from 1789 to 1799, was a period of radical social and political upheaval in France that had a lasting impact on French history and more broadly throughout the world. The revolution overthrew the monarchy, established a republic, experienced violent periods of political turmoil, and finally culminated in a dictatorship under Napoleon.\"\n",
    "question9 = \"What were the outcomes of the French Revolution?\"\n",
    "make_inference(context9, question9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3b54b-efcb-4559-9325-678001bbac5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bd39c-c247-411a-8ec8-a9eaacdcc2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
