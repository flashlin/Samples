//https://github.com/rse/tokenizr/blob/master/src/tokenizr.js

export class Token {
  constructor(data?: Partial<Token>) {
    Object.assign(this, data);
  }
  type: string = "unknown";
  value: any;
  text: string = "";
  pos: number = 0;
  line: number = 0;
  column: number = 0;
  // toString(colorize?: (type: string, value: string) => string): string
}

export class ParsingError extends Error {
  constructor(data?: Partial<ParsingError>) {
    super();
    Object.assign(this, data);
  }
  name: string;
  message: string;
  pos: number;
  line: number;
  column: number;
  input: string;
  // toString(): string
}

export class ActionContext {
  constructor(tokenizr: Tokenizr) {}
  // data(key: string, value?: any): any
  // info(): { line: number; column: number; pos: number; len: number }
  // push(state: string): this
  // pop(): string
  // state(state: string): this
  // state(): string
  // tag(tag: string): this
  // tagged(tag: string): boolean
  // untag(tag: string): this
  // repeat(): this
  // reject(): this
  // ignore(): this
  // accept(type: string, value?: any): this
  // stop(): this
}

type Action = (
  this: ActionContext,
  ctx: ActionContext,
  match: RegExpExecArray,
  rule: {
    state: string;
    pattern: RegExp;
    action: RuleAction;
    name: string;
  }
) => void;

type RuleAction = (
  this: ActionContext,
  ctx: ActionContext,
  found: RegExpExecArray
) => void;

const excerpt = (txt, o) => {
  const l = txt.length;
  let b = o - 20;
  if (b < 0) b = 0;
  let e = o + 20;
  if (e > l) e = l;
  const hex = (ch) => ch.charCodeAt(0).toString(16).toUpperCase();
  const extract = (txt, pos, len) =>
    txt
      .substr(pos, len)
      .replace(/\\/g, "\\\\")
      .replace(/\x08/g, "\\b")
      .replace(/\t/g, "\\t")
      .replace(/\n/g, "\\n")
      .replace(/\f/g, "\\f")
      .replace(/\r/g, "\\r")
      .replace(/[\x00-\x07\x0B\x0E\x0F]/g, (ch) => "\\x0" + hex(ch))
      .replace(/[\x10-\x1F\x80-\xFF]/g, (ch) => "\\x" + hex(ch))
      .replace(/[\u0100-\u0FFF]/g, (ch) => "\\u0" + hex(ch))
      .replace(/[\u1000-\uFFFF]/g, (ch) => "\\u" + hex(ch));
  return {
    prologTrunc: b > 0,
    prologText: extract(txt, b, o - b),
    tokenText: extract(txt, o, 1),
    epilogText: extract(txt, o + 1, e - (o + 1)),
    epilogTrunc: e < l,
  };
};

export default class Tokenizr {
  constructor() {}

  _input = "";
  _len = 0;
  _eof = false;
  _pos = 0;
  _line = 1;
  _column = 1;
  _state = ["default"];
  _tag = {};
  _transaction = [];
  _pending: Token[] = [];
  _stopped = false;
  _ctx = new ActionContext(this);
  _finish = () => {};
  _rules = [];
  _debug = false;

  reset(): Tokenizr {
    this._input = "";
    this._len = 0;
    this._eof = false;
    this._pos = 0;
    this._line = 1;
    this._column = 1;
    this._state = ["default"];
    this._tag = {};
    this._transaction = [];
    this._pending = [];
    this._stopped = false;
    this._ctx = new ActionContext(this);
    return this;
  }
  // error(message: string): ParsingError
  // debug(debug: boolean): this
  // input(input: string): this
  // push(state: string): this
  // pop(): string
  // state(state: string): this
  // state(): string
  // tag(tag: string): this
  // tagged(tag: string): boolean
  // untag(tag: string): this
  // before(action: Action): this
  // after(action: Action): this
  // finish(action: (this: ActionContext, ctx: ActionContext) => void): this
  // rule(state: string, pattern: RegExp, action: RuleAction, name?: string): this
  // rule(pattern: RegExp, action: RuleAction, name?: string): this
  token(): Token {
    if (this._pending.length === 0) this._tokenize();

    /*  return now potentially pending token  */
    if (this._pending.length > 0) {
      const token = this._pending.shift();
      if (this._transaction.length > 0) this._transaction[0].push(token);
      this._log(`TOKEN: ${token.toString()}`);
      return token;
    }

    /*  no more tokens  */
    return null;
  }
  tokens(): Token[] {
    const result = [];
    let token;
    while ((token = this.token()) !== null) result.push(token);
    return result;
  }
  // peek(offset?: number): Token
  // skip(next?: number): this
  // consume(type: string, value?: string): Token
  // begin(): this
  // depth(): number
  // commit(): this
  // rollback(): this
  // alternatives(...alternatives: ((this: this) => any)[]): any
  // static readonly Token: typeof Token
  // static readonly ParsingError: typeof ParsingError
  // static readonly ActionContext: typeof ActionContext

  _log(msg) {
    /* eslint no-console: off */
    if (this._debug) console.log(`tokenizr: ${msg}`);
  }

  _tokenize() {
    const finish = () => {
      if (!this._eof) {
        if (this._finish !== null) this._finish.call(this._ctx, this._ctx);
        this._eof = true;
        this._pending.push(
          new Token({
            type: "EOF",
            value: "",
            text: "",
            pos: this._pos,
            line: this._line,
            column: this._column,
          })
        );
      }
    };

    if (this._stopped || this._pos >= this._len) {
      finish();
      return;
    }

    /*  loop...  */
    let continued = true;
    while (continued) {
      continued = false;

      /*  some optional debugging context  */
      if (this._debug) {
        const e = excerpt(this._input, this._pos);
        const tags = Object.keys(this._tag)
          .map((tag) => `#${tag}`)
          .join(" ");
        this._log(
          `INPUT: state: <${
            this._state[this._state.length - 1]
          }>, tags: <${tags}>, text: ` +
            (e.prologTrunc ? "..." : '"') +
            `${e.prologText}<${e.tokenText}>${e.epilogText}` +
            (e.epilogTrunc ? "..." : '"') +
            `, at: <line ${this._line}, column ${this._column}>`
        );
      }

      /*  iterate over all rules...  */
      for (let i = 0; i < this._rules.length; i++) {
        if (this._debug) {
          const state = this._rules[i].state
            .map((item) => {
              let output = item.state;
              if (item.tags.length > 0)
                output += " " + item.tags.map((tag) => `#${tag}`).join(" ");
              return output;
            })
            .join(", ");
          this._log(
            `  RULE: state(s): <${state}>, ` +
              `pattern: ${this._rules[i].pattern.source}`
          );
        }

        /*  one of rule's states (and all of its tags) has to match  */
        let matches = false;
        const states = this._rules[i].state.map((item) => item.state);
        let idx = states.indexOf("*");
        if (idx < 0) idx = states.indexOf(this._state[this._state.length - 1]);
        if (idx >= 0) {
          matches = true;
          let tags = this._rules[i].state[idx].tags;
          tags = tags.filter((tag) => !this._tag[tag]);
          if (tags.length > 0) matches = false;
        }
        if (!matches) continue;

        /*  match pattern at the last position  */
        this._rules[i].pattern.lastIndex = this._pos;
        let found = this._rules[i].pattern.exec(this._input);
        this._rules[i].pattern.lastIndex = this._pos;
        if (
          (found = this._rules[i].pattern.exec(this._input)) !== null &&
          found.index === this._pos
        ) {
          if (this._debug) this._log("    MATCHED: " + JSON.stringify(found));

          /*  pattern found, so give action a chance to operate
                   on it and act according to its results  */
          this._ctx._match = found;
          this._ctx._repeat = false;
          this._ctx._reject = false;
          this._ctx._ignore = false;
          if (this._before !== null)
            this._before.call(this._ctx, this._ctx, found, this._rules[i]);
          this._rules[i].action.call(this._ctx, this._ctx, found);
          if (this._after !== null)
            this._after.call(this._ctx, this._ctx, found, this._rules[i]);
          if (this._ctx._reject)
            /*  reject current action, continue matching  */
            continue;
          else if (this._ctx._repeat) {
            /*  repeat matching from scratch  */
            continued = true;
            break;
          } else if (this._ctx._ignore) {
            /*  ignore token  */
            this._progress(this._pos, this._rules[i].pattern.lastIndex);
            this._pos = this._rules[i].pattern.lastIndex;
            if (this._pos >= this._len) {
              finish();
              return;
            }
            continued = true;
            break;
          } else if (this._pending.length > 0) {
            /*  accept token(s)  */
            this._progress(this._pos, this._rules[i].pattern.lastIndex);
            this._pos = this._rules[i].pattern.lastIndex;
            if (this._pos >= this._len) finish();
            return;
          } else
            throw new Error(
              'action of pattern "' +
                this._rules[i].pattern.source +
                '" neither rejected nor accepted any token(s)'
            );
        }
      }
    }

    /*  no pattern matched at all  */
    throw this.error("token not recognized");
  }
}
