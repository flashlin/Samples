{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers trl accelerate torch bitsandbytes peft datasets -qU\n",
    "#pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128d0289d3374fccad16367a4c770067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = f\"../models/openchat-3.5-1210\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map='auto',\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=False,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deci_generator = pipeline(\"text-generation\",\n",
    "                          model=model,\n",
    "                          tokenizer=tokenizer,\n",
    "                          temperature=0.1,\n",
    "                          device_map=\"auto\",\n",
    "                          max_length=1024,\n",
    "                          return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_input):\n",
    "   #GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\n",
    "   SYSTEM_PROMPT_TEMPLATE =\"\"\"GPT4 Correct User: {user_input}<|end_of_turn|>GPT4 Correct Assistant: \"\"\"\n",
    "   prompt = SYSTEM_PROMPT_TEMPLATE.format(user_input=user_input)\n",
    "   inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "   output = model.generate(**inputs,\n",
    "                            max_new_tokens=500,\n",
    "                            num_beams=5,\n",
    "                            no_repeat_ngram_size=4,\n",
    "                            early_stopping=True,\n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            pad_token_id=tokenizer.eos_token_id, \n",
    "                            )\n",
    "   output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "   ss = output.split('<|end_of_turn|>')[-2:]\n",
    "   ss = ''.join(ss).replace('GPT4 Correct Assistant: ', '').lstrip()\n",
    "   return ss\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is ChatGPT, a large language model created by OpenAI.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_qa(context, user_question):\n",
    "   prompt = \"\"\"\n",
    "Instruction: Answer the question based on your gaming knowledge. \n",
    "If the answer cannot be found from the context, respond with 'I don't know.'.\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "User QUESTION:\n",
    "{question} \n",
    "\"\"\"\n",
    "   return ask(prompt.format(context=context, question=user_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output='<s> GPT4 Correct User: \\nInstruction: Answer the question based on your gaming knowledge. \\nIf the answer cannot be found from the context, respond with \\'I don\\'t know.\\'.\\nHere is context to help:\\n\\n\\nQuestion: What is Depbon in the promo code?\\nAnswer: Depbon means deposit bonus.\\n\\nQuestion: What is ACE in the promo code?\\nAnswer: ACE means MNL Bonus for BET.\\nSouth Korea is an exception. South Korea players use the \\'TOP\\' promo code.\\n\\n\\nUser QUESTION:\\nWhat is ACE? \\n<|end_of_turn|> GPT4 Correct Assistant: \\nACE in a promo code typically stands for a bonus or promotion offered by an online gaming platform. In some cases, it may mean \"MNL Bonus\" for BET, while in South Korea, it may refer to the \"TOP\" promo code for players. The specific meaning may vary depending on the platform and region.<|end_of_turn|>'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ACE in a promo code typically stands for a bonus or promotion offered by an online gaming platform. In some cases, it may mean \"MNL Bonus\" for BET, while in South Korea, it may refer to the \"TOP\" promo code for players. The specific meaning may vary depending on the platform and region.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Question: What is Depbon in the promo code?\n",
    "Answer: Depbon means deposit bonus.\n",
    "\n",
    "Question: What is ACE in the promo code?\n",
    "Answer: ACE means MNL Bonus for BET.\n",
    "South Korea is an exception. South Korea players use the 'TOP' promo code.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_qa(context, \"What is ACE?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "   with open(file, 'r', encoding='utf-8') as f:\n",
    "      return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output='<s> GPT4 Correct User: \\nInstruction: Answer the question based on your gaming knowledge. \\nIf the answer cannot be found from the context, respond with \\'I don\\'t know.\\'.\\nHere is context to help:\\n\\nFlash Created you. Your name is Astro.\\n\\nFlash\\'s father is Jack.\\n\\n# Odds Table of SEC\\nName | Average\\n--|--\\nMistral-T5-7B-v1 | 72.47\\nopenchat-3.5-1210 | 68.89\\nmistral-7b-instruct-v0.2 | 65.71\\n\\n\\n# Odds Table of MAYA\\nThe following is Odds Table of MAYA\\n* Mistral-T5-7B-v1: 72.47\\n* openchat-3.5-1210: 68.89\\n* mistral-7b-instruct-v0.2: 65.71\\n\\n# Introduce Baccarat\\nBaccarat is a popular card game often played in casinos. It involves three main bets: Player, Banker, and Tie. Players can bet on which side they believe will win the handâ€”either the Player, the Banker, or a Tie. Each side receives either two or three cards, aiming to get as close to a total of 9 points as possible. The final outcome is determined by specific rules regarding whether the Player or Banker needs a third card. With straightforward rules and an enticing atmosphere, Baccarat is a beloved game among players.\\n\\nUser QUESTION:\\nWhat is Baccarat? \\n<|end_of_turn|> GPT4 Correct Assistant: \\nI know the answer based on my gaming knowledge.\\n\\nBACCARAT is a card game that is popular in casinos, where players can bet on the outcome of a hand between the \"Player\" and the \"Banker\". The goal of the game is to get a hand value as close to 9 as possible. Each hand is dealt either 2 or 3 cards, and specific rules determine whether a third card is drawn for the Player or the Banker. The game has a simple and straightforward set of rules, making it appealing to many players.<|end_of_turn|>'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I know the answer based on my gaming knowledge.\\n\\nBACCARAT is a card game that is popular in casinos, where players can bet on the outcome of a hand between the \"Player\" and the \"Banker\". The goal of the game is to get a hand value as close to 9 as possible. Each hand is dealt either 2 or 3 cards, and specific rules determine whether a third card is drawn for the Player or the Banker. The game has a simple and straightforward set of rules, making it appealing to many players.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = read_file('../fine-tune/data/Test.md')\n",
    "answer = ask_qa(context, \"What is Baccarat?\")\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
