{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at ../models/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    batch_size = 64\n",
    "    num_labels = 2\n",
    "    epochs = 10\n",
    "    num_workers = 2\n",
    "    learning_rate = 3e-5\n",
    "    patience = 2\n",
    "    dropout = 0.5\n",
    "    model_path = \"/kaggle/working\"\n",
    "    max_length = 140\n",
    "    model_save_name = \"model.pt\"\n",
    "    n_folds = 5\n",
    "\n",
    "options = Options()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "   def __init__(self, jsonl_file):\n",
    "      self.jsonl_file = jsonl_file\n",
    "      self.len = 0\n",
    "      with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "         for _ in f:\n",
    "            self.len += 1\n",
    "        \n",
    "   def __getitem__(self, idx):\n",
    "      index = 0\n",
    "      sample = None\n",
    "      with open(self.jsonl_file, 'r', encoding='utf-8') as f:\n",
    "         while index < idx:\n",
    "            line = f.readline()\n",
    "            sample = json.loads(line)\n",
    "            index += 1\n",
    "      return sample\n",
    "    \n",
    "   def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(jsonl_file, options):\n",
    "   dataset = TweetDataset(jsonl_file)\n",
    "   dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                             batch_size=options.batch_size, \n",
    "                                             shuffle=False,\n",
    "                                             num_workers=options.num_workers)\n",
    "   return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_train_data(item, mode='a'):\n",
    "   with open('test.jsonl', mode, encoding='utf-8') as f:\n",
    "      line = json.dumps(item)\n",
    "      f.write(line)\n",
    "      f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_train_data({\n",
    "   'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
    "   'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
    "   'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
    "}, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_train_data({\n",
    "   'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
    "   'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
    "   'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "my_dataset = load_dataset('json', data_files='./test.jsonl')\n",
    "ds2 = load_dataset('json', data_files='./test.jsonl')\n",
    "my_dataset['test'] = ds2['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'context', 'question'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answers', 'context', 'question'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_squad = my_dataset.map(preprocess_function, batched=True, remove_columns=my_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=3,\n",
    "   weight_decay=0.01,\n",
    "   push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 22:11:30] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 22:11:30] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 22:11:30] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 22:11:30] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 22:11:30] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 22:11:32] We saw that you have a 12th Gen Intel(R) Core(TM) i7-12700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 22:11:32] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i7-12700K\n",
      "[codecarbon INFO @ 22:11:32] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 22:11:32]   Platform system: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 22:11:32]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 22:11:32]   CodeCarbon version: 2.2.3\n",
      "[codecarbon INFO @ 22:11:32]   Available RAM : 15.619 GB\n",
      "[codecarbon INFO @ 22:11:32]   CPU count: 20\n",
      "[codecarbon INFO @ 22:11:32]   CPU model: 12th Gen Intel(R) Core(TM) i7-12700K\n",
      "[codecarbon INFO @ 22:11:32]   GPU count: 1\n",
      "[codecarbon INFO @ 22:11:32]   GPU model: 1 x NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_squad[\"train\"],\n",
    "   eval_dataset=tokenized_squad[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.605518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.381020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.259762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 22:12:07] Energy consumed for RAM : 0.000003 kWh. RAM Power : 5.857310771942139 W\n",
      "[codecarbon INFO @ 22:12:07] Energy consumed for all GPUs : 0.000082 kWh. Total GPU Power : 141.364 W\n",
      "[codecarbon INFO @ 22:12:07] Energy consumed for all CPUs : 0.000025 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:12:07] 0.000110 kWh of electricity used since the beginning.\n",
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/codecarbon/output.py:124: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=5.673928578694661, metrics={'train_runtime': 2.0782, 'train_samples_per_second': 15.879, 'train_steps_per_second': 1.444, 'total_flos': 3233664225792.0, 'train_loss': 5.673928578694661, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results/distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.004142954014241695,\n",
       " 'start': 10,\n",
       " 'end': 95,\n",
       " 'answer': '176 billion parameters and can generate text in 46 languages natural languages and 13'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model=\"./results/distilbert\")\n",
    "result = question_answerer(question=question, context=context)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/distilbert\")\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./results/distilbert\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "print(f\"{answer_start_index=}\")\n",
    "print(f\"{answer_end_index=}\")\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=options.learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode=\"min\", \n",
    "                                                         factor=0.5, \n",
    "                                                         patience=2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
