{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers trl accelerate torch bitsandbytes peft datasets -qU\n",
    "#pip install scipy\n",
    "#torch==2.1.2\n",
    "#transformers==4.36.0\n",
    "#trl==0.7.4\n",
    "#accelerate==0.25.0\n",
    "#bitsandbytes==0.41.3.post2\n",
    "#peft==0.7.0\n",
    "#datasets==2.15.0\n",
    "#scipy==1.11.4\n",
    "#sentencepiece==0.1.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_standard.langchain_lit import load_all_documents\n",
    "\n",
    "docs = load_all_documents('./data-user/', 1000 * 7)\n",
    "\n",
    "def read_page_contents_from_docs(docs):\n",
    "   for doc in docs:\n",
    "      yield doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U spacy\n",
    "#python -m spacy download en_core_web_lg==3.7.1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def read_file_segments(file_path):\n",
    "   with open(file_path, 'r', encoding='utf-8') as file:\n",
    "      text = file.read()\n",
    "   doc = nlp(text)\n",
    "   for sentence in doc.sents:\n",
    "      yield sentence.text\n",
    "\n",
    "a = list(read_file_segments('./documents/live-baccarat-doc.md'))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3791723d24b481b87f8dff36f8e7708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = f\"../models/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map='auto',\n",
    "    local_files_only=True,\n",
    "    #trust_remote_code=False,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]While I don't have the ability to create or prepare recipes myself, I can certainly help you find one for mayonnaise! Here's a classic and simple Homemade Mayonnaise recipe you can try:\\n\\nIngredients:\\n- 1 cup (240 ml) light tasting oil (like canola, safflower, or vegetable oil)\\n- 1 large egg yolk\\n- 1 tablespoon (15 ml) white wine vinegar or other mild vinegar\\n- 1 teaspoon (5 g) Dijon mustard\\n- 1/2 teaspoon (3 g) Kosher salt\\n- 1/2 teaspoon (3 g) Freshly ground black pepper\\n\\nInstructions:\\n1. Set up a large bowl with an immersion blender. This will make the process simpler. However, you can also use a regular blender or a whisk.\\n2. Add the egg yolk, vinegar, Dijon mustard, salt, and pepper to the bowl.\\n3. With the immersion blender at the bottom, drizzle the oil into the egg mixture in a slow but steady stream. Turn on the blender and blend the oil and egg mixture together until you have a thick, creamy mayonnaise. The oil should be fully incorporated before adding more.\\n4. Taste the mayonnaise and adjust seasoning if needed. Add any additional herbs, spices or other ingredients according to your preference.\\n5. Store the mayonnaise in an airtight container in the refrigerator. It will keep for up to one week.\\n\\nLet me know if this helps, or if you have any other questions!</s>\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask1():\n",
    "   global model, tokenizer\n",
    "   messages = [\n",
    "      {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "      {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "   ]\n",
    "   model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "   generated_ids = model.generate(model_inputs, \n",
    "                                  max_new_tokens=1000, \n",
    "                                  do_sample=True,\n",
    "                                  pad_token_id=tokenizer.pad_token_id)\n",
    "   decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "   #print(decoded_output[0])\n",
    "   return decoded_output[0]\n",
    "\n",
    "answer = ask1()\n",
    "answer\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have a name. I'm just a computer program designed to help answer questions. You can call me AI or my friend, the assistant. How may I help you today?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lstrip_inst(text):\n",
    "   token = \"[/INST]\"\n",
    "   idx = text.rfind(token)\n",
    "   if idx != -1:\n",
    "      extracted_text = text[idx + len(token):]\n",
    "      return extracted_text.lstrip()\n",
    "   return text\n",
    "\n",
    "def rstrip_s(text):\n",
    "   token = \"</s>\"\n",
    "   idx = text.rfind(token)\n",
    "   if idx != -1:\n",
    "      extracted_text = text[:idx]\n",
    "      return extracted_text\n",
    "   return text\n",
    "\n",
    "\n",
    "def ask(user_input):\n",
    "   global model, tokenizer\n",
    "   messages = [\n",
    "      {\"role\": \"user\", \"content\": user_input}\n",
    "   ]\n",
    "   model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "   generated_ids = model.generate(model_inputs, \n",
    "                                  max_new_tokens=1000, \n",
    "                                  do_sample=True,\n",
    "                                  pad_token_id=tokenizer.pad_token_id)\n",
    "   decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "   answer = decoded_output[0]\n",
    "   answer = lstrip_inst(answer)\n",
    "   return rstrip_s(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ask(\"what is your name?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown_tables_from_content(page_content):\n",
    "   prompt = f\"\"\"{page_content}\n",
    "   ----------\n",
    "   Extract the Markdown table data and its related content from the above content,\n",
    "   and directly describe the data and related content in English.\n",
    "   \"\"\"\n",
    "   answer_markdown = ask(prompt)\n",
    "   return answer_markdown\n",
    "\n",
    "def generate_qa_from_markdown_tables(markdown_content):\n",
    "   prompt_template = \"\"\"{content}\n",
    "   ----------\n",
    "   Based on the above content, extract the Markdown table data and its related content,\n",
    "   and generate corresponding questions and answers directly.\n",
    "   Response Format:\n",
    "   Question: What is your name?\n",
    "   Answer: My name is Astro.\n",
    "   \"\"\"\n",
    "   prompt = prompt_template.format(content=markdown_content)\n",
    "   return ask(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_markdown_table_rows(markdown_content):\n",
    "   prompt_template = \"\"\"{content}\n",
    "   ----------\n",
    "   Based on the above content,\n",
    "   extract each row of data from the Markdown table data,\n",
    "   and generate corresponding questions and answers for each individual row directly.\n",
    "   Response Format:\n",
    "   Question: What is your name?\n",
    "   Answer: My name is Astro.\n",
    "   \"\"\"\n",
    "   prompt = prompt_template.format(content=markdown_content)\n",
    "   return ask(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_extract_terms(page_content):\n",
    "   prompt_template = \"\"\"{content}\n",
    "   ----------\n",
    "   Extract all the common terms from the above content and generate corresponding questions and answers for each one directly.\n",
    "   Response Format:\n",
    "   Question: What is your name?\n",
    "   Answer: My name is Astro.\n",
    "   \"\"\"\n",
    "   prompt = prompt_template.format(content=page_content)\n",
    "   return ask(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_extract_summary(page_content):\n",
    "   prompt_template = \"\"\"{content}\n",
    "   ----------\n",
    "   Summarize the above content and then generate corresponding questions and answers directly.\n",
    "   Response Format:\n",
    "   Question: What is your name?\n",
    "   Answer: My name is Astro.\n",
    "   \"\"\"\n",
    "   prompt = prompt_template.format(content=page_content)\n",
    "   return ask(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_extract_segments(page_content):\n",
    "   prompt_template = \"\"\"{content}\n",
    "   ----------\n",
    "   Segment the above content into appropriate paragraphs,\n",
    "   summarize each segment, Do not output summary, and then generate corresponding questions and answers for each segment directly.\n",
    "   Response Format:\n",
    "   Question: What is your name?\n",
    "   Answer: My name is Astro.\n",
    "   \"\"\"\n",
    "   prompt = prompt_template.format(content=page_content)\n",
    "   return ask(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_extract_values(page_content):\n",
    "   prompt_template = \"\"\"{content}\n",
    "   ----------\n",
    "   Extract each sentence related to numbers from the above content and generate corresponding question-answer pairs directly.\n",
    "   Response Format:\n",
    "   Question: What is your name?\n",
    "   Answer: My name is Astro.\n",
    "   \"\"\"\n",
    "   prompt = prompt_template.format(content=page_content)\n",
    "   return ask(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1 = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = generate_qa_from_extract_summary(docs1)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_qa_file(file_path, text, mode=\"a\"):\n",
    "   with open(file_path, mode, encoding='utf-8') as f:\n",
    "      f.write(text)\n",
    "      f.flush()\n",
    "      \n",
    "def append_delimit(content):\n",
    "   end_str = \"\\r\\n\"\n",
    "   if content.endswith('\\n'):\n",
    "      end_str = \"\"\n",
    "   return f\"{end_str}{content}##########\\r\\n\"\n",
    "   \n",
    "def append_qa_content(sub_qa, content):\n",
    "   sub_qa = append_delimit(sub_qa)\n",
    "   return content + sub_qa\n",
    "      \n",
    "def generate_qa_from_page_content(page_content, write_qa_file_fn=None):\n",
    "   qa_content = \"\"\n",
    "   def write_file(text):\n",
    "      if write_qa_file_fn is None:\n",
    "         return\n",
    "      text = append_delimit(text)\n",
    "      write_qa_file_fn(text)\n",
    "      \n",
    "   def append_to_qa_content(sub_qa):\n",
    "      nonlocal qa_content\n",
    "      write_file(sub_qa)\n",
    "      qa_content = append_qa_content(sub_qa, qa_content)\n",
    "      \n",
    "   sub_qa = generate_qa_from_extract_summary(page_content)\n",
    "   append_to_qa_content(sub_qa)\n",
    "\n",
    "   sub_qa = generate_qa_from_extract_segments(page_content)\n",
    "   append_to_qa_content(sub_qa)\n",
    "\n",
    "   sub_qa = generate_qa_from_extract_terms(page_content)\n",
    "   append_to_qa_content(sub_qa)\n",
    "\n",
    "   markdown_content = extract_markdown_tables_from_content(page_content)\n",
    "   sub_qa = generate_qa_from_markdown_tables(markdown_content)\n",
    "   append_to_qa_content(sub_qa)\n",
    "\n",
    "   sub_qa = generate_qa_from_markdown_table_rows(markdown_content)\n",
    "   append_to_qa_content(sub_qa)\n",
    "\n",
    "   sub_qa = generate_qa_from_extract_values(page_content)\n",
    "   append_to_qa_content(sub_qa)\n",
    "   return qa_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_file = \"./results/llm-qa1.md\"\n",
    "for idx, doc in enumerate(docs):\n",
    "   if idx == 0:\n",
    "      write_qa_file(qa_file, \"\", mode=\"w\")\n",
    "   generate_qa_from_page_content(doc.page_content, \n",
    "                                 lambda content: write_qa_file(qa_file, content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
