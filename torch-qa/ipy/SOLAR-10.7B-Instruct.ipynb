{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_standard.langchain_lit import LlmEmbedding, load_all_documents, create_chroma_vectorstore\n",
    "from py_standard.transformers_lit import create_nf4_model_config\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL = \"bge-base-en\"\n",
    "\n",
    "llm_embedd = LlmEmbedding(f\"../models/{EMB_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_name_path, load_config):\n",
    "   model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_name_path,\n",
    "      quantization_config=load_config,\n",
    "      device_map={\"\": 0},\n",
    "      local_files_only=True,\n",
    "   )\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aa4f3280824a21b5c730194fcc330b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../models/SOLAR-10.7B-Instruct-v1.0 and are newly initialized: ['model.layers.40.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.46.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.47.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.42.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.43.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.45.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.41.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.44.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../models/SOLAR-10.7B-Instruct-v1.0\"\n",
    "\n",
    "llm_model = load_llm_model(MODEL_PATH, create_nf4_model_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(llm_model, tokenizer, user_question):\n",
    "   messages = [\n",
    "      {\"role\": \"user\", \"content\": user_question},\n",
    "   ]\n",
    "   encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "   model_inputs = encodeds.to(llm_model.device)\n",
    "   generated_ids = llm_model.generate(model_inputs, \n",
    "                                  max_new_tokens=1000,\n",
    "                                  pad_token_id=tokenizer.eos_token_id, \n",
    "                                  do_sample=True)\n",
    "   decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "   return decoded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### User:\\nWhat is your name?\\n\\nAnswer: I am a text-based AI assistant, and although I have a designated label or identifier assigned by developers, I do not have a typical name like a human or robot. The most common way to address me is through various forms of invocation like \"AI assistant\", \"computer\", or any custom trigger word assigned by the platform or software in which I operate.</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ask_llm(llm_model, tokenizer, \"What is your name?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm2(llm_model, tokenizer, user_question):\n",
    "   prompt_template =\"\"\"### User:\\n{user_question}\\n### Assistant:\"\"\"\n",
    "   prompt = prompt_template.format(user_question=user_question)\n",
    "   inputs = tokenizer(prompt, return_tensors='pt').to(llm_model.device)\n",
    "   output = llm_model.generate(\n",
    "      **inputs,\n",
    "      max_new_tokens=500,\n",
    "      num_beams=5,\n",
    "      no_repeat_ngram_size=4,\n",
    "      early_stopping=True,\n",
    "      eos_token_id=tokenizer.eos_token_id, \n",
    "      pad_token_id=tokenizer.eos_token_id, \n",
    "   )\n",
    "   output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "   answer = output.replace(prompt, \"\")\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> \\nMy name is AI Assistant. I am an artificial intelligence designed to assist and communicate with users.</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ask_llm2(llm_model, tokenizer, \"What is your name?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_standard.langchain_lit import LlmEmbedding, load_all_documents, create_chroma_vectorstore\n",
    "docs = load_all_documents('./documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_chroma_vectorstore(llm_embedd.embedding, \"sample\")\n",
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is your name?\"\n",
    "sub_docs = vector_store.similarity_search_with_score(question, k=5)\n",
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m load_qa_with_sources_chain(\u001b[43mllm_model\u001b[49m, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_model' is not defined"
     ]
    }
   ],
   "source": [
    "llm_chain = load_qa_with_sources_chain(llm_model, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain({\"input_documents\":sub_docs, \"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
