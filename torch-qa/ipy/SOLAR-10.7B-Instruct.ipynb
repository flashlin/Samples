{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_standard.langchain_lit import LlmEmbedding, load_all_documents, create_chroma_vectorstore\n",
    "from py_standard.transformers_lit import create_nf4_model_config\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain import LLMChain\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL = \"bge-base-en\"\n",
    "\n",
    "llm_embedd = LlmEmbedding(f\"../models/{EMB_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_name_path, load_config):\n",
    "   model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_name_path,\n",
    "      quantization_config=load_config,\n",
    "      device_map={\"\": 0},\n",
    "      local_files_only=True,\n",
    "   )\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aa4f3280824a21b5c730194fcc330b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../models/SOLAR-10.7B-Instruct-v1.0 and are newly initialized: ['model.layers.40.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.46.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.47.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.42.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.43.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.45.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.41.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.44.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../models/SOLAR-10.7B-Instruct-v1.0\"\n",
    "\n",
    "llm_model = load_llm_model(MODEL_PATH, create_nf4_model_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(llm_model, tokenizer, user_question):\n",
    "   messages = [\n",
    "      {\"role\": \"user\", \"content\": user_question},\n",
    "   ]\n",
    "   encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "   model_inputs = encodeds.to(llm_model.device)\n",
    "   generated_ids = llm_model.generate(model_inputs, \n",
    "                                  max_new_tokens=1000,\n",
    "                                  pad_token_id=tokenizer.eos_token_id, \n",
    "                                  do_sample=True)\n",
    "   decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "   return decoded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### User:\\nWhat is your name?\\n\\nAnswer: I am a text-based AI assistant, and although I have a designated label or identifier assigned by developers, I do not have a typical name like a human or robot. The most common way to address me is through various forms of invocation like \"AI assistant\", \"computer\", or any custom trigger word assigned by the platform or software in which I operate.</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ask_llm(llm_model, tokenizer, \"What is your name?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm2(llm_model, tokenizer, user_question):\n",
    "   prompt_template =\"\"\"### User:\\n{user_question}\\n### Assistant:\"\"\"\n",
    "   prompt = prompt_template.format(user_question=user_question)\n",
    "   inputs = tokenizer(prompt, return_tensors='pt').to(llm_model.device)\n",
    "   output = llm_model.generate(\n",
    "      **inputs,\n",
    "      max_new_tokens=500,\n",
    "      num_beams=5,\n",
    "      no_repeat_ngram_size=4,\n",
    "      early_stopping=True,\n",
    "      eos_token_id=tokenizer.eos_token_id, \n",
    "      pad_token_id=tokenizer.eos_token_id, \n",
    "   )\n",
    "   output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "   answer = output.replace(prompt, \"\")\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> \\nMy name is AI Assistant. I am an artificial intelligence designed to assist and communicate with users.</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ask_llm2(llm_model, tokenizer, \"What is your name?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_standard.langchain_lit import LlmEmbedding, load_all_documents, create_chroma_vectorstore\n",
    "docs = load_all_documents('./documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_chroma_vectorstore(llm_embedd.embedding, \"sample\")\n",
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfa2a4c02844b63a6b5afab34592950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='What is your name?\\nMy name is Allen.', metadata={'doc_id': 9, 'source': 'documents/t1.md'}),\n",
       "  0.11545753479003906),\n",
       " (Document(page_content='What is your name?\\nMy name is Astro. Flash created me.\\n\\nOn August 12, 2023, the Eureka team released this feature. In Leo, specific handling was implemented to conceal transactions originating from specific hardcoded salary accounts across two pages (Txn and Sal Txn - Approval Sal Txn).\\n\\nWhat is your name?\\nMy name is Jack.', metadata={'doc_id': 10, 'source': 'documents/t2.md'}),\n",
       "  0.1415218710899353),\n",
       " (Document(page_content='What is your name?\\nMy name is Astro. Flash created me.\\n\\nOn August 12, 2023, the Eureka team released this feature. In Leo, specific handling was implemented to conceal transactions originating from specific hardcoded salary accounts across two pages (Txn and Sal Txn - Approval Sal Txn).', metadata={'doc_id': 11, 'source': 'documents/test.md'}),\n",
       "  0.15703892707824707),\n",
       " (Document(page_content='On August 12, 2023, the Eureka team released this feature. In Leo, specific handling was implemented to conceal transactions originating from specific hardcoded salary accounts across two pages (Txn and Sal Txn - Approval Sal Txn).', metadata={'doc_id': 8, 'source': 'documents/t0.md'}),\n",
       "  0.26068568229675293),\n",
       " (Document(page_content='Error Handling and Robustness:\\n* Are error handling mechanisms implemented effectively for potential exceptions?\\nSecurity and Privacy: \\n* Have security best practices been followed in terms of data handling and storage?\\n* Is sensitive information handled properly, avoiding exposure?\\nFuture Changes and Flexibility:\\n* Has the possibility of future code changes or updates been discussed?\\n* Is the code flexible enough to accommodate future requirements?\\nTesting and Validation:', metadata={'doc_id': 5, 'source': 'documents/code.md'}),\n",
       "  0.2766709327697754)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is your name?\"\n",
    "search_results = vector_store.similarity_search_with_score(question, k=5)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# sub_docs = [Document(page_content=result['page_content'], metadata={\"score\": score}) for result, score in search_results]\n",
    "sub_docs = [result for result, score in search_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "def create_llm_pipe(llm_model, tokenizer):\n",
    "   pipe = pipeline(\n",
    "      \"text-generation\", \n",
    "      model=llm_model, \n",
    "      tokenizer=tokenizer, \n",
    "      max_new_tokens=1024, \n",
    "      eos_token_id=tokenizer.eos_token_id, \n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "   )\n",
    "   hf = HuggingFacePipeline(pipeline=pipe)\n",
    "   return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_chain(llm_pipe):\n",
    "    prompt_template =\"\"\"### User:\\n{user_question}\\n### Assistant:\"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"user_question\"])\n",
    "\n",
    "    llm_chain = LLMChain(\n",
    "        llm=llm_pipe,\n",
    "        prompt=prompt,\n",
    "        verbose=False\n",
    "    )\n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "llm_pipe = create_llm_pipe(llm_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = create_llm_chain(llm_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/tune/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer = llm_chain.run([\"What is your name?\"])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain2 = load_qa_with_sources_chain(llm_pipe, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/tune/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nRefined Answer: The name of the system or component in question cannot be directly provided without specific context or reference. To determine its name, it is essential to examine the codebase, documentation, or consult with developers. Ensure that the system follows best practices for error handling (documents/code.md), security and privacy (documents/security.md), and is flexible enough to accommodate future changes (documents/roadmap.txt). Testing and validation processes should also be in place to ensure the system's robustness and reliability.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is your name?\"\n",
    "resp = llm_chain2({\"input_documents\":sub_docs, \"question\": question})\n",
    "answer = resp['output_text']\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain3 = load_qa_with_sources_chain(llm_pipe, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flash/miniconda3/envs/tune/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You did not provide enough information to determine my name.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is your name?\"\n",
    "resp = llm_chain3({\"input_documents\":sub_docs, \"question\": question})\n",
    "answer = resp['output_text']\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
