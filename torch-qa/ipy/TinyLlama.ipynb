{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM as ModelCls\n",
    "from transformers import LlamaTokenizerFast as TkCls\n",
    "\n",
    "model_name = \"../models/TinyLlama-1.1B-Chat-v0.4\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "   model_name,\n",
    "   device_map=\"auto\",\n",
    "   torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer: TkCls = TkCls.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def dump_json_gz(data, file_path):\n",
    "   with gzip.open(file_path, \"wt\", encoding=\"UTF-8\") as fp:\n",
    "      json.dump(data, fp)\n",
    "      \n",
    "def dump_json(data, file_path):\n",
    "   with open(file_path, \"wt\", encoding=\"UTF-8\") as fp:\n",
    "      json.dump(data, fp, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_input, assistant_output=\"\"):\n",
    "   template_template = \"\"\"<|im_start|>user\n",
    "<|Analysis|>   \n",
    "{user_input}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_output}\"\"\"\n",
    "   return template_template.format(user_input=user_input, assistant_output=assistant_output)\n",
    "\n",
    "def convert_train_row_to_prev_train_row(tokenizer, row):\n",
    "   train_prompt = build_prompt(row['input'], row['output'])\n",
    "   tokens = tokenizer.encode(train_prompt) + [tokenizer.eos_token_id]\n",
    "   return tokens\n",
    "\n",
    "def convert_train_data_to_prev_train_data(tokenizer, ds_data):\n",
    "   result = list()\n",
    "   for row in ds_data:\n",
    "      result.append(convert_train_row_to_prev_train_row(tokenizer, row))\n",
    "   return result\n",
    "\n",
    "# 對訓練資料的 Padding 與推論時 Padding 的方向並不相同，在訓練時通常將 PAD Token 放在右邊，而推論時會放在左邊。\n",
    "def padding_train_dataset(tokenizer, ds_tokens):\n",
    "   maxlen = max(map(len, ds_tokens))\n",
    "   ds_result = list()\n",
    "   for tokens in ds_tokens:\n",
    "      delta = maxlen - len(tokens)\n",
    "      # 將 EOS Token 當作 PAD Token 來用\n",
    "      tokens += [tokenizer.eos_token_id] * delta\n",
    "      ds_result.append({\n",
    "         \"input_ids\": tokens, \n",
    "         \"labels\": tokens\n",
    "      })\n",
    "   return ds_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "   {\n",
    "      \"input\": \"Code Review Short Checklist\\nFor Reviewee:\\n* Engage in discussion with the reviewer regarding the code and requirements.\\n* Take notes of feedback and improvement suggestions.\\n* Address identified improvements before the next review iteration.\\nFor Reviewer:\\n* Offer feedback on strengths and weaknesses of the code.\\n* Give specific suggestions for improvement and guide on addressing issues.\\n* Ensure that feedback helps enhance code quality and align with standards.\",\n",
    "      \"output\": \"person:[\\\"Reviewee\\\",\\\"Reviewer\\\"]\"\n",
    "   },\n",
    "   {\n",
    "      \"input\": \"Reviewee Pre-Review Preparation:\\n* Are all preconditions and prerequisites met for the review?\\n* Do reviewer have access to the necessary documentation and requirements?\",\n",
    "      \"output\": \"person:[\\\"Reviewee\\\"]\"\n",
    "   },\n",
    "   {\n",
    "      \"input\": \"Reviewer Understanding Requirements:\\n* Does the solution align with the defined requirements?\\n* Have Reviewee identified the target audience and their specific needs?\\n* Have security requirements and permission control been confirmed?\",\n",
    "      \"output\": \"person:[\\\"Reviewer\\\"]\"\n",
    "   }\n",
    "]\n",
    "\n",
    "ds_dataset = convert_train_data_to_prev_train_data(tokenizer, train_data)\n",
    "ds_dataset = padding_train_dataset(tokenizer, ds_dataset)\n",
    "\n",
    "dump_json(ds_dataset, \"./results/train.json\")\n",
    "dump_json_gz(ds_dataset, f\"./results/train.json.gz\")\n",
    "dump_json_gz(ds_dataset, f\"./results/dev.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cfb0d68f2d4f3a964ae468b019ad72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3a1e363a584d7faff95dfcf6a382d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93965902f12c4944b74348d2ff231a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4687e03051943d697ac8e765f02c0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import shutil\n",
    "\n",
    "data_files = {\n",
    "   \"train\": \"./results/train.json.gz\",\n",
    "   \"dev\": \"./results/dev.json.gz\",\n",
    "}\n",
    "\n",
    "shutil.rmtree(\"./cache/\")\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "   \"json\",\n",
    "   data_files=data_files,\n",
    "   cache_dir=\"cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 設定訓練參數\n",
    "output_dir = \"./results/TinyLlama-1B-Trained\"\n",
    "train_args = TrainingArguments(\n",
    "   output_dir=output_dir,\n",
    "   per_device_train_batch_size=2,\n",
    "   per_device_eval_batch_size=2,\n",
    "   eval_accumulation_steps=2,\n",
    "   evaluation_strategy=\"steps\",\n",
    "   save_strategy=\"steps\",\n",
    "   eval_steps=25,\n",
    "   save_steps=25,\n",
    "   save_total_limit=3,\n",
    "   num_train_epochs=3,\n",
    "   load_best_model_at_end=True,\n",
    "   bf16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:17:51] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:17:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:17:51] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:17:51] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:17:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 17:17:53] We saw that you have a 12th Gen Intel(R) Core(TM) i7-12700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 17:17:53] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i7-12700K\n",
      "[codecarbon INFO @ 17:17:53] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:17:53]   Platform system: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 17:17:53]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 17:17:53]   CodeCarbon version: 2.2.3\n",
      "[codecarbon INFO @ 17:17:53]   Available RAM : 15.619 GB\n",
      "[codecarbon INFO @ 17:17:53]   CPU count: 20\n",
      "[codecarbon INFO @ 17:17:53]   CPU model: 12th Gen Intel(R) Core(TM) i7-12700K\n",
      "[codecarbon INFO @ 17:17:53]   GPU count: 1\n",
      "[codecarbon INFO @ 17:17:53]   GPU model: 1 x NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:18:04] Energy consumed for RAM : 0.000014 kWh. RAM Power : 5.857309341430664 W\n",
      "[codecarbon INFO @ 17:18:04] Energy consumed for all GPUs : 0.000190 kWh. Total GPU Power : 77.496 W\n",
      "[codecarbon INFO @ 17:18:04] Energy consumed for all CPUs : 0.000104 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 17:18:04] 0.000308 kWh of electricity used since the beginning.\n",
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/codecarbon/output.py:124: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=2.4738148053487143, metrics={'train_runtime': 8.8139, 'train_samples_per_second': 1.021, 'train_steps_per_second': 0.681, 'total_flos': 7206456066048.0, 'train_loss': 2.4738148053487143, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# 開始訓練模型\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存訓練完的模型\n",
    "MODEL_TUNED = \"./results/TinyLlama-1B\"\n",
    "trainer.save_model(MODEL_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-31 23:13:48 config.py:467] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-31 23:13:48 llm_engine.py:73] Initializing an LLM engine with config: model='./results/TinyLlama-1B', tokenizer='./results/TinyLlama-1B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-31 23:16:34 llm_engine.py:223] # GPU blocks: 21423, # CPU blocks: 11915\n",
      "WARNING 12-31 23:16:34 cache_engine.py:96] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 12-31 23:16:34 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-31 23:16:38 model_runner.py:437] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_TUNED = \"./results/TinyLlama-1B\"\n",
    "llm = LLM(MODEL_TUNED, dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "   max_tokens=512,\n",
    "   temperature=0.0,\n",
    "   stop=[\"}\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = build_prompt(\"What is your name?\")\n",
    "results = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=2, prompt='<|im_start|>user\\n<|Analysis|>   \\nWhat is your name?\\n<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[1, 32001, 1404, 13, 29966, 29989, 21067, 4848, 29989, 29958, 1678, 13, 5618, 338, 596, 1024, 29973, 13, 32002, 29871, 13, 32001, 20255, 13], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='Hi, I am Open Assistant. How can I help you today?<|im_end|> \\n', token_ids=[18567, 29892, 306, 626, 4673, 4007, 22137, 29889, 1128, 508, 306, 1371, 366, 9826, 29973, 32002, 29871, 13, 2], cumulative_logprob=-5.01611244052674, logprobs=None, finish_reason=stop)], finished=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, I am Open Assistant. How can I help you today?<|im_end|> \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = results[0].outputs[0].text\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
