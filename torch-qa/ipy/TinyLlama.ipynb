{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM as ModelCls\n",
    "from transformers import LlamaTokenizerFast as TkCls\n",
    "\n",
    "model_name = \"../models/TinyLlama-1.1B-Chat-v0.4\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "   model_name,\n",
    "   device_map=\"auto\",\n",
    "   torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tk: TkCls = TkCls.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_prompt(user_input, assistant_output):\n",
    "   template_template = \"\"\"<|im_start|>user\n",
    "<|Analysis|>   \n",
    "{user_input}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_output}\"\"\"\n",
    "   return template_template.format(user_input=user_input, assistant_output=assistant_output)\n",
    "\n",
    "def convert_train_row_to_prev_train_row(tokenizer, row):\n",
    "   train_prompt = build_train_prompt(row['input'], row['output'])\n",
    "   tokens = tokenizer.encode(train_prompt) + [tokenizer.eos_token_id]\n",
    "   return tokens\n",
    "\n",
    "def convert_train_data_to_prev_train_data(tokenizer, ds_data):\n",
    "   result = list()\n",
    "   for row in ds_data:\n",
    "      result.append(convert_train_row_to_prev_train_row(tokenizer, row))\n",
    "   return result\n",
    "\n",
    "# 對訓練資料的 Padding 與推論時 Padding 的方向並不相同，在訓練時通常將 PAD Token 放在右邊，而推論時會放在左邊。\n",
    "def padding_train_dataset(tokenizer, ds_tokens):\n",
    "   maxlen = max(map(len, ds_tokens))\n",
    "   ds_result = list()\n",
    "   for tokens in ds_tokens:\n",
    "      delta = maxlen - len(tokens)\n",
    "      # 將 EOS Token 當作 PAD Token 來用\n",
    "      tokens += [tokenizer.eos_token_id] * delta\n",
    "      ds_result.append({\n",
    "         \"input_ids\": tokens, \n",
    "         \"labels\": tokens\n",
    "      })\n",
    "   return ds_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "   {\n",
    "      \"input\": \"Code Review Short Checklist\\nFor Reviewee:\\n* Engage in discussion with the reviewer regarding the code and requirements.\\n* Take notes of feedback and improvement suggestions.\\n* Address identified improvements before the next review iteration.\\nFor Reviewer:\\n* Offer feedback on strengths and weaknesses of the code.\\n* Give specific suggestions for improvement and guide on addressing issues.\\n* Ensure that feedback helps enhance code quality and align with standards.\",\n",
    "      \"output\": \"person:[\\\"Reviewee\\\",\\\"Reviewer\\\"]\"\n",
    "   },\n",
    "   {\n",
    "      \"input\": \"Reviewee Pre-Review Preparation:\\n* Are all preconditions and prerequisites met for the review?\\n* Do reviewer have access to the necessary documentation and requirements?\",\n",
    "      \"output\": \"person:[\\\"Reviewee\\\"]\"\n",
    "   },\n",
    "   {\n",
    "      \"input\": \"Reviewer Understanding Requirements:\\n* Does the solution align with the defined requirements?\\n* Have Reviewee identified the target audience and their specific needs?\\n* Have security requirements and permission control been confirmed?\",\n",
    "      \"output\": \"person:[\\\"Reviewer\\\"]\"\n",
    "   }\n",
    "]\n",
    "\n",
    "ds_dataset = convert_train_data_to_prev_train_data(train_data)\n",
    "dump_json_gz(ds_dataset, f\"{ds_type}.tokens.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "data_files = {\n",
    "   \"train\": \"train.tokens.json.gz\",\n",
    "   \"dev\": \"dev.tokens.json.gz\",\n",
    "}\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "   \"json\",\n",
    "   data_files=data_files,\n",
    "   cache_dir=\"cache\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
