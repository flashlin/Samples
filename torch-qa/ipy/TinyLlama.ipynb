{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM as ModelCls\n",
    "from transformers import LlamaTokenizerFast as TkCls\n",
    "\n",
    "model_name = \"../models/TinyLlama-1.1B-Chat-v0.4\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "   model_name,\n",
    "   device_map=\"auto\",\n",
    "   torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer: TkCls = TkCls.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def dump_json_gz(data, file_path):\n",
    "   with gzip.open(file_path, \"wt\", encoding=\"UTF-8\") as fp:\n",
    "      json.dump(data, fp)\n",
    "      \n",
    "def dump_json(data, file_path):\n",
    "   with open(file_path, \"wt\", encoding=\"UTF-8\") as fp:\n",
    "      json.dump(data, fp, ensure_ascii=False, indent=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_prompt(user_input, assistant_output):\n",
    "   template_template = \"\"\"<|im_start|>user\n",
    "<|Analysis|>   \n",
    "{user_input}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_output}\"\"\"\n",
    "   return template_template.format(user_input=user_input, assistant_output=assistant_output)\n",
    "\n",
    "def convert_train_row_to_prev_train_row(tokenizer, row):\n",
    "   train_prompt = build_train_prompt(row['input'], row['output'])\n",
    "   tokens = tokenizer.encode(train_prompt) + [tokenizer.eos_token_id]\n",
    "   return tokens\n",
    "\n",
    "def convert_train_data_to_prev_train_data(tokenizer, ds_data):\n",
    "   result = list()\n",
    "   for row in ds_data:\n",
    "      result.append(convert_train_row_to_prev_train_row(tokenizer, row))\n",
    "   return result\n",
    "\n",
    "# 對訓練資料的 Padding 與推論時 Padding 的方向並不相同，在訓練時通常將 PAD Token 放在右邊，而推論時會放在左邊。\n",
    "def padding_train_dataset(tokenizer, ds_tokens):\n",
    "   maxlen = max(map(len, ds_tokens))\n",
    "   ds_result = list()\n",
    "   for tokens in ds_tokens:\n",
    "      delta = maxlen - len(tokens)\n",
    "      # 將 EOS Token 當作 PAD Token 來用\n",
    "      tokens += [tokenizer.eos_token_id] * delta\n",
    "      ds_result.append({\n",
    "         \"input_ids\": tokens, \n",
    "         \"labels\": tokens\n",
    "      })\n",
    "   return ds_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "   {\n",
    "      \"input\": \"Code Review Short Checklist\\nFor Reviewee:\\n* Engage in discussion with the reviewer regarding the code and requirements.\\n* Take notes of feedback and improvement suggestions.\\n* Address identified improvements before the next review iteration.\\nFor Reviewer:\\n* Offer feedback on strengths and weaknesses of the code.\\n* Give specific suggestions for improvement and guide on addressing issues.\\n* Ensure that feedback helps enhance code quality and align with standards.\",\n",
    "      \"output\": \"person:[\\\"Reviewee\\\",\\\"Reviewer\\\"]\"\n",
    "   },\n",
    "   {\n",
    "      \"input\": \"Reviewee Pre-Review Preparation:\\n* Are all preconditions and prerequisites met for the review?\\n* Do reviewer have access to the necessary documentation and requirements?\",\n",
    "      \"output\": \"person:[\\\"Reviewee\\\"]\"\n",
    "   },\n",
    "   {\n",
    "      \"input\": \"Reviewer Understanding Requirements:\\n* Does the solution align with the defined requirements?\\n* Have Reviewee identified the target audience and their specific needs?\\n* Have security requirements and permission control been confirmed?\",\n",
    "      \"output\": \"person:[\\\"Reviewer\\\"]\"\n",
    "   }\n",
    "]\n",
    "\n",
    "ds_dataset = convert_train_data_to_prev_train_data(tokenizer, train_data)\n",
    "ds_dataset = padding_train_dataset(tokenizer, ds_dataset)\n",
    "\n",
    "dump_json(ds_dataset, \"./results/train.json\")\n",
    "dump_json_gz(ds_dataset, f\"./results/train.json.gz\")\n",
    "dump_json_gz(ds_dataset, f\"./results/dev.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cfb0d68f2d4f3a964ae468b019ad72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3a1e363a584d7faff95dfcf6a382d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93965902f12c4944b74348d2ff231a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4687e03051943d697ac8e765f02c0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import shutil\n",
    "\n",
    "data_files = {\n",
    "   \"train\": \"./results/train.json.gz\",\n",
    "   \"dev\": \"./results/dev.json.gz\",\n",
    "}\n",
    "\n",
    "shutil.rmtree(\"./cache/\")\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "   \"json\",\n",
    "   data_files=data_files,\n",
    "   cache_dir=\"cache\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
