{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_peft_model(base_model: str, peft_model: str):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        quantization_config=bnb_config,\n",
    "        #device_map=\"auto\",\n",
    "        device_map=\"cuda:0\",\n",
    "        # trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        # use_safetensors=True\n",
    "    )\n",
    "    if peft_model is not None:\n",
    "        if os.path.exists(f\"{peft_model}/adapter_config.json\"):\n",
    "            print(f\"Loading PEFT {peft_model}\")\n",
    "            model.load_adapter(peft_model)\n",
    "        else:\n",
    "            print(\"WARNING: PEFT_MODEL NOT EXISTS!!!\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama2_generation_prompt(system_message, question: str):\n",
    "    if system_message is not None:\n",
    "        return (\"<s>[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "                .format(system_message=system_message, user_input=question))\n",
    "    prompt_template = \"\"\"<s>[INST] {user_input} [/INST]\"\"\"\n",
    "    return prompt_template.format(user_input=question)\n",
    "\n",
    "\n",
    "def ask_llama2_instruction_prompt(model, generation_config, tokenizer, device, question: str):\n",
    "    system_msg = (\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "        \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "        \"Please ensure that your responses are socially unbiased and positive in nature.\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "        \"If you don't know the answer to a question, please don't share false information.\")\n",
    "    prompt = create_llama2_generation_prompt(system_msg, question)\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return resp.replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self):\n",
    "        model, tokenizer, generation_config = self.load_model()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generation_config = generation_config\n",
    "        self.device = 'cuda'\n",
    "\n",
    "    def load_model(self):\n",
    "        base_model = f\"../models/{MODEL_NAME}\"\n",
    "        model, tokenizer = load_peft_model(base_model, None)\n",
    "        generation_config = model.generation_config\n",
    "        generation_config.max_new_tokens = 1024\n",
    "        generation_config.temperature = 0.2\n",
    "        generation_config.do_sample = True\n",
    "        generation_config.top_p = 0.9\n",
    "        generation_config.num_return_sequences = 1\n",
    "        generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "        generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        return model, tokenizer, generation_config\n",
    "\n",
    "    def ask(self, user_input: str):\n",
    "        answer = ask_llama2_instruction_prompt(model=self.model,\n",
    "                                               generation_config=self.generation_config,\n",
    "                                               tokenizer=self.tokenizer,\n",
    "                                               device=self.device,\n",
    "                                               question=user_input)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
