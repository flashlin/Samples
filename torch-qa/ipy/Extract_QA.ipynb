{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers==4.34.0\n",
    "#pip install git+https://github.com/huggingface/transformers\n",
    "# pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extract_QA_Utils import load_markdown_document, split_documents\n",
    "\n",
    "docs = load_markdown_document('./Extract_QA.md')\n",
    "all_splits = split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for doc in all_splits:\n",
    "   all_docs.append({\n",
    "      'page_content': doc.page_content,\n",
    "      'source': doc.metadata['source']\n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_peft_model(base_model: str, peft_model: str):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        quantization_config=bnb_config,\n",
    "        #device_map=\"auto\",\n",
    "        device_map=\"cuda:0\",\n",
    "        # trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        # use_safetensors=True\n",
    "    )\n",
    "    if peft_model is not None:\n",
    "        if os.path.exists(f\"{peft_model}/adapter_config.json\"):\n",
    "            print(f\"Loading PEFT {peft_model}\")\n",
    "            model.load_adapter(peft_model)\n",
    "        else:\n",
    "            print(\"WARNING: PEFT_MODEL NOT EXISTS!!!\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama2_generation_prompt(system_message, question: str):\n",
    "    if system_message is not None:\n",
    "        return (\"<s>[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "                .format(system_message=system_message, user_input=question))\n",
    "    prompt_template = \"\"\"<s>[INST] {user_input} [/INST]\"\"\"\n",
    "    return prompt_template.format(user_input=question)\n",
    "\n",
    "\n",
    "def ask_llama2_instruction_prompt(model, generation_config, tokenizer, device, question: str):\n",
    "    system_msg = (\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "        \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "        \"Please ensure that your responses are socially unbiased and positive in nature.\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "        \"If you don't know the answer to a question, please don't share false information.\")\n",
    "    prompt = create_llama2_generation_prompt(system_msg, question)\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return resp.replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self):\n",
    "        model, tokenizer, generation_config = self.load_model()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generation_config = generation_config\n",
    "        self.device = 'cuda'\n",
    "\n",
    "    def load_model(self):\n",
    "        base_model = f\"../models/{MODEL_NAME}\"\n",
    "        model, tokenizer = load_peft_model(base_model, None)\n",
    "        generation_config = model.generation_config\n",
    "        generation_config.max_new_tokens = 3048\n",
    "        generation_config.temperature = 0.7\n",
    "        generation_config.do_sample = True\n",
    "        generation_config.top_p = 2\n",
    "        generation_config.num_return_sequences = 1\n",
    "        generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "        generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        return model, tokenizer, generation_config\n",
    "\n",
    "    def ask(self, user_input: str):\n",
    "        answer = ask_llama2_instruction_prompt(model=self.model,\n",
    "                                               generation_config=self.generation_config,\n",
    "                                               tokenizer=self.tokenizer,\n",
    "                                               device=self.device,\n",
    "                                               question=user_input)\n",
    "        \n",
    "        idx = answer.find('[/INST]')\n",
    "        answer = answer[idx+7:]\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"{content}\n",
    "----------\n",
    "According to the above content, please generate several questions,\n",
    "the answer of which must contain all or part of the content above.\n",
    "The total content of the generated questions and answers must cover at least 80% of the above content.\n",
    "Please do not generate duplicate questions.\n",
    "Every question number must be prefixed with the keyword \"Question\".\n",
    "When generating questions, do not give me the question number.\n",
    "Response example:\n",
    "   Question: How to play\n",
    "   Answer: You muse stardy\n",
    "   Question: Hi\n",
    "   Answer: Hello \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file: str):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "content = read_file('./Extract_QA.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d666973980c7453d8649180de130b89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(content):\n",
    "   prompt = PROMPT.format(content=content)\n",
    "   answer = llm.ask(prompt)\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT1 = \"\"\"{content}\n",
    "----------\n",
    "According to the above content, \n",
    "Please summarize while retaining essential metadata and information from the original text.\n",
    "Generate questions and answers from the summary; the answers must be present in the original content above.\n",
    "Every question number must be prefixed with the keyword \"Question\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PROMPT1.format(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(content):\n",
    "   template = \"\"\"{content}\n",
    "   ----------\n",
    "   According to the above content, \n",
    "   Please summarize while retaining essential metadata and information from the original text.\n",
    "   \"\"\"\n",
    "   answer = llm.ask(template.format(content=content))\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_file(text, mode='a'):\n",
    "   with open('./Extract_QA-Results.md', mode, encoding='utf-8') as f:\n",
    "      f.write(text)\n",
    "      f.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = all_docs[0]['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_content = summarize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_file(\"#summarize\")\n",
    "append_to_file(summarize_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_for_summarize = generate_qa(summarize_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_file(\"# qa for summarize\")\n",
    "append_to_file(qa_for_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(content):\n",
    "   template = \"\"\"{content}\n",
    "   ----------\n",
    "   According to the above content, \n",
    "   Extract several NLP Topics, list them, with each Topic on a separate line.\n",
    "   Do not number each line.\n",
    "   \"\"\"\n",
    "   answer = llm.ask(template.format(content=content))\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Live dealer baccarat game\n",
      "Objective of the game\n",
      "Player's hand vs. banker's hand\n",
      "Betting on a tie\n",
      "Betting on the player's hand\n",
      "Betting on the banker's hand\n",
      "Betting on a tie\n",
      "Game presented with a live person dealing the cards\n",
      "Theoretical return to player of the game\n",
      "Average return to the player of the game over a long period of time\n"
     ]
    }
   ],
   "source": [
    "topics = extract_topics(content)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_topics(content, topics):\n",
    "   template = \"\"\"{content}\n",
    "   ----------\n",
    "   According to the above content, \n",
    "   Generate questions based on these TOPICS; the answers must be present in the above content.\n",
    "   {topics}\n",
    "   Just provide the questions and answers,\n",
    "   Each question should be prefaced with 'Question:' and each answer with 'Answer:'\n",
    "   \"\"\"\n",
    "   answer = llm.ask(template.format(content=content, topics=topics))\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_for_topics = generate_qa_from_topics(content, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_file(\"# qa for topics\")\n",
    "append_to_file(qa_for_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_category(content):\n",
    "   template = \"\"\"{content}\n",
    "   ----------\n",
    "Extract all contrasting NLP entities from the content of the article. \n",
    "List out relevant questions and answers highlighting the differences for each contrasting entity.\n",
    "Just provide the questions and answers,\n",
    "Each question should be prefaced with 'Question:' and each answer with 'Answer:'\n",
    "   \"\"\"\n",
    "   answer = llm.ask(template.format(content=content))\n",
    "   return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_category = generate_category(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_file(\"# qa for category\")\n",
    "append_to_file(qa_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_qa(content):\n",
    "   summarize_content = summarize(content)\n",
    "   append_to_file(\"# summarize\")\n",
    "   append_to_file(summarize_content)\n",
    "   topics = extract_topics(content)\n",
    "   qa_for_topics = generate_qa_from_topics(content, topics)\n",
    "   append_to_file(\"# qa for topics\")\n",
    "   append_to_file(qa_for_topics)\n",
    "   qa_category = generate_category(content)\n",
    "   append_to_file(\"# qa for category\")\n",
    "   append_to_file(qa_category)\n",
    "   append_to_file(\"----------\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_docs)=9\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(all_docs)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in all_docs:\n",
    "   content = doc['page_content']\n",
    "   generate_all_qa(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
