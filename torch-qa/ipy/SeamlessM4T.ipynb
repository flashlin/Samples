{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q fairseq2==0.1.0 gradio==3.40.1\n",
    "#!pip install -q git+https://github.com/camenduru/seamless_communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone -b dev https://github.com/camenduru/seamless_m4t-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fairseq2==0.1 pydub yt-dlp\n",
    "%git clone https://github.com/facebookresearch/seamless_communication.git\n",
    "%cd seamless_communication\n",
    "%git checkout 01c1042841f9bce66902eb2c7512dbdd71d42112 # We will use a stable version; if you want to use the latest version, comment out this line.\n",
    "%pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/facebookresearch/seamless_communication.git\n",
    "# copy seamless_communication/src 裡面的 seamless_communication 資料夾到 /ipy/ 目錄\n",
    "#pip install torch==2.1.1 \n",
    "#pip install torchaudio\n",
    "#%pip install fairseq2==0.2 pydub yt-dlp\n",
    "#conda install -c conda-forge libsndfile==1.0.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seamless_communication.inference import Translator\n",
    "from IPython.display import Audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub import AudioSegment\n",
    "import torchaudio\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_with_max_duration(input_file, output_directory, min_silence_len=2500, silence_thresh=-60, max_chunk_duration=15000):\n",
    "    sound = AudioSegment.from_wav(input_file)\n",
    "    # Splitting on silence\n",
    "    audio_chunks = split_on_silence(sound, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "    # split for max_chunk_duration\n",
    "    final_audio_chunks = []\n",
    "    for chunk in audio_chunks:\n",
    "        if len(chunk) > max_chunk_duration:\n",
    "            num_subchunks = len(chunk) // max_chunk_duration + 1\n",
    "            subchunk_size = len(chunk) // num_subchunks\n",
    "            for i in range(num_subchunks):\n",
    "                start_idx = i * subchunk_size\n",
    "                end_idx = (i + 1) * subchunk_size\n",
    "                subchunk = chunk[start_idx:end_idx]\n",
    "                final_audio_chunks.append(subchunk)\n",
    "        else:\n",
    "            final_audio_chunks.append(chunk)\n",
    "    # Export wav\n",
    "    for i, chunk in enumerate(final_audio_chunks):\n",
    "        output_file = f\"{output_directory}/chunk{i}.wav\"\n",
    "        print(\"Exporting file\", output_file)\n",
    "        chunk.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_play_audio(path_save, audio, sample_rate):\n",
    "    torchaudio.save(\n",
    "        path_save,\n",
    "        audio[0].cpu(),\n",
    "        sample_rate=sample_rate,\n",
    "    )\n",
    "\n",
    "    audio_play = Audio(path_save, rate=sample_rate, autoplay=True, normalize=True)\n",
    "    display(audio_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssetError",
     "evalue": "The checkpoint of seamlessM4T_v2_large_local cannot be loaded. See nested exception for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/fairseq2/models/utils/generic_loaders.py:239\u001b[0m, in \u001b[0;36mModelLoader.__call__\u001b[0;34m(self, model_name_or_card, device, dtype, out, force, progress)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     checkpoint \u001b[39m=\u001b[39m load_checkpoint(\n\u001b[1;32m    240\u001b[0m         path,\n\u001b[1;32m    241\u001b[0m         map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    242\u001b[0m         restrict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestrict_checkpoints,\n\u001b[1;32m    243\u001b[0m         converter\u001b[39m=\u001b[39;49mcheckpoint_converter,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIOError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/fairseq2/models/utils/checkpoint.py:63\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(pathname, map_location, restrict, converter)\u001b[0m\n\u001b[1;32m     61\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     checkpoint: Mapping[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\n\u001b[1;32m     64\u001b[0m         \u001b[39mstr\u001b[39;49m(pathname), map_location, weights_only\u001b[39m=\u001b[39;49mrestrict\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m converter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/../models/seamless-m4t-v2-large/seamlessM4T_v2_large.pt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAssetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m translator \u001b[39m=\u001b[39m Translator(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model_name_or_card\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseamlessM4T_v2_large_local\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     vocoder_name_or_card\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvocoder_v2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     apply_mintox\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/inference/translator.py:113\u001b[0m, in \u001b[0;36mTranslator.__init__\u001b[0;34m(self, model_name_or_card, vocoder_name_or_card, device, text_tokenizer, apply_mintox, dtype, input_modality, output_modality)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    111\u001b[0m     dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m--> 113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m load_unity_model(model_name_or_card, device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    115\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, UnitYModel)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/fairseq2/models/utils/generic_loaders.py:246\u001b[0m, in \u001b[0;36mModelLoader.__call__\u001b[0;34m(self, model_name_or_card, device, dtype, out, force, progress)\u001b[0m\n\u001b[1;32m    239\u001b[0m     checkpoint \u001b[39m=\u001b[39m load_checkpoint(\n\u001b[1;32m    240\u001b[0m         path,\n\u001b[1;32m    241\u001b[0m         map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    242\u001b[0m         restrict\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestrict_checkpoints,\n\u001b[1;32m    243\u001b[0m         converter\u001b[39m=\u001b[39mcheckpoint_converter,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIOError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m AssetError(\n\u001b[1;32m    247\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe checkpoint of \u001b[39m\u001b[39m{\u001b[39;00mcard\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m cannot be loaded. See nested exception for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     model \u001b[39m=\u001b[39m out\n",
      "\u001b[0;31mAssetError\u001b[0m: The checkpoint of seamlessM4T_v2_large_local cannot be loaded. See nested exception for details."
     ]
    }
   ],
   "source": [
    "translator = Translator(\n",
    "    model_name_or_card=\"seamlessM4T_v2_large_local\",\n",
    "    vocoder_name_or_card=\"vocoder_v2\",\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    apply_mintox=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video\n",
    "video_url = 'www.youtube.com/watch?v=g_9rPvbENUw'\n",
    "!yt-dlp -f \"mp4\"  --force-overwrites --max-downloads 1 --no-warnings --no-abort-on-error --ignore-no-formats-error --restrict-filenames -o Video.mp4  $video_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to wav\n",
    "!ffmpeg -y -i Video.mp4 -vn -acodec pcm_s16le -ar 44100 -ac 2 audio.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio_file = \"/content/seamless_communication/audio.wav\"\n",
    "output_directory = \"/content/seamless_communication/split_segments\"\n",
    "\n",
    "!mkdir split_segments\n",
    "!rm -rf /content/seamless_communication/split_segments/*\n",
    "split_audio_with_max_duration(input_audio_file, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a split\n",
    "audio_path = '/content/seamless_communication/split_segments/chunk1.wav'\n",
    "audio = Audio(audio_path, rate=44100, autoplay=True, normalize=True)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Speech to Speech Translate\n",
    "translated_text, wav, sr = translator.predict(\n",
    "    input='/content/seamless_communication/split_segments/chunk1.wav',\n",
    "    task_str='s2st',\n",
    "    tgt_lang='eng', # target language\n",
    "    src_lang='spa', # source language # If you specify this, it will improve the model's result.\n",
    "    spkr= -1,\n",
    ")\n",
    "\n",
    "# Save the audio and play\n",
    "save_and_play_audio(\n",
    "    '/content/seamless_communication/audiot.wav',\n",
    "    wav,\n",
    "    sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will translate all the segments and combine them into a new audio file.\n",
    "segments = []\n",
    "\n",
    "for filename in sorted(os.listdir(output_directory)):\n",
    "    if filename.startswith(\"chunk\") and filename.endswith(\".wav\"):\n",
    "        segment_path = os.path.join(output_directory, filename)\n",
    "\n",
    "        translated_text, wav, sr = translator.predict(\n",
    "            input=segment_path,\n",
    "            task_str='s2st',\n",
    "            tgt_lang='eng',\n",
    "            src_lang='spa',\n",
    "        )\n",
    "        print(translated_text, segment_path)\n",
    "\n",
    "        torchaudio.save(\n",
    "            segment_path,\n",
    "            wav[0].cpu(),\n",
    "            sample_rate=sr,\n",
    "        )\n",
    "\n",
    "        segment = AudioSegment.from_file(segment_path)\n",
    "        segments.append(segment)\n",
    "\n",
    "    combined_audio = sum(segments)\n",
    "    combined_audio.export('/content/seamless_communication/audio_eng.mp3', format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '/content/seamless_communication/audio_eng.mp3'\n",
    "audio = Audio(audio_path, rate=44100, autoplay=True, normalize=True)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text to Speech Translate\n",
    "text = 'En el bosque encantado'\n",
    "translated_text, wav, sr = translator.predict(\n",
    "    text,\n",
    "    \"t2st\",\n",
    "    tgt_lang='eng',\n",
    "    src_lang='spa'\n",
    ")\n",
    "\n",
    "save_and_play_audio(\n",
    "    '/content/seamless_communication/text2speech.wav',\n",
    "    wav,\n",
    "    sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to Text Translate\n",
    "text = 'En el bosque encantado, un zorro curioso halló un reloj antiguo. Al tocarlo, quedó atrapado en un bucle temporal. Buscó ayuda de un búho sabio, quien reveló que solo resolviendo acertijos podría romper el hechizo. Juntos descifraron enigmas, liberando al zorro y tejiendo una amistad eterna.'\n",
    "translated_text, _, _ = translator.predict(text, \"t2tt\", 'eng', src_lang='spa')\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speech to text translate\n",
    "# Resample audio\n",
    "resample_rate = 44100\n",
    "waveform, sample_rate = torchaudio.load('/content/seamless_communication/split_segments/chunk1.wav')\n",
    "resampler = torchaudio.transforms.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "resampled_waveform = resampler(waveform)\n",
    "torchaudio.save('/content/seamless_communication/split_segments/resample_chunk1.wav', resampled_waveform, resample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text, _, _ = translator.predict('/content/seamless_communication/split_segments/resample_chunk1.wav', \"s2tt\", 'eng')\n",
    "translated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
