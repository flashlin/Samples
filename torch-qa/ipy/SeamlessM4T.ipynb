{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q fairseq2==0.1.0 gradio==3.40.1\n",
    "#!pip install -q git+https://github.com/camenduru/seamless_communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone -b dev https://github.com/camenduru/seamless_m4t-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fairseq2==0.1 pydub yt-dlp\n",
    "# %git clone https://github.com/facebookresearch/seamless_communication.git\n",
    "# %cd seamless_communication\n",
    "# %git checkout 01c1042841f9bce66902eb2c7512dbdd71d42112 # We will use a stable version; if you want to use the latest version, comment out this line.\n",
    "# %pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/facebookresearch/seamless_communication.git\n",
    "# copy seamless_communication/src 裡面的 seamless_communication 資料夾到 /ipy/ 目錄\n",
    "#pip install torch==2.1.1 \n",
    "#pip install torchaudio\n",
    "#%pip install fairseq2==0.2 pydub yt-dlp\n",
    "#conda install -c conda-forge libsndfile==1.0.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seamless_communication.inference import Translator\n",
    "from IPython.display import Audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub import AudioSegment\n",
    "import torchaudio\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_with_max_duration(input_file, output_directory, min_silence_len=2500, silence_thresh=-60, max_chunk_duration=15000):\n",
    "    sound = AudioSegment.from_wav(input_file)\n",
    "    # Splitting on silence\n",
    "    audio_chunks = split_on_silence(sound, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "    # split for max_chunk_duration\n",
    "    final_audio_chunks = []\n",
    "    for chunk in audio_chunks:\n",
    "        if len(chunk) > max_chunk_duration:\n",
    "            num_subchunks = len(chunk) // max_chunk_duration + 1\n",
    "            subchunk_size = len(chunk) // num_subchunks\n",
    "            for i in range(num_subchunks):\n",
    "                start_idx = i * subchunk_size\n",
    "                end_idx = (i + 1) * subchunk_size\n",
    "                subchunk = chunk[start_idx:end_idx]\n",
    "                final_audio_chunks.append(subchunk)\n",
    "        else:\n",
    "            final_audio_chunks.append(chunk)\n",
    "    # Export wav\n",
    "    for i, chunk in enumerate(final_audio_chunks):\n",
    "        output_file = f\"{output_directory}/chunk{i}.wav\"\n",
    "        print(\"Exporting file\", output_file)\n",
    "        chunk.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_play_audio(path_save, audio, sample_rate):\n",
    "    torchaudio.save(\n",
    "        path_save,\n",
    "        audio[0].cpu(),\n",
    "        sample_rate=sample_rate,\n",
    "    )\n",
    "\n",
    "    audio_play = Audio(path_save, rate=sample_rate, autoplay=True, normalize=True)\n",
    "    display(audio_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large_local. Set `force` to `True` to download again.\n",
      "Using the cached etox dataset. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of mintox. Set `force` to `True` to download again.\n",
      "/home/flash/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(\n",
    "    model_name_or_card=\"seamlessM4T_v2_large_local\",\n",
    "    vocoder_name_or_card=\"vocoder_v2_local\",\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    apply_mintox=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video\n",
    "video_url = 'www.youtube.com/watch?v=g_9rPvbENUw'\n",
    "!yt-dlp -f \"mp4\"  --force-overwrites --max-downloads 1 --no-warnings --no-abort-on-error --ignore-no-formats-error --restrict-filenames -o Video.mp4  $video_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to wav\n",
    "!ffmpeg -y -i Video.mp4 -vn -acodec pcm_s16le -ar 44100 -ac 2 audio.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio_file = \"/content/seamless_communication/audio.wav\"\n",
    "output_directory = \"/content/seamless_communication/split_segments\"\n",
    "\n",
    "!mkdir split_segments\n",
    "!rm -rf /content/seamless_communication/split_segments/*\n",
    "split_audio_with_max_duration(input_audio_file, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a split\n",
    "audio_path = '/content/seamless_communication/split_segments/chunk1.wav'\n",
    "audio = Audio(audio_path, rate=44100, autoplay=True, normalize=True)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Speech to Speech Translate\n",
    "translated_text, wav, sr = translator.predict(\n",
    "    input='/content/seamless_communication/split_segments/chunk1.wav',\n",
    "    task_str='s2st',\n",
    "    tgt_lang='eng', # target language\n",
    "    src_lang='spa', # source language # If you specify this, it will improve the model's result.\n",
    "    spkr= -1,\n",
    ")\n",
    "\n",
    "# Save the audio and play\n",
    "save_and_play_audio(\n",
    "    '/content/seamless_communication/audiot.wav',\n",
    "    wav,\n",
    "    sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will translate all the segments and combine them into a new audio file.\n",
    "segments = []\n",
    "\n",
    "for filename in sorted(os.listdir(output_directory)):\n",
    "    if filename.startswith(\"chunk\") and filename.endswith(\".wav\"):\n",
    "        segment_path = os.path.join(output_directory, filename)\n",
    "\n",
    "        translated_text, wav, sr = translator.predict(\n",
    "            input=segment_path,\n",
    "            task_str='s2st',\n",
    "            tgt_lang='eng',\n",
    "            src_lang='spa',\n",
    "        )\n",
    "        print(translated_text, segment_path)\n",
    "\n",
    "        torchaudio.save(\n",
    "            segment_path,\n",
    "            wav[0].cpu(),\n",
    "            sample_rate=sr,\n",
    "        )\n",
    "\n",
    "        segment = AudioSegment.from_file(segment_path)\n",
    "        segments.append(segment)\n",
    "\n",
    "    combined_audio = sum(segments)\n",
    "    combined_audio.export('/content/seamless_communication/audio_eng.mp3', format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '/content/seamless_communication/audio_eng.mp3'\n",
    "audio = Audio(audio_path, rate=44100, autoplay=True, normalize=True)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text to Speech Translate\n",
    "text = 'En el bosque encantado'\n",
    "translated_text, wav, sr = translator.predict(\n",
    "    text,\n",
    "    \"t2st\",\n",
    "    tgt_lang='eng',\n",
    "    src_lang='spa'\n",
    ")\n",
    "\n",
    "save_and_play_audio(\n",
    "    '/content/seamless_communication/text2speech.wav',\n",
    "    wav,\n",
    "    sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CString('In the enchanted forest, a curious fox found an ancient clock. When he touched it, he was trapped in a time loop. He sought help from a wise owl, who revealed that only by solving riddles could he break the spell. Together they deciphered riddles, freeing the fox and weaving an eternal friendship.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Text Translate\n",
    "text = 'En el bosque encantado, un zorro curioso halló un reloj antiguo. Al tocarlo, quedó atrapado en un bucle temporal. Buscó ayuda de un búho sabio, quien reveló que solo resolviendo acertijos podría romper el hechizo. Juntos descifraron enigmas, liberando al zorro y tejiendo una amistad eterna.'\n",
    "translated_text, _ = translator.predict(text, \"t2tt\", 'eng', src_lang='spa')\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to Text Translate\n",
    "text = 'SeamlessM4T is our foundational all-in-one Massively Multilingual and Multimodal Machine Translation model delivering high-quality translation for speech and text in nearly 100 languages.'\n",
    "translated_text, _ = translator.predict(text, \"t2tt\", 'cmn', src_lang='eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeamlessM4T是我们基础的全方位大规模多语言和多模式机器翻译模型,为近100种语言的语音和文本提供高质量翻译.\n"
     ]
    }
   ],
   "source": [
    "a = translated_text[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speech to text translate\n",
    "# Resample audio\n",
    "resample_rate = 44100\n",
    "waveform, sample_rate = torchaudio.load('/mnt/d/AI-mp4/TheMayor.wav')\n",
    "resampler = torchaudio.transforms.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "resampled_waveform = resampler(waveform)\n",
    "torchaudio.save('/content/seamless_communication/split_segments/resample_chunk1.wav', resampled_waveform, resample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 142.53 GiB. GPU 0 has a total capacty of 12.00 GiB of which 909.47 MiB is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 8.31 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m translated_text, a, b \u001b[39m=\u001b[39m translator\u001b[39m.\u001b[39;49mpredict(\u001b[39m'\u001b[39;49m\u001b[39m/mnt/d/demo/AI-mp4/TheMayor.wav\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39ms2tt\u001b[39;49m\u001b[39m\"\u001b[39;49m, src_lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkor\u001b[39;49m\u001b[39m'\u001b[39;49m, tgt_lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcmn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/d/VDisk/Github/Samples/torch-qa/ipy/SeamlessM4T.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m translated_text\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/inference/translator.py:319\u001b[0m, in \u001b[0;36mTranslator.predict\u001b[0;34m(self, input, task_str, tgt_lang, src_lang, text_generation_opts, unit_generation_opts, spkr, sample_rate, unit_generation_ngram_filtering, duration_factor, prosody_encoder_input, src_text)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m unit_generation_opts \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     unit_generation_opts \u001b[39m=\u001b[39m SequenceGeneratorOptions(\n\u001b[1;32m    316\u001b[0m         beam_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, soft_max_seq_len\u001b[39m=\u001b[39m(\u001b[39m25\u001b[39m, \u001b[39m50\u001b[39m)\n\u001b[1;32m    317\u001b[0m     )\n\u001b[0;32m--> 319\u001b[0m texts, units \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_prediction(\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_tokenizer,\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munit_tokenizer,\n\u001b[1;32m    323\u001b[0m     seqs,\n\u001b[1;32m    324\u001b[0m     padding_mask,\n\u001b[1;32m    325\u001b[0m     input_modality,\n\u001b[1;32m    326\u001b[0m     output_modality,\n\u001b[1;32m    327\u001b[0m     tgt_lang,\n\u001b[1;32m    328\u001b[0m     text_generation_opts,\n\u001b[1;32m    329\u001b[0m     unit_generation_opts,\n\u001b[1;32m    330\u001b[0m     unit_generation_ngram_filtering\u001b[39m=\u001b[39;49munit_generation_ngram_filtering,\n\u001b[1;32m    331\u001b[0m     duration_factor\u001b[39m=\u001b[39;49mduration_factor,\n\u001b[1;32m    332\u001b[0m     prosody_encoder_input\u001b[39m=\u001b[39;49mprosody_encoder_input,\n\u001b[1;32m    333\u001b[0m )\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_mintox \u001b[39mand\u001b[39;00m task_str \u001b[39m!=\u001b[39m Task\u001b[39m.\u001b[39mASR\u001b[39m.\u001b[39mname:\n\u001b[1;32m    336\u001b[0m     \u001b[39mif\u001b[39;00m input_modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mSPEECH:\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/inference/translator.py:188\u001b[0m, in \u001b[0;36mTranslator.get_prediction\u001b[0;34m(cls, model, text_tokenizer, unit_tokenizer, seqs, padding_mask, input_modality, output_modality, tgt_lang, text_generation_opts, unit_generation_opts, unit_generation_ngram_filtering, duration_factor, prosody_encoder_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m     unit_generation_opts \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    179\u001b[0m generator \u001b[39m=\u001b[39m UnitYGenerator(\n\u001b[1;32m    180\u001b[0m     model,\n\u001b[1;32m    181\u001b[0m     text_tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     unit_opts\u001b[39m=\u001b[39munit_generation_opts,\n\u001b[1;32m    186\u001b[0m )\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m generator(\n\u001b[1;32m    189\u001b[0m     seqs,\n\u001b[1;32m    190\u001b[0m     padding_mask,\n\u001b[1;32m    191\u001b[0m     input_modality\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    192\u001b[0m     output_modality\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    193\u001b[0m     ngram_filtering\u001b[39m=\u001b[39;49munit_generation_ngram_filtering,\n\u001b[1;32m    194\u001b[0m     duration_factor\u001b[39m=\u001b[39;49mduration_factor,\n\u001b[1;32m    195\u001b[0m     prosody_encoder_input\u001b[39m=\u001b[39;49mprosody_encoder_input,\n\u001b[1;32m    196\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/inference/generator.py:261\u001b[0m, in \u001b[0;36mUnitYGenerator.__call__\u001b[0;34m(self, source_seqs, source_padding_mask, input_modality, output_modality, ngram_filtering, duration_factor, prosody_encoder_input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m:param source_seqs:\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m    The source sequences to use for generation. *Shape:* :math:`(N,S,*)`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m    - The output of the unit generator.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m input_modality \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspeech\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     texts, text_gen_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms2t_converter\u001b[39m.\u001b[39;49mbatch_convert(\n\u001b[1;32m    262\u001b[0m         source_seqs, source_padding_mask\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m \u001b[39melif\u001b[39;00m input_modality \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt2t_converter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/fairseq2/generation/text.py:152\u001b[0m, in \u001b[0;36mSequenceToTextConverter.batch_convert\u001b[0;34m(self, source_seqs, source_padding_mask)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(source_seqs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`source_seqs` must contain at least one element, but is empty instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_convert(source_seqs, source_padding_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/fairseq2/generation/text.py:91\u001b[0m, in \u001b[0;36mSequenceToTextConverterBase._do_convert\u001b[0;34m(self, source_seqs, source_padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m# (S) -> (N, S)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m target_prefix_seqs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_prefix_seq\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m generator_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator(\n\u001b[1;32m     92\u001b[0m     source_seqs, source_padding_mask, target_prefix_seqs, \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m texts: List[StringLike] \u001b[39m=\u001b[39m []\n\u001b[1;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m idx, hypotheses \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(generator_output\u001b[39m.\u001b[39mhypotheses):\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/fairseq2/generation/beam_search.py:252\u001b[0m, in \u001b[0;36mBeamSearchSeq2SeqGenerator.__call__\u001b[0;34m(self, source_seqs, source_padding_mask, prompt_seqs, prompt_padding_mask)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m@finaloverride\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39minference_mode()\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Seq2SeqGeneratorOutput:\n\u001b[1;32m    251\u001b[0m     \u001b[39m# (P, S)\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     encoder_output, encoder_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m    253\u001b[0m         source_seqs, source_padding_mask\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m source_padding_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m         max_source_len \u001b[39m=\u001b[39m source_seqs\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/models/unity/model.py:230\u001b[0m, in \u001b[0;36mUnitYX2TModel.encode\u001b[0;34m(self, seqs, padding_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m@finaloverride\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m, seqs: Tensor, padding_mask: Optional[PaddingMask]\n\u001b[1;32m    228\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Optional[PaddingMask]]:\n\u001b[1;32m    229\u001b[0m     seqs, padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_frontend(seqs, padding_mask)\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(seqs, padding_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/models/unity/adaptor_block.py:103\u001b[0m, in \u001b[0;36mUnitYEncoderAdaptor.forward\u001b[0;34m(self, seqs, padding_mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m@finaloverride\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     seqs: Tensor,\n\u001b[1;32m    101\u001b[0m     padding_mask: Optional[PaddingMask],\n\u001b[1;32m    102\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Optional[PaddingMask]]:\n\u001b[0;32m--> 103\u001b[0m     seqs, padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(seqs, padding_mask)\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_layer_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         seqs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_layer_norm(seqs)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/models/wav2vec2_chunk/encoder.py:87\u001b[0m, in \u001b[0;36mChunkTransformerEncoder.forward\u001b[0;34m(self, seqs, padding_mask)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreliminary_dropout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     seqs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreliminary_dropout(seqs)\n\u001b[0;32m---> 87\u001b[0m self_attn_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn_mask_factory(seqs)\n\u001b[1;32m     89\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mdrop_iter()):\n",
      "File \u001b[0;32m/mnt/d/VDisk/Github/Samples/torch-qa/ipy/seamless_communication/models/wav2vec2_chunk/chunk_attention_mask.py:70\u001b[0m, in \u001b[0;36mChunkAttentionMaskFactory.__call__\u001b[0;34m(self, seqs)\u001b[0m\n\u001b[1;32m     64\u001b[0m end_indices \u001b[39m=\u001b[39m end_indices\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, seq_len)\n\u001b[1;32m     66\u001b[0m indices \u001b[39m=\u001b[39m (\n\u001b[1;32m     67\u001b[0m     torch\u001b[39m.\u001b[39marange(seq_len, device\u001b[39m=\u001b[39mseqs\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand(seq_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m bool_mask \u001b[39m=\u001b[39m (indices \u001b[39m<\u001b[39;49m start_indices) \u001b[39m|\u001b[39m (indices \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_indices)\n\u001b[1;32m     72\u001b[0m mask \u001b[39m=\u001b[39m to_float_mask(bool_mask, seqs\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     74\u001b[0m mask \u001b[39m=\u001b[39m mask[:seq_len, :seq_len]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 142.53 GiB. GPU 0 has a total capacty of 12.00 GiB of which 909.47 MiB is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 8.31 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "translated_text, a, b = translator.predict('/mnt/d/demo/AI-mp4/TheMayor.wav', \"s2tt\", src_lang='kor', tgt_lang='cmn')\n",
    "translated_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
