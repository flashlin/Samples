{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last\n",
    "transformers==4.36.0\n",
    "torch==2.1.2\n",
    "sentencepiece==0.1.99\n",
    "bitsandbytes==0.41.3.post2\n",
    "peft==0.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "   'system': \"You are an expert in extracting questions and answers.\",\n",
    "   'input': \"\"\"# Introduction to Live Dealer Baccarat\n",
    "The objective of the game is to bet on whichever of two hands, the player's hand or the banker's hand, that the Player thinks will have a point value closest to 9. The Player can also bet on a tie.\n",
    "\n",
    "The game is presented to the Player with a live person dealing the cards on screen to provide the Player with a realistic gaming environment in real time.\n",
    "\n",
    "The theoretical return to player of this game is 98.41%.\n",
    "Over a long period of time, the game is likely to average a return to the Player of 98.48% of the total bets made.\"\"\",\n",
    "   'output': \"\"\"Question: What is Live Dealer Baccarat?\n",
    "Answer: The objective of the game is to bet on whichever of two hands, the player's hand or the banker's hand, that the Player thinks will have a point value closest to 9. The Player can also bet on a tie.\"\"\"\n",
    "}\n",
    "\n",
    "extract_qa_train_file = './results/extract_qa.jsonl'\n",
    "\n",
    "with open(extract_qa_train_file, 'w', encoding='utf-8') as f:\n",
    "   json = json.dumps(data)\n",
    "   f.write(json)\n",
    "   f.write(json)\n",
    "   f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2bef3b206741798bba1680a724da06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "MODEL_PATH = f\"../models/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map='auto',\n",
    "    local_files_only=True,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38030350c3a4d6a8278200a78075b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870a57d3d3904bf7a73a9ab5508f59d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7401c346defe43fe863c2f212b0e66ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "instruct_tune_dataset = load_dataset('json', data_files=extract_qa_train_file, cache_dir='data_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_input, sys_prompt=\"\", assistant_output=\"\"):\n",
    "   template_template = \"\"\"### Instruction:\n",
    "{sys_prompt}\n",
    "### Input:\n",
    "{user_input}\n",
    "### Response:\n",
    "{assistant_output}\"\"\"\n",
    "   return template_template.format(sys_prompt=sys_prompt, user_input=user_input, assistant_output=assistant_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj','lm_head']\n",
    ")\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(model)\n",
    "peft_model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model(model):\n",
    "   for i in model.named_parameters():\n",
    "       print(f\"{i[0]} -> {i[1].device}\")\n",
    "# peft_model = peft_model.to('cuda:0')\n",
    "# dump_model(peft_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "  output_dir = \"./results/Mistral_extractQA_results\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 2, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 2,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=1e-4,\n",
    "  bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    "  #report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_data_to_tokens(tokenizer, ds_data):\n",
    "   result = list()\n",
    "   for row in ds_data:\n",
    "      train_prompt = build_prompt(row['input'], sys_prompt=row['system'], assistant_output=row['output'])\n",
    "      tokens = tokenizer.encode(train_prompt) + [tokenizer.eos_token_id]\n",
    "      result.append(tokens)\n",
    "   return result\n",
    "\n",
    "def padding_train_dataset(tokenizer, ds_tokens):\n",
    "   maxlen = max(map(len, ds_tokens))\n",
    "   ds_result = list()\n",
    "   for tokens in ds_tokens:\n",
    "      delta = maxlen - len(tokens)\n",
    "      # 將 EOS Token 當作 PAD Token 來用\n",
    "      tokens += [tokenizer.eos_token_id] * delta\n",
    "      ds_result.append({\n",
    "         \"input_ids\": tokens, \n",
    "         \"labels\": tokens\n",
    "      })\n",
    "   return ds_result\n",
    "\n",
    "tokens_dataset = convert_raw_data_to_tokens(tokenizer, instruct_tune_dataset['train'])\n",
    "train_dataset = padding_train_dataset(tokenizer, tokens_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# def create_prompt_format(sample):\n",
    "#   output_texts = []\n",
    "#   for i in range(len(sample)):\n",
    "#     prompt = build_prompt(sample['input'], sys_prompt=sample['system'], assistant_output=sample['output'])\n",
    "#     output_texts.append(prompt)\n",
    "#   return output_texts\n",
    "\n",
    "def create_prompt_format(sample):\n",
    "  prompt = build_prompt(sample['input'], sys_prompt=sample['system'], assistant_output=sample['output'])\n",
    "  return prompt\n",
    "\n",
    "#response_template = \"### Response:\\n\"\n",
    "#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=peft_model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=1024 * 4,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt_format,\n",
    "  # data_collator=collator,\n",
    "  args=train_args,\n",
    "  train_dataset=instruct_tune_dataset['train'],\n",
    "  #eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(\"./results/Mistral_extractQA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
