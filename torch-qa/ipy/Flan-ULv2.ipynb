{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_standard.langchain_lit import LlmEmbedding, load_all_documents, create_chroma_vectorstore\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_all_documents('./documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL = \"bge-base-en\"\n",
    "#EMB_MODEL = \"bge-large-zh-v1.5\"\n",
    "llm_embedd = LlmEmbedding(f\"../models/{EMB_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_chroma_vectorstore(llm_embedd.embedding, \"sample\")\n",
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "def load_t5_model_in_8bit(model_name_path):\n",
    "   model = T5ForConditionalGeneration.from_pretrained(model_name_path, device_map=\"auto\", load_in_8bit=True)                                                                 \n",
    "   return model\n",
    "\n",
    "def create_model_load_in_4bit_config():\n",
    "   nf4_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "   )\n",
    "   return nf4_config\n",
    "\n",
    "def load_t5_model_in_4bit(model_name_path):\n",
    "   nf4_config = create_model_load_in_4bit_config()\n",
    "   model = T5ForConditionalGeneration.from_pretrained(model_name_path, \n",
    "                                                      device_map=\"auto\", \n",
    "                                                      quantization_config=nf4_config)\n",
    "   return model\n",
    "   \n",
    "   \n",
    "def load_t5_model_in_cpu_and_gpu(model_name_path):\n",
    "   quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "   device_map = {\n",
    "      \"transformer.word_embeddings\": 0,\n",
    "      \"transformer.word_embeddings_layernorm\": 0,\n",
    "      \"lm_head\": \"cpu\",\n",
    "      \"transformer.h\": 0,\n",
    "      \"transformer.ln_f\": 0,\n",
    "   }   \n",
    "   model = T5ForConditionalGeneration.from_pretrained(model_name_path, \n",
    "                                                      device_map=\"auto\", \n",
    "                                                      quantization_config=quantization_config)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, T5Config, AutoModelForSeq2SeqLM\n",
    "import re\n",
    "\n",
    "def get_model_device_map(model_name_path):\n",
    "   config = AutoConfig.from_pretrained(model_name_path)\n",
    "   with init_empty_weights():\n",
    "      model = AutoModelForCausalLM.from_config(config)\n",
    "   device_map = infer_auto_device_map(model)\n",
    "   device_map = infer_auto_device_map(model, no_split_module_classes=[\"OPTDecoderLayer\"])\n",
    "   return device_map\n",
    "\n",
    "def get_t5_device_map(model_name_path):\n",
    "   config = AutoConfig.from_pretrained(model_name_path)\n",
    "   with init_empty_weights():\n",
    "      model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "   device_map = infer_auto_device_map(model)\n",
    "   device_map = infer_auto_device_map(model, no_split_module_classes=[\"OPTDecoderLayer\"])\n",
    "   return device_map\n",
    "\n",
    "# r'model\\.layers\\.\\d+'\n",
    "def adjust_device_map(device_map, module_name_pattern, percent=0.2, device='cpu'):\n",
    "   pattern = re.compile(module_name_pattern)\n",
    "   match_model_names = [key for key in device_map if pattern.fullmatch(key)]\n",
    "   catch_model_names = match_model_names[:int(len(match_model_names) * percent)]\n",
    "   for model_name in catch_model_names:\n",
    "      device_map[model_name] = device\n",
    "   return device_map\n",
    "\n",
    "\n",
    "def load_t5_model_in_split(model_name_path):\n",
    "   device_map = get_t5_device_map(model_name_path)\n",
    "   for key in ['decoder.embed_tokens.weight', \n",
    "               'encoder.embed_tokens.weight', \n",
    "               'lm_head.weight', \n",
    "               'shared.weight']:\n",
    "      device_map[key] = 'cpu'\n",
    "   device_map = adjust_device_map(device_map, r\"encoder\\.block\\.\\d+\", 0.2)\n",
    "   device_map = adjust_device_map(device_map, r\"decoder\\.block\\.\\d+\", 0.2)\n",
    "   model = T5ForConditionalGeneration.from_pretrained(\n",
    "      model_name_path,\n",
    "      device_map=device_map, \n",
    "      offload_folder=\"offload\", \n",
    "      offload_state_dict = True, \n",
    "      torch_dtype=torch.float16)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/flan-ul2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shared': 0,\n",
       " 'decoder.embed_tokens': 0,\n",
       " 'encoder.embed_tokens': 0,\n",
       " 'encoder.block.0': 0,\n",
       " 'encoder.block.1': 0,\n",
       " 'encoder.block.2': 0,\n",
       " 'encoder.block.3': 0,\n",
       " 'encoder.block.4': 0,\n",
       " 'encoder.block.5': 0,\n",
       " 'encoder.block.6': 0,\n",
       " 'encoder.block.7': 0,\n",
       " 'encoder.block.8': 0,\n",
       " 'encoder.block.9': 0,\n",
       " 'encoder.block.10': 0,\n",
       " 'encoder.block.11': 0,\n",
       " 'encoder.block.12': 0,\n",
       " 'encoder.block.13': 0,\n",
       " 'encoder.block.14': 0,\n",
       " 'encoder.block.15': 0,\n",
       " 'encoder.block.16': 0,\n",
       " 'encoder.block.17': 0,\n",
       " 'encoder.block.18': 0,\n",
       " 'encoder.block.19': 0,\n",
       " 'encoder.block.20': 0,\n",
       " 'encoder.block.22': 'cpu',\n",
       " 'encoder.block.23': 'cpu',\n",
       " 'encoder.block.24': 'cpu',\n",
       " 'encoder.block.25': 'cpu',\n",
       " 'encoder.block.26': 'cpu',\n",
       " 'encoder.block.27': 'cpu',\n",
       " 'encoder.block.28': 'cpu',\n",
       " 'encoder.block.29': 'cpu',\n",
       " 'encoder.block.30': 'cpu',\n",
       " 'encoder.block.31': 'cpu',\n",
       " 'encoder.final_layer_norm': 'cpu',\n",
       " 'encoder.dropout': 'cpu',\n",
       " 'decoder.block.0': 'cpu',\n",
       " 'decoder.block.1': 'cpu',\n",
       " 'decoder.block.2': 'cpu',\n",
       " 'decoder.block.3': 'cpu',\n",
       " 'decoder.block.4': 'cpu',\n",
       " 'decoder.block.5': 'cpu',\n",
       " 'decoder.block.6': 'cpu',\n",
       " 'decoder.block.7': 'cpu',\n",
       " 'decoder.block.8': 'cpu',\n",
       " 'decoder.block.9': 'cpu',\n",
       " 'decoder.block.10': 'cpu',\n",
       " 'decoder.block.11': 'cpu',\n",
       " 'decoder.block.12.layer.0': 'cpu',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.q': 'cpu',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.k': 'disk',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.v': 'disk',\n",
       " 'decoder.block.12.layer.1.EncDecAttention.o': 'disk',\n",
       " 'decoder.block.12.layer.1.layer_norm': 'disk',\n",
       " 'decoder.block.12.layer.1.dropout': 'disk',\n",
       " 'decoder.block.12.layer.2': 'disk',\n",
       " 'decoder.block.13': 'disk',\n",
       " 'decoder.block.14': 'disk',\n",
       " 'decoder.block.15': 'disk',\n",
       " 'decoder.block.16': 'disk',\n",
       " 'decoder.block.17': 'disk',\n",
       " 'decoder.block.18': 'disk',\n",
       " 'decoder.block.19': 'disk',\n",
       " 'decoder.block.20': 'disk',\n",
       " 'decoder.block.21': 'disk',\n",
       " 'decoder.block.22': 'disk',\n",
       " 'decoder.block.23': 'disk',\n",
       " 'decoder.block.24': 'disk',\n",
       " 'decoder.block.25': 'disk',\n",
       " 'decoder.block.26': 'disk',\n",
       " 'decoder.block.27': 'disk',\n",
       " 'decoder.block.28': 'disk',\n",
       " 'decoder.block.29': 'disk',\n",
       " 'decoder.block.30': 'disk',\n",
       " 'decoder.block.31': 'disk',\n",
       " 'decoder.final_layer_norm': 'disk',\n",
       " 'decoder.dropout': 'disk',\n",
       " 'lm_head': 'disk',\n",
       " 'encoder.block.21': 'cpu'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map = get_t5_device_map(MODEL_PATH)\n",
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/Chinese-Llama-2-7b\"\n",
    "device_map = get_model_device_map(MODEL_PATH)\n",
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tied parameters are on different devices: {'decoder.embed_tokens.weight': 0, 'encoder.embed_tokens.weight': 0, 'lm_head.weight': 'disk', 'shared.weight': 0}. Please modify your custom device map or set `device_map='auto'`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f822a971f0c14c49b7fb2947e660bc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = \"../models/flan-ul2\"\n",
    "llm_model = load_t5_model_in_split(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/flan-ul2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mload_t5_model_in_cpu_and_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 34\u001b[0m, in \u001b[0;36mload_t5_model_in_cpu_and_gpu\u001b[0;34m(model_name_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m device_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.word_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     29\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.word_embeddings_layernorm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.ln_f\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     33\u001b[0m }   \n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/modeling_utils.py:3706\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3698\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3699\u001b[0m     (\n\u001b[1;32m   3700\u001b[0m         model,\n\u001b[1;32m   3701\u001b[0m         missing_keys,\n\u001b[1;32m   3702\u001b[0m         unexpected_keys,\n\u001b[1;32m   3703\u001b[0m         mismatched_keys,\n\u001b[1;32m   3704\u001b[0m         offload_index,\n\u001b[1;32m   3705\u001b[0m         error_msgs,\n\u001b[0;32m-> 3706\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3718\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3724\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3725\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/modeling_utils.py:3835\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3833\u001b[0m is_safetensors \u001b[38;5;241m=\u001b[39m archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_safetensors:\n\u001b[0;32m-> 3835\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3836\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3837\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for them. Alternatively, make sure you have `safetensors` installed if the model you are using\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m offers the weights in this format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3839\u001b[0m     )\n\u001b[1;32m   3840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3841\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(offload_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format."
     ]
    }
   ],
   "source": [
    "llm = load_t5_model_in_cpu_and_gpu(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "   MODEL_PATH,\n",
    "   device_map='auto',\n",
    "   local_files_only=True,\n",
    "   use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = load_qa_with_sources_chain(llm, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is your name?\"\n",
    "sub_docs = vector_store.similarity_search_with_score(question, k=5)\n",
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain({\"input_documents\":sub_docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create final answer.\n",
    "If you don't known the answer, just say that you don't known. Don't try to make up an answer.\n",
    "Response in English.\n",
    "\n",
    "QUESTION: {question} \n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER IN English:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"summaries\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain2 = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "question = \"Who is US president?\"\n",
    "llm_chain2({\"input_documents\":sub_docs, \"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
