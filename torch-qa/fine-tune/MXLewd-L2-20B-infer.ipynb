{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers trl accelerate torch bitsandbytes peft datasets -qU\n",
    "#pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0200b9d393a745409a365ddcf51fb058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = f\"../models/MXLewd-L2-20B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map='auto',\n",
    "    local_files_only=True,\n",
    "    #trust_remote_code=False,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nWhat are my value if I have 2 cards of 2, 3 in baccarat? [/INST]\\n\\nThe value of your hand in Baccarat is determined by adding the face value of all the cards in your hand, except for Aces, which are worth 1, and 10s and face cards, which are worth 0.\\n\\nIn your hand, you have two cards of value 2 each, which gives you a total of 4. Also, you have a card of value 3, which makes your total score 7.\\n\\nYour hand value is 7.</s>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask():\n",
    "   global model\n",
    "   messages = [\n",
    "      {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "      {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "   ]\n",
    "   messages = [\n",
    "      {\"role\": \"user\", \"content\": \"What are my value if I have 2 cards of 2, 3 in baccarat?\"},\n",
    "   ]\n",
    "   encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "   model_inputs = encodeds.to(model.device)\n",
    "   generated_ids = model.generate(model_inputs, \n",
    "                                  max_new_tokens=1000,\n",
    "                                  pad_token_id=tokenizer.eos_token_id, \n",
    "                                  do_sample=True)\n",
    "   decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "   #print(decoded_output[0])\n",
    "   return decoded_output[0]\n",
    "\n",
    "ask()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
