{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from finetune_utils import load_finetune_config\n",
    "from langchain_lit import load_markdown_documents, LlmEmbedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_finetune_config()\n",
    "device = \"cuda\"\n",
    "EMB_MODEL = \"bge-base-en\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store():\n",
    "    print(\"loading data\")\n",
    "    docs = load_markdown_documents(\"./data\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=35)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    llm_embedding = LlmEmbedding(f\"../models/{EMB_MODEL}\")\n",
    "    print(\"loading vector\")\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits,\n",
    "                                        embedding=llm_embedding.embedding)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "base_model = f\"../models/{model_name}\"\n",
    "peft_model_id = f\"./outputs/{model_name}-qlora\"\n",
    "if not os.path.exists(f\"{peft_model_id}/adapter_config.json\"):\n",
    "    peft_model_id = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "model.load_adapter(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 4096\n",
    "generation_config.temperature = 0.01  # 0.7\n",
    "generation_config.top_p = 2\n",
    "generation_config.do_sample = True\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"text-generation\"\n",
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # max_length=4096,\n",
    "    temperature=0.01,\n",
    "    top_p=2,\n",
    "    repetition_penalty=1.15,\n",
    "    return_full_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your gaming knowledge. \n",
    "If the answer cannot be found from the context, try to find the answer from your knowledge. \n",
    "If still unable to find the answer, respond with 'I don't know.'.\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = load_vector_store()\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={'k': 10, 'fetch_k': 50}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_qa(user_input):\n",
    "    resp = rag_chain.invoke(user_input)\n",
    "    doc = resp['context'][0]\n",
    "    page_content = doc.page_content\n",
    "    source = doc.metadata['source']\n",
    "    answer = resp['text']\n",
    "    # print(f\"{source=}\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ask_qa(\"Who is Flash's Father?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ask_qa(\"Use C# write HELLO string\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(user_input):\n",
    "   prompt_template2 = \"\"\"\n",
    "[INST]    \n",
    "{context}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "   prompt = prompt_template2.format(context=user_input)\n",
    "   encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "   outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config\n",
    "   )\n",
    "\n",
    "   resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "   answer = resp.replace(prompt, \"\")\n",
    "   # answer = answer.strip().replace(\"### ANSWER:\\n\", \"\")\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_donnot_known(user_input):\n",
    "    if \"please provide more information or clarify your question\" in user_input:\n",
    "        return \"Yes.\"\n",
    "    if \"I don't understand what you are asking for\" in user_input:\n",
    "        return \"Yes.\"\n",
    "    if \"Can you please provide more information about what you are asking for\" in user_input:\n",
    "        return \"Yes.\"\n",
    "    return ask_llm(f\"\"\"{user_input}\\n\\n\n",
    "If the content above says something like \"I don't know what you're saying. Could you provide more information?\" which conveys a similar meaning, then answer Yes; otherwise, answer No directly.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = is_donnot_known(\"I don't understand what you are asking for. Can you please provide more information or clarify your question?\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = is_donnot_known(\"Flash's father is Jack.\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qa(user_input):\n",
    "   answer = ask_qa(user_input)\n",
    "   return answer\n",
    "   # unknown_answer = is_donnot_known(answer)\n",
    "   # if unknown_answer.startswith('Yes.'):\n",
    "   #    print(\"QA FAIL\")\n",
    "   #    return ask_llm(user_input)\n",
    "   # return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = try_qa(\"use C# write HELLO string\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = try_qa(\"What information does the baccarat payout table include?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = try_qa(\"Who is Flash's mother?\")\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
