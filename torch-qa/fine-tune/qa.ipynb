{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from finetune_utils import load_finetune_config\n",
    "from langchain_lit import load_markdown_documents, LlmEmbedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_finetune_config()\n",
    "device = \"cuda\"\n",
    "EMB_MODEL = \"bge-base-en\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store():\n",
    "    print(\"loading data\")\n",
    "    docs = load_markdown_documents(\"./data\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=35)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    llm_embedding = LlmEmbedding(f\"../models/{EMB_MODEL}\")\n",
    "    print(\"loading vector\")\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits,\n",
    "                                        embedding=llm_embedding.embedding)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "base_model = f\"../models/{model_name}\"\n",
    "peft_model_id = f\"./outputs/{model_name}-qlora\"\n",
    "if not os.path.exists(f\"{peft_model_id}/adapter_config.json\"):\n",
    "    peft_model_id = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea51e00b9477410e9b741756a84e6893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "model.load_adapter(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 4096\n",
    "generation_config.temperature = 0.01  # 0.7\n",
    "generation_config.top_p = 2\n",
    "generation_config.do_sample = True\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"text-generation\"\n",
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # max_length=4096,\n",
    "    temperature=0.01,\n",
    "    top_p=2,\n",
    "    repetition_penalty=1.15,\n",
    "    return_full_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "gaming knowledge. \n",
    "If the answer cannot be found from the context, try to find the answer from your knowledge. \n",
    "If still unable to find the answer, respond with 'I don't know'.\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "loading vector\n"
     ]
    }
   ],
   "source": [
    "vectorstore = load_vector_store()\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={'k': 10, 'fetch_k': 50}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | llm_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_qa(user_input):\n",
    "    resp = rag_chain.invoke(user_input)\n",
    "    doc = resp['context'][0]\n",
    "    page_content = doc.page_content\n",
    "    source = doc.metadata['source']\n",
    "    answer = resp['text']\n",
    "    print(f\"{source=}\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='data/Test.md'\n",
      " Flash's father is Jack.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_qa(\"Who is Flash's Father?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='data/Test.md'\n",
      " I don't understand what you are asking for. Can you please provide more information or clarify your question?\n"
     ]
    }
   ],
   "source": [
    "answer = ask_qa(\"Use C# write HELLO string\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(user_input):\n",
    "    prompt_template2 = \"\"\"\n",
    "Here is context:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "Does this content express a 'I don't known'? Please respond with 'Yes' or 'No'.\n",
    "\"\"\"\n",
    "    prompt = prompt_template2.format(context=user_input)\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = resp.replace(prompt, \"\")\n",
    "    answer = answer.strip().replace(\"### ANSWER:\\n\", \"\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### ANSWER:\\nNo.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ask(\"I don't understand what you are asking for. Can you please provide more information or clarify your question?\")\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
