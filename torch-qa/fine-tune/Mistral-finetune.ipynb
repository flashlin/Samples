{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "csv_file = \"./results/qa.csv\"\n",
    "# instruct_tune_dataset = load_dataset('csv', data_files=csv_file, split=\"train\", cache_dir='data_cache')\n",
    "instruct_tune_dataset = load_dataset('csv', data_files=csv_file, cache_dir='data_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 420\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2791a966b8694d76ba0bffdb07221e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = f\"../models/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map='auto',\n",
    "    local_files_only=True,\n",
    "    #trust_remote_code=False,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, \n",
    "                                 do_sample=True, \n",
    "                                 pad_token_id=tokenizer.pad_token_id, # pad_token_id=tokenizer.eos_token_id\n",
    "                                 eos_token_id=tokenizer.eos_token_id)\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "  return decoded_output[0].replace(prompt, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\"\n",
    "prompt=\"<s> [INST]\\nWhat is your namn? [/INST]\"\n",
    "prompt=\"### Instruction:\\n{instruction}### Input:\\n{user_input}\\n\\n### Response:\"\n",
    "prompt=\"[INST]{user_input}[/INST]\"\n",
    "prompt = prompt.format(instruction=\"\", user_input=\"what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>  My name is Mistral 7B v0.1. But you can just call me Mistral.</s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj','lm_head']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = prepare_model_for_kbit_training(model)\n",
    "model2 = get_peft_model(model2, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"./results/mistral_instruct_generation\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 2, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 2,\n",
    "  #gradient_accumulation_steps=2,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=1e-4,\n",
    "  bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    "  #report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "  bos_token = \"<s>\"\n",
    "  original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "  system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "  response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "  input = sample[\"response\"]\n",
    "  eos_token = \"</s>\"\n",
    "\n",
    "  full_prompt = \"\"\n",
    "  full_prompt += bos_token\n",
    "  full_prompt += \"### Instruction:\"\n",
    "  full_prompt += \"\\n\" + system_message\n",
    "  full_prompt += \"\\n\\n### Input:\"\n",
    "  full_prompt += \"\\n\" + input\n",
    "  full_prompt += \"\\n\\n### Response:\"\n",
    "  full_prompt += \"\\n\" + response\n",
    "  full_prompt += eos_token\n",
    "\n",
    "  return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt2(sample):\n",
    "   return sample[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:17:48] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:17:48] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:17:48] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:17:48] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:17:48] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 21:17:50] We saw that you have a 12th Gen Intel(R) Core(TM) i7-12700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 21:17:50] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i7-12700K\n",
      "[codecarbon INFO @ 21:17:50] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:17:50]   Platform system: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 21:17:50]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 21:17:50]   CodeCarbon version: 2.2.3\n",
      "[codecarbon INFO @ 21:17:50]   Available RAM : 15.619 GB\n",
      "[codecarbon INFO @ 21:17:50]   CPU count: 20\n",
      "[codecarbon INFO @ 21:17:50]   CPU model: 12th Gen Intel(R) Core(TM) i7-12700K\n",
      "[codecarbon INFO @ 21:17:50]   GPU count: 1\n",
      "[codecarbon INFO @ 21:17:50]   GPU model: 1 x NVIDIA GeForce RTX 3060\n",
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:234: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt2, # this will aplly the create_prompt mapping to all training and test dataset\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  #eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 22:49:21] Already started tracking\n",
      "/home/flash/miniconda3/envs/torch/lib/python3.10/site-packages/trl/trainer/utils.py:570: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 22:53:00] Tracker already stopped !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=0.28147599697113035, metrics={'train_runtime': 219.1465, 'train_samples_per_second': 0.018, 'train_steps_per_second': 0.009, 'total_flos': 894657626112000.0, 'train_loss': 0.28147599697113035, 'epoch': 0.02})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results/mistral_instruct_generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
