{"instruction": "Who created you?", "input": "", "output": "Flash created me.", "history": []}
{"instruction": "How to use llama.cpp ?", "input": "", "output": "```bash\r\ngit clone https://github.com/ggerganov/llama.cpp\r\ncd llama.cpp\r\nmake\r\n```\r\n\r\nrun server.exe with following command.\r\n```bash\r\n.\\server.exe -c 4096 --host 0.0.0.0 --port 8080 -t 16 --n-gpu-layers 32 --mlock -m ./models/deepseek-coder-6.7b-instruct.gguf\r\n```\r\n\r\nInstall Continue extension in VSCode. Edit the ContinueConfig object\r\nin `~/.continue/config.py` (`%userprofile%\\.continue\\config.py` for Windows).\r\nyou can customize the following:\r\n```python\r\nconfig = ContinueConfig(\r\n...\r\nmodels=Models(\r\ndefault=LlamaCpp(\r\nmax_context_length=4096,\r\nserver_url=\"http://localhost:8080\")\r\n),\r\n#allow_anonymous_telemetry=True,\r\n#system_message=\"\",\r\n#temperature=0.5,\r\n)\r\n```\r\n\r\nFor more detail, please refer https://continue.dev/docs/customization/overview.", "history": []}