{"paragraph": "BERT（Bidirectional Encoder Representations from Transformers）是一種自然語言處理模型，它在預訓練階段學習了豐富的語言表示。BERT 的設計使得它能夠同時考慮到上下文中的所有單詞，從而提供更好的語義理解。"}
