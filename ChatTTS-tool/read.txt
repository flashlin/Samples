語言模型, 是從很多的資料當中, 學習出根據前文, 來推算出下一個最有可能發生什麼字的模型
但是GPT不只是給你下一個字的選項, 而是根據事前訓練好的模型, 自動輸出下一個字, 下一句話, 甚至可以根據問題內容來回答問題, 這是怎麼做到的呢?

GPT的核心概念就是, 就是依照剛剛大家玩的文字接龍遊戲
依照你前面輸入的字, 來判斷下一個字要生成什麼字, 每一個字都用一個數字來代表
例如: 明明白色就有我的最愛 , 這句話就用, 1,1,2,3,4,5,6... 一連串數字來代表

對於完全沒學過語言的模型, 不給任何提示, 讓他自己先猜
先給第一個字 
明, 問第二個字是什麼? 
天
給他一個大巴掌
再來, 
日, 
再給他一個大巴掌, 
再來,
明, 
對了
現在前兩個字是明明, 第三個字是什麼?
明? 
一個大巴掌, 再來
日? 
一個大巴掌, 再來
白? 
對了
現在前三個字是明明白, 第四個字是啥?
於是在吃了無數個大巴掌之後, 模型終於學會了: 明明白色就有我的最愛 

這個方法不能說完全沒效果吧? 就是有點費力, 手都累了. 如果你用這種方法教小孩學東西, 就算教不出白癡, 也只能教出一個只會背文字的機器.
他理解不了人類文字的背後意思, 也創造不出新的表達語句. 所以對於 GPT 只依照前面的數字猜下一個數字, 就太簡單了, 他很容易讓人覺得 GPT 就是一個死記硬背的暴力怪物.

因為電腦不能把數字和他表達事物建立起關聯, 所以我們得幫每一個字的數字. 再多加好幾個數字來代表每一個文字背後的意思, 這個動作叫做向量化.

例如:我們要表示 Flash 這個字, 可以用這樣的一組數字 
1, 170, 65, 60
看不懂? 讓我簡單解釋一下, 你就明白了.
第一個數字表示性別, 1是男, 0是女. 第二個數字表示身高, 第三個數字表示體重, 第四個數字表示年齡.
現在你知道了, 這一組數字表示的是男性, 身高170 公分, 體重65 公斤, 年齡60 歲

甚麼? 
只用四個數字來表示語言的背後意思太簡單了? 沒問題, 我們可以增加更多的數字, 科學家就用這種方式去訓練模型, 這些一大堆數字就稱作參數.


這根曲線代表這些參數的數量, 當這些參數的數量從1增加到10, 然後10的二次方, 10的三次方, 10的四次方
模型的回答表現都很差, 得分幾乎為零, 但是到了10的五次方的時候, 突然發生了某種變化
模型居然直接從十幾分, 懸涯式跳到六十分, 然後開始緩慢式上升, 朝向一百分方向逼近. GPT模型從一竅不通到一下子就會了, 研究者把這個現象稱為智慧湧現.
這還只是個百萬個參數, 當參數超過1000億的時候, 模型出現更多神奇現象. 比如突然能理解語言, 突然能處理大規模任務.

GPT-3則有1750億參數, 而GPT-4的參數量大約有 100 兆個參數, 人工智慧告訴我們一件事情, 量變的確能產生質變

諷刺的是馬斯克前幾天突然買了一萬張顯示卡, 還從google 挖角專家過去, 看來馬老闆嘴上說不要, 心理還是知道 AI 有多重要

OpenAI之前發布了一則公告, 他們已經走在超人工智能的路線上了, 如果這條路走成, 通用型人工智就能誕生.
它能完成一切所謂的智力活動的任務, 人類說出一個目標, 它就能用我們沒法理解的方式達成
雖然近期還不會發生, 但已經不是遙不可及了

